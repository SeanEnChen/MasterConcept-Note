# 2.2 Encoder-Decoder Architecture：Lab Walkthrough

今天的內容是搭配該系列課程的程式碼實作，目標是從零開始建立一個以字元為基礎的詩詞產生器。這個產生器會學習莎士比亞的劇本資料集，並生成類似風格的文字。

## **設定與資料下載**

首先，需要匯入必要的程式庫，主要是 TensorFlow 和 Keras。

```Python
import os
import warnings

warnings.filterwarnings("ignore")
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import time

import numpy as np
import tensorflow as tf
```

接著，使用 `tf.keras.utils.get_file` 從 Google Cloud Storage 下載莎士比亞的劇本資料集。

```Python
path_to_file = tf.keras.utils.get_file(
    "shakespeare.txt",
	"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt",
)
```

## **讀取資料**

下載完成後，將劇本檔案讀取到名為 `text` 的變數中，並印出檔案的長度和前 250 個字元。

```Python
text = open(path_to_file, "rb").read().decode(encoding="utf-8")
print(f"Length of text: {len(text)} characters")
print(text[:250])
```

程式碼還會計算資料集中不重複字元的數量，這些字元將作為模型訓練的基本單位（符記）。

```Python
vocab = sorted(set(text))
print(f"{len(vocab)} unique characters")
```

## **文字預處理**

在訓練模型之前，需要將文字轉換成模型可以理解的數字表示。

1. **字元分割 (Character Splitting):** 使用 `tf.strings.unicode_split` 將原始文本分割成單個字元的序列。

```Python
example_texts = ["abcdefg", "xyz"]
chars = tf.strings.unicode_split(example_texts, input_encoding="UTF-8")
print(chars)
```

2. **建立字元到 ID 的映射 (Character to ID Mapping):** 使用 `tf.keras.layers.StringLookup` 建立一個將每個不重複字元映射到唯一數字 ID 的層。這就像建立一個詞彙表。

```Python
ids_from_chars = tf.keras.layers.StringLookup(
	vocabulary=list(vocab), mask_token=None
)
ids = ids_from_chars(chars)
print(ids)
```

3. **建立 ID 到字元的映射 (ID to Character Mapping):** 為了將模型輸出的數字 ID 轉換回人類可讀的文字，需要建立一個反向的映射。同樣使用 `tf.keras.layers.StringLookup`，但這次設定 `invert=True`。

```Python
chars_from_ids = tf.keras.layers.StringLookup(
	vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None
)
chars = chars_from_ids(ids)
print(chars)

def text_from_ids(ids):
	return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)
```

## **建立訓練資料集**

接下來，需要將文本資料轉換成模型可以訓練的格式，也就是輸入序列和目標序列的配對。

1. **將所有字元轉換為 ID:** 使用之前建立的 `ids_from_chars` 將整個文本轉換成一個 ID 序列。

```Python
all_ids = ids_from_chars(tf.strings.unicode_split(text, "UTF-8"))
print(all_ids)
ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)
for ids in ids_dataset.take(10):
	print(chars_from_ids(ids).numpy().decode("utf-8"))
```

2. **建立序列 (Sequences):** 將 ID 序列分割成固定長度的序列。這裡設定 `seq_length` 為 100，表示每個輸入序列包含 100 個字元。使用 `tf.data.Dataset.batch` 完成這個步驟。

```Python
seq_length = 100
examples_per_epoch = len(text) // (seq_length + 1)
sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)
for seq in sequences.take(1):
	print(chars_from_ids(seq))
for seq in sequences.take(5):
	print(text_from_ids(seq).numpy())
```

3. **建立輸入和目標序列 (Input and Target Sequences):** 對於每個序列，輸入序列是除了最後一個字元之外的所有字元，而目標序列是從第二個字元開始到結尾的所有字元。這樣做的目的是讓模型學習預測序列中的下一個字元。

```Python
def split_input_target(sequence):
	input_text = sequence[:-1]
	target_text = sequence[1:]
	return input_text, target_text

split_input_target(list("Tensorflow"))
dataset = sequences.map(split_input_target)
for input_example, target_example in dataset.take(1):
	print("Input :", text_from_ids(input_example).numpy())
	print("Target:", text_from_ids(target_example).numpy())
```

4. **建立訓練批次 (Training Batches):** 最後，將資料集打亂並分成批次，以便更有效地進行訓練。`BUFFER_SIZE` 用於打亂資料，`BATCH_SIZE` 定義每個批次的大小。`prefetch` 能夠在模型訓練的同時預先載入下一批資料，提高效率。

```Python
BATCH_SIZE = 64
BUFFER_SIZE = 10000
dataset = (
	dataset.shuffle(BUFFER_SIZE)
	.batch(BATCH_SIZE, drop_remainder=True)
	.prefetch(tf.data.experimental.AUTOTUNE)
)
print(dataset)
```

## **建立模型**

這裡使用 Keras 的模型子類別化 (Subclassing) 來定義 RNN 模型。模型包含以下幾層：

- **Embedding 層 (`tf.keras.layers.Embedding`):** 將每個字元的 ID 轉換成一個固定維度的向量表示。這個向量能夠捕捉字元之間的語義關係。`embedding_dim` 定義了這個向量的維度，這裡設定為 256。
- **GRU 層 (`tf.keras.layers.GRU`):** 一種循環神經網路層，用於處理序列資料。GRU 層能夠記住序列中的上下文信息，並用於預測下一個字元。`rnn_units` 定義了 GRU 層中神經元的數量，這裡設定為 1024。`return_sequences=True` 表示返回每個時間步的輸出序列，`return_state=True` 表示返回最終的隱藏狀態。
- **Dense 層 (`tf.keras.layers.Dense`):** 一個全連接層，作為輸出層。它的輸出大小等於詞彙表的大小 (`vocab_size`)，每個輸出對應於詞彙表中一個字元的 logits（未經歸一化的對數機率）。

```Python
vocab_size = len(vocab)
embedding_dim = 256
rnn_units = 1024

class MyModel(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, rnn_units):
        super().__init__(self)
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.gru = tf.keras.layers.GRU(
            rnn_units, return_sequences=True, return_state=True
        )
        self.dense = tf.keras.layers.Dense(vocab_size)

    def call(self, inputs, states=None, return_state=False, training=False):
        x = self.embedding(inputs, training=training)
        if states is None:
            states = self.gru.get_initial_state(x)
        x, states = self.gru(x, initial_state=states, training=training)
        x = self.dense(x, training=training)
        if return_state:
            return x, states
        else:
            return x

model = MyModel(
    vocab_size=len(ids_from_chars.get_vocabulary()),
    embedding_dim=embedding_dim,
    rnn_units=rnn_units,
)
```

## **測試模型 (未訓練)**

在訓練之前，可以簡單地測試一下模型的輸出形狀。對於一個輸入批次，模型的輸出形狀應該是 `(batch_size, sequence_length, vocab_size)`，表示對於批次中的每個序列的每個字元，模型都輸出了詞彙表中每個字元的 logits。

```Python
for input_example_batch, target_example_batch in dataset.take(1):
    example_batch_predictions = model(input_example_batch)
    print(
        example_batch_predictions.shape,
        "# (batch_size, sequence_length, vocab_size)",
    )
```

`model.summary()` 可以查看模型的結構和參數數量。

為了從模型的輸出 logits 中得到實際的字元預測，需要從輸出分佈中進行抽樣。直接使用 `argmax` 可能會導致模型陷入重複的預測。`tf.random.categorical` 函數可以根據 logits 的分佈進行隨機抽樣。

```Python
sampled_indices = tf.random.categorical(
    example_batch_predictions[0], num_samples=1
)
sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()
print("Input:\n", text_from_ids(input_example_batch[0]).numpy())
print()
print("Next Char Predictions:\n", text_from_ids(sampled_indices).numpy())
```

由於模型尚未經過訓練，此時的預測結果是隨機的。

## **訓練模型**

現在開始訓練模型。

1. **定義損失函數 (Loss Function):** 由於這是一個多類別分類問題（預測下一個字元是詞彙表中的哪一個），這裡使用 `tf.keras.losses.SparseCategoricalCrossentropy` 作為損失函數。`from_logits=True` 表示損失函數的輸入是模型的 logits 輸出。

```Python
loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)
example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)
print("Mean loss:         ", example_batch_mean_loss)
print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
```

2. **配置訓練過程 (Compile the Model):** 使用 `model.compile` 方法配置訓練過程，指定優化器（這裡使用 Adam 優化器）和損失函數。

    ```Python
    model.compile(optimizer='adam', loss=loss)
    ```

3. **配置檢查點 (Configure Checkpoints):** 使用 `tf.keras.callbacks.ModelCheckpoint` 在訓練過程中定期保存模型的權重。這樣可以在訓練中斷後恢復，或者選擇在驗證集上表現最好的模型。

```Python
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
	filepath=checkpoint_prefix,
	save_weights_only=True
)
```

4. **執行訓練 (Execute Training):** 使用 `model.fit` 方法開始訓練。`epochs` 參數指定了訓練的輪數（這裡設定為 10）。`callbacks` 參數指定了在訓練過程中使用的回調函數，這裡使用了檢查點回調函數。

```Python
EPOCHS = 10
history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])
```

## **生成文字**

訓練完成後，需要編寫程式碼來使用訓練好的模型生成新的文字。由於 RNN 模型在每個時間步都會維護一個內部狀態，因此生成文字需要逐步進行。

1. **創建單步生成模型 (One-Step Generation Model):** 定義一個新的 Keras 模型子類 `OneStep`，它包含訓練好的模型以及字元和 ID 之間的映射。`generate_one_step` 方法接收一個輸入字元（或字元序列）和之前的模型狀態，然後預測下一個字元和新的模型狀態。

```Python
class OneStep(tf.keras.Model):
	def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):
		super().__init__()
		self.temperature = temperature
		self.model = model
		self.chars_from_ids = chars_from_ids
		self.ids_from_chars = ids_from_chars

		# Create a mask to prevent "[UNK]" from being generated.
		skip_ids = self.ids_from_chars(["[UNK]"])[:, None]
		sparse_mask = tf.SparseTensor(
			# Put a -inf at each bad index.
			values=[-float("inf")] * len(skip_ids),
			indices=skip_ids,
			# Match the shape to the vocabulary
			dense_shape=[len(ids_from_chars.get_vocabulary())],
		)
		self.prediction_mask = tf.sparse.to_dense(sparse_mask)

	@tf.function
	def generate_one_step(self, inputs, states=None):
		# Convert strings to token IDs.
		input_chars = tf.strings.unicode_split(inputs, "UTF-8")
		input_ids = self.ids_from_chars(input_chars).to_tensor()

		# Run the model.
		# predicted_logits.shape is [batch, char, next_char_logits]
		predicted_logits, states = self.model(
			inputs=input_ids, states=states, return_state=True
		)
		# Only use the last prediction.
		predicted_logits = predicted_logits[:, -1, :]
		predicted_logits = predicted_logits / self.temperature
		# Apply the prediction mask: prevent "[UNK]" from being generated.
		predicted_logits = predicted_logits + self.prediction_mask

		# Sample the output logits to generate token IDs.
		predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)
		predicted_ids = tf.squeeze(predicted_ids, axis=-1)

		# Convert from token ids to characters
		predicted_chars = self.chars_from_ids(predicted_ids)

		# Return the characters and model state.
		return predicted_chars, states

one_step_model = OneStep(model, chars_from_ids, ids_from_chars)
```

**Temperature 參數:** `temperature` 是一個控制生成文本隨機性的參數。

- 較低的 temperature (例如 0.2) 會使模型更具確定性，總是選擇機率最高的下一個字元，生成的文本可能更保守和可預測。
- 較高的 temperature (例如 1.0 或更高) 會增加隨機性，使模型更有可能選擇機率較低的字元，生成的文本可能更具創造性和多樣性，但也可能包含更多錯誤或無意義的內容。

2. **生成文本的迴圈 (Generation Loop):** 使用一個迴圈來重複調用 `generate_one_step` 方法，從一個初始的提示詞開始，逐步生成指定長度的文本。

```Python
start = time.time()
states = None
next_char = tf.constant(["ROMEO:"])
result = [next_char]

for n in range(1000):
	next_char, states = one_step_model.generate_one_step(
		next_char, states=states
	)
	result.append(next_char)

result = tf.strings.join(result)
end = time.time()
print(result[0].numpy().decode("utf-8"), "\n\n" + "_" * 80)
print("\nRun time:", end - start)
```

在這個例子中，初始提示詞是 "ROMEO:"，迴圈會執行 1000 次，每次生成一個新的字元，並將其添加到結果列表中。

2. **批量生成 (Batch Generation):** 程式碼還展示了如何批量生成文本，可以提高生成速度。

```Python
start = time.time()
states = None
next_char = tf.constant(["ROMEO:", "ROMEO:", "ROMEO:", "ROMEO:", "ROMEO:"])
result = [next_char]

for n in range(1000):
	next_char, states = one_step_model.generate_one_step(
		next_char, states=states
	)
	result.append(next_char)

result = tf.strings.join(result)
end = time.time()
for s in result:
	print(s.numpy().decode("utf-8"))
print("\n\n" + "_" * 80)
print("\nRun time:", end - start)
```

## **匯出生成器**

訓練好的單步生成模型可以保存為 `tf.saved_model`，以便在其他地方使用。

```Python
tf.saved_model.save(one_step_model, 'one_step')
one_step_reloaded = tf.saved_model.load('one_step')

states = None
next_char = tf.constant(['ROMEO:'])
result = [next_char]

for n in range(100):
    next_char, states = one_step_reloaded.generate_one_step(
        next_char, states=states
    )
    result.append(next_char)

print(tf.strings.join(result)[0].numpy().decode("utf-8"))
```

## **進階：自訂訓練**

最後一部分介紹了如何自訂訓練迴圈，以便更精細地控制訓練過程。這部分使用了 `tf.GradientTape` 來追蹤梯度，並手動應用梯度更新。

1. **自訂訓練步驟 (Custom Training Step):** 定義一個包含自訂訓練邏輯的 `train_step` 方法。

```Python
class CustomTraining(MyModel):
	@tf.function
	def train_step(self, inputs):
		inputs, labels = inputs
		with tf.GradientTape() as tape:
			predictions = self(inputs, training=True)
			loss = self.loss(labels, predictions)
		grads = tape.gradient(loss, model.trainable_variables)
		self.optimizer.apply_gradients(zip(grads, model.trainable_variables))

		return {"loss": loss}

model = CustomTraining(
	vocab_size=len(ids_from_chars.get_vocabulary()),
	embedding_dim=embedding_dim,
	rnn_units=rnn_units,
)
model.compile(
	optimizer=tf.keras.optimizers.Adam(),
	loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
)
model.fit(dataset, epochs=1)
```

2. **完整的自訂訓練迴圈 (Complete Custom Training Loop):** 展示了如何編寫一個完整的訓練迴圈，包括計算損失、應用梯度、記錄指標和保存模型權重。

```Python
EPOCHS = 10
mean = tf.metrics.Mean()

for epoch in range(EPOCHS):
	start = time.time()
	mean.reset_states()
	for batch_n, (inp, target) in enumerate(dataset):
		logs = model.train_step([inp, target])
		mean.update_state(logs["loss"])
		if batch_n % 50 == 0:
			template = f"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}"
			print(template)
	if (epoch + 1) % 5 == 0:
		model.save_weights(checkpoint_prefix.format(epoch=epoch))
	print()
	print(f"Epoch {epoch+1} Loss: {mean.result().numpy():.4f}")
	print(f"Time taken for 1 epoch {time.time() - start:.2f} sec")
	print("_" * 80)

model.save_weights(checkpoint_prefix.format(epoch=epoch))
```

自訂訓練迴圈提供了更大的靈活性，可以實現更複雜的訓練策略，例如課程學習 (curriculum learning)。