# 2.1 Encoder-Decoder Architecture: Overview

**1. Encoder-Decoder 架構總覽：**

- 這是一種**序列至序列 (sequence-to-sequence)** 的架構。
- **輸入：** 接收一個詞語序列，例如英文句子或提示字詞。
- **輸出：** 產生另一個詞語序列，例如法文翻譯或模型針對提示的回應。
- 包含兩個主要階段：
    - **編碼器 (Encoder)：** 將輸入語句轉換成一個**向量表示內容**。
    - **解碼器 (Decoder)：** 根據這個向量表示內容**產生輸出語句**。
- 編碼器和解碼器的內部機制可以是**循環類神經網路 (RNN)** 或更複雜的 **Transformer 模塊**。

**2. Encoder-Decoder 如何運作 (使用 RNN 為例)：**

- **Encoder：** 逐一接收輸入序列中的每個詞 (符記)，並產生一個 **狀態** 來表示當前詞以及之前處理過的詞。處理完所有輸入詞後，輸出一個 **向量** 來代表整個輸入語句。
- **Decoder：** 接收編碼器輸出的向量表示內容，並逐步產生輸出語句，一次產生一個詞 (符記)。它會根據當前狀態和已解碼的內容來預測下一個詞。

**3. 如何訓練編碼器-解碼器模型：**

- 需要一個**輸入/輸出配對的資料集**，例如原文和譯文語句。
- 模型在訓練過程中會調整**權重**，依據是模型產生的結果與資料集中真實輸出之間的**錯誤**。
- **強制指導法 (Teacher Forcing)：** 在訓練解碼器時，會將**先前已完成翻譯的正確詞**作為輸入，來引導模型產生下一個詞，而不是使用解碼器自己先前產生的內容。
- 解碼器在每個步驟會產生詞彙中每個詞成為下一個詞的**機率**。
- **詞彙選擇方法：**
    - **貪婪搜尋法 (Greedy Search)：** 選擇機率最高的詞。
    - **集束搜尋法 (Beam Search)：** 保留每個步驟中最有可能產生的語句片段，以產生更優質的結果。

**4. 如何使用訓練好的模型提供服務 (產生文字)：**

- 對於新的翻譯或提示，首先將提示透過編碼器轉換成向量表示內容。
- 將這個編碼後的向量和一個特殊的 "**GO**" 符記提供給解碼器。
- 解碼器產生第一個詞。
- 重複這個過程：
    - 將產生的詞轉換為向量表示。
    - 更新解碼器的狀態。
    - 透過 softmax 層產生詞彙機率。
    - 根據機率選擇下一個詞（使用貪婪或集束搜尋法）。
    - 直到產生完整的輸出序列。

**5. 後續發展：Transformer 架構**

- 現代大型語言模型（如 Google 的模型）已將簡單的 RNN 替換為更強大的 **Transformer 模塊**。
- Transformer 架構的基礎是 **注意力機制 (Attention Mechanism)**。
- 影片推薦觀看其他課程以了解更多關於注意力機制和 Transformer 模型。

總之，編碼器-解碼器架構是理解大型語言模型如何處理和生成序列資料的基礎。影片簡潔地介紹了這個架構的原理、訓練方法以及在實際應用中如何產生文字。