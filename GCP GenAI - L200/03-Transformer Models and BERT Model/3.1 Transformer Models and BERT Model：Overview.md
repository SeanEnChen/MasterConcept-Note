# 3.1 Transformer Models and BERT Model：Overview

**核心概念：**

- **Transformer 模型：**
    
    - 是一種基於 2017 年論文《Attention Is All You Need》開發的編碼器-解碼器模型。
    - **核心創新在於注意力機制 (Attention Mechanism)：** 允許模型在處理序列時關注輸入的不同部分，解決了傳統模型無法有效處理長序列的問題。
    - **與以往模型的不同：** 傳統模型產生的字詞向量不包含上下文資訊，而 Transformer 能夠根據上下文動態地表示字詞。
    - **優勢：**
        - 可運用平行處理，提高效率。
        - 能同時處理大量資料。
        - 顯著改善了機器翻譯等應用。
    - **架構：**
        - 由編碼器 (Encoder) 和解碼器 (Decoder) 構成。
        - **編碼器：** 由多個相同的編碼器堆疊而成（論文中為六個），每個編碼器包含兩個子層：
            - **自注意層 (Self-Attention Layer)：** 對輸入語句中的每個字詞，同時考慮其相關部分進行編碼。
            - **前饋層 (Feed-Forward Layer)：** 對自注意層的輸出進行獨立處理。
        - **解碼器：** 類似於編碼器，但包含一個額外的**編碼器-解碼器注意層 (Encoder-Decoder Attention Layer)**，用於關注輸入語句的相關部分。
    - **自注意機制細節：** 輸入嵌入會轉換為查詢 (Query) 向量、鍵 (Key) 向量和值 (Value) 向量，通過矩陣運算計算注意力分數，然後將值向量根據注意力分數加權求和，得到該位置的輸出。
    - **多頭注意力 (Multi-Head Attention)：** 通常會執行多次自注意機制（例如八次），並將結果串聯起來。
    - **Transformer 的變體：** 有些模型只使用編碼器（如 BERT），有些只使用解碼器，有些則同時使用。
- **BERT 模型 (Bidirectional Encoder Representations from Transformers)：**
    
    - 是一種僅使用 Transformer 編碼器部分的模型，由 Google 於 2018 年開發。
    - 是一種**預訓練**的 Transformer 模型，經過大量資料的訓練。
    - **應用：** 目前 Google 搜尋也使用了 BERT 技術，提升搜尋結果的相關性。
    - **模型大小：**
        - **BERT Base：** 12 層 Transformer，約 1.1 億個參數。
        - **BERT Large：** 24 層 Transformer，約 3.4 億個參數。
    - **訓練資料：** 整個 Wikipedia 語料庫和書籍語料庫。
    - **訓練目標：** 多任務學習，使其非常強大。
- **BERT 的訓練方式：**
    
    - **掩碼語言模型 (Masked Language Model, MLM)：** 隨機遮蓋輸入語句中的一部分字詞（建議比例為 15%），訓練模型預測被遮蓋的字詞。
    - **預測下一個語句 (Next Sentence Prediction, NSP)：** 給定兩個語句 A 和 B，訓練模型判斷語句 B 是否是語句 A 的下一句（二元分類任務）。
- **BERT 的輸入嵌入 (Input Embeddings)：** 為了讓 BERT 理解輸入，需要提供三種類型的嵌入：
    
    - **符記嵌入 (Token Embeddings)：** 將輸入語句中的每個符記（字詞）轉換為向量表示。
    - **分段嵌入 (Segment Embeddings)：** 用於區分輸入中的不同段落（例如兩個句子），使用特殊的 `[SEP]` 符記分隔。
    - **位置嵌入 (Position Embeddings)：** 由於 Transformer 本身不具備處理順序的能力，位置嵌入用於編碼語句中每個符記的位置資訊。BERT 可以處理長度上限為 512 個符記的輸入序列。
- **BERT 的下游任務 (Downstream Tasks)：** 雖然 BERT 的基礎訓練是 MLM 和 NSP，但它可以被用於各種常見的自然語言處理任務，例如：
    
    - 單一語句分類 (Single Sentence Classification)
    - 語句組合分類 (Sentence Pair Classification)
    - 問答 (Question Answering)
    - 單一語句標記 (Single Sentence Tagging)

**總結來說，Transformer 模型通過引入注意力機制，徹底改變了自然語言處理領域，而 BERT 作為一種基於 Transformer 編碼器的預訓練模型，在多種 NLP 任務中都取得了巨大的成功。**