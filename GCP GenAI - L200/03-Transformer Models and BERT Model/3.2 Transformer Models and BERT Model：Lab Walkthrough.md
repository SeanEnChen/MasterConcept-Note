# 3.2 Transformer Models and BERT Model：Lab Walkthrough

好的，本研究室的目標是使用預先訓練的 BERT 模型進行文本分類，具體來說是對 IMDB 電影評論進行情感分析（正面或負面）。

**1. 設定與匯入 (Setup and Imports):**

影片中提到需要登入 Google Cloud 並開啟 Vertex AI Workbench，然後建立筆記本。對應到程式碼的開頭部分，會匯入必要的函式庫：

```Python
import os
import warnings
warnings.filterwarnings("ignore")
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import datetime
import shutil
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from google.cloud import aiplatform
from official.nlp import optimization  # to create AdamW optimizer
tf.get_logger().setLevel("ERROR")
```

確認 GPU 是否已連接，對應到這段程式碼：

```Python
print("Num GPUs Available: ", len(tf.config.list_physical_devices("GPU")))
```

這段程式碼會列出可用的 GPU 數量，確認是否可以使用 GPU 加速訓練。

**2. 下載 IMDB 資料集 (Download the IMDB dataset):**

影片中提到使用 IMDB 資料集進行訓練，並從特定網址下載。對應到這段程式碼：

```Python
url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
path = "/home/jupyter/"  # 設定下載路徑
dataset = tf.keras.utils.get_file(
    "aclImdb_v1.tar.gz", url, untar=True, cache_dir=path, cache_subdir=""
)
dataset_dir = os.path.join(os.path.dirname(dataset), "aclImdb")
train_dir = os.path.join(dataset_dir, "train")
remove_dir = os.path.join(train_dir, "unsup")
shutil.rmtree(remove_dir)
```

這段程式碼使用 `tf.keras.utils.get_file` 下載並解壓縮 IMDB 資料集，然後移除 `unsup` 這個未使用的資料夾，方便後續載入資料。

**3. 準備訓練、驗證和測試資料集 (Prepare Train, Validation, and Test Datasets):**

影片中提到將 20,000 個檔案用於訓練，5,000 個用於測試，並使用 `text_dataset_from_directory` 建立資料集。對應到這段程式碼：

```Python
AUTOTUNE = tf.data.AUTOTUNE
batch_size = 32
seed = 42
raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    path + "aclImdb/train",
    batch_size=batch_size,
    validation_split=0.2,
    subset="training",
    seed=seed,
)
class_names = raw_train_ds.class_names
train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = tf.keras.preprocessing.text_dataset_from_directory(
    path + "aclImdb/train",
    batch_size=batch_size,
    validation_split=0.2,
    subset="validation",
    seed=seed,
)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = tf.keras.preprocessing.text_dataset_from_directory(
    path + "aclImdb/test", batch_size=batch_size
)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)
```

這段程式碼使用 `tf.keras.preprocessing.text_dataset_from_directory` 從下載的資料夾中建立 TensorFlow 的 `Dataset` 物件。它將訓練集分割為訓練集和驗證集（80:20），並建立測試集。`cache()` 和 `prefetch()` 用於優化資料載入效能。

**4. 從 TensorFlow Hub 載入預先訓練的 BERT 模型 (Loading models from TensorFlow Hub):**

影片中提到從 TensorFlow Hub 載入預先訓練的 BERT 模型，並選擇使用較小的 BERT 模型。對應到這段程式碼：

```Python
tfhub_handle_encoder = (
    "https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1"
)
tfhub_handle_preprocess = (
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"
)
print(f"BERT model selected           : {tfhub_handle_encoder}")
print(f"Preprocess model auto-selected: {tfhub_handle_preprocess}")
```

這裡定義了 Small BERT 模型的 URL (`tfhub_handle_encoder`) 和對應的預處理模型的 URL (`tfhub_handle_preprocess`)。

**5. 預處理模型 (Preprocessing model):**

影片中展示了預處理模型的作用，將原始文字轉換為 BERT 模型可以接受的格式。對應到這段程式碼：

```Python
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
text_test = ["this is such an amazing movie!"]
text_preprocessed = bert_preprocess_model(text_test)
print(f"Keys       : {list(text_preprocessed.keys())}")
print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, :12]}')
print(f'Input Mask : {text_preprocessed["input_mask"][0, :12]}')
print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, :12]}')
```

這段程式碼載入預處理模型並對一個範例句子進行預處理。輸出展示了預處理後的結果，包括 `input_word_ids`（符記 ID）、`input_mask`（遮罩）和 `input_type_ids`（句子 ID）。

**6. 使用 BERT 模型 (Using the BERT model):**

影片中提到載入 BERT 模型並查看其輸出。對應到這段程式碼：

```Python
bert_model = hub.KerasLayer(tfhub_handle_encoder)
bert_results = bert_model(text_preprocessed)
print(f"Loaded BERT: {tfhub_handle_encoder}")
print(f'Pooled Outputs Shape:{bert_results["pooled_output"].shape}')
print(f'Pooled Outputs Values:{bert_results["pooled_output"][0, :12]}')
print(f'Sequence Outputs Shape:{bert_results["sequence_output"].shape}')
print(f'Sequence Outputs Values:{bert_results["sequence_output"][0, :12]}')
```

這段程式碼載入 BERT 模型並對預處理後的文字進行處理。輸出展示了 BERT 模型的輸出，包括 `pooled_output`（整個序列的表示）和 `sequence_output`（每個符記的上下文表示）。

**7. 定義分類模型 (Define your model):**

影片中說明了如何建立分類模型，將預處理層、BERT 模型、Dropout 層和 Dense 層結合起來。對應到這段程式碼：

```Python
def build_classifier_model(dropout_rate=0.1):
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name="text")
    preprocessing_layer = hub.KerasLayer(
        tfhub_handle_preprocess, name="preprocessing"
    )
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(
        tfhub_handle_encoder, trainable=True, name="BERT_encoder"
    )
    outputs = encoder(encoder_inputs)
    net = outputs["pooled_output"]
    net = tf.keras.layers.Dropout(dropout_rate)(net)
    net = tf.keras.layers.Dense(1, activation="sigmoid", name="classifier")(net)
    return tf.keras.Model(text_input, net)

dropout_rate = 0.15
classifier_model = build_classifier_model(dropout_rate)
bert_raw_result = classifier_model(tf.constant(text_test))
print(bert_raw_result)
```

這個函數 `build_classifier_model` 定義了分類模型的架構。它接收原始文本作為輸入，通過預處理層，然後輸入到 BERT 編碼器。使用 BERT 的 `pooled_output` 作為文本的整體表示，接著通過一個 Dropout 層進行正則化，最後通過一個 Dense 層（使用 sigmoid 激活函數）輸出情感分類的機率（正面或負面）。`trainable=True` 這個引數表示在訓練過程中會微調預先訓練的 BERT 模型的權重。

**8. 模型訓練 (Model training):**

影片中提到了損失函數（Binary CrossEntropy）、優化器（AdamW）和訓練過程。對應到這段程式碼：

```Python
loss = tf.keras.losses.BinaryCrossentropy()
metrics = tf.metrics.BinaryAccuracy()
epochs = 5
steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1 * num_train_steps)
init_lr = 3e-5
optimizer = optimization.create_optimizer(
    init_lr=init_lr,
    num_train_steps=num_train_steps,
    num_warmup_steps=num_warmup_steps,
    optimizer_type="adamw",
)
classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
print(f"Training model with {tfhub_handle_encoder}")
history = classifier_model.fit(
    x=train_ds, validation_data=val_ds, epochs=epochs
)
```

這部分定義了訓練所需的損失函數 (`BinaryCrossentropy`)、評估指標 (`BinaryAccuracy`)、訓練的輪數 (`epochs`) 和優化器 (`AdamW`，一種改進的 Adam 優化器，常用於微調 Transformer 模型）。然後使用 `classifier_model.compile` 編譯模型，並使用 `classifier_model.fit` 開始訓練，將訓練資料集 (`train_ds`) 和驗證資料集 (`val_ds`) 傳入。

**9. 評估模型 (Evaluate the model):**

影片中展示了如何評估模型在測試集上的表現。對應到這段程式碼：

```Python
loss, accuracy = classifier_model.evaluate(test_ds)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")
```

這段程式碼使用 `classifier_model.evaluate` 在測試資料集 (`test_ds`) 上評估模型的性能，輸出損失值和準確率。

**10. 繪製準確率和損失 (Plot the accuracy and loss over time):**

影片中提到可以繪製訓練期間準確率和損失的變化。對應到這段程式碼：

```Python
history_dict = history.history
print(history_dict.keys())

acc = history_dict["binary_accuracy"]
val_acc = history_dict["val_binary_accuracy"]
loss = history_dict["loss"]
val_loss = history_dict["val_loss"]

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()

plt.subplot(2, 1, 1)
# "bo" is for "blue dot"
plt.plot(epochs, loss, "r", label="Training loss")
# b is for "solid blue line"
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
# plt.xlabel('Epochs')
plt.ylabel("Loss")
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(epochs, acc, "r", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(loc="lower right")
```

這部分從 `history` 物件中提取訓練和驗證的準確率和損失，並使用 `matplotlib` 繪製圖表，以便視覺化模型的訓練過程。

**11. 匯出模型 (Export for inference):**

影片中說明了如何儲存訓練好的模型。對應到這段程式碼：

```Python
dataset_name = "imdb"
saved_model_path = "./{}_bert".format(dataset_name.replace("/", "_"))
TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
EXPORT_PATH = os.path.join(saved_model_path, TIMESTAMP)
classifier_model.save(EXPORT_PATH, include_optimizer=False)
```

這段程式碼定義了模型儲存的路徑 (`EXPORT_PATH`)，並使用 `classifier_model.save` 將訓練好的模型以 TensorFlow SavedModel 格式儲存到本地路徑。`include_optimizer=False` 表示不儲存優化器的狀態。

**12. 載入模型並進行推論 (Let's reload the model so you can try it side by side with the model that is still in memory):**

影片中展示了如何重新載入已儲存的模型並進行預測。對應到這段程式碼：

```Python
reloaded_model = tf.saved_model.load(EXPORT_PATH)

def print_my_examples(inputs, results):
    result_for_printing = [
        f"input: {inputs[i]:<30} : score: {results[i][0]:.6f}"
        for i in range(len(inputs))
    ]
    print(*result_for_printing, sep="\n")
    print()

examples = [
    "this is such an amazing movie!",
    "The movie was great!",
    "The movie was meh.",
    "The movie was okish.",
    "The movie was terrible...",
]
reloaded_results = reloaded_model(tf.constant(examples))
original_results = classifier_model(tf.constant(examples))
print("Results from the saved model:")
print_my_examples(examples, reloaded_results)
print("Results from the model in memory:")
print_my_examples(examples, original_results)
```

這部分載入了剛剛儲存的模型，並定義了一個輔助函數 `print_my_examples` 來格式化輸出預測結果。然後，它使用原始模型和重新載入的模型對一些範例句子進行預測，並打印結果。

**13. （可選）在 Vertex AI 上部署模型以獲得線上預測 ((Optional) Deploy your model on Vertex AI to get online predictions):**

影片的最後部分介紹了如何將模型部署到 Vertex AI 以便通過網路獲取預測結果，包括檢查模型簽名、將模型推送到 Vertex Model Registry、創建 Google Cloud Storage 值區、將模型複製到 GCS、將模型上傳到 Model Registry，以及將模型部署到端點。這部分對應到筆記本的後續程式碼，包括使用 `saved_model_cli` 檢查模型簽名，以及使用 `google.cloud.aiplatform` 庫將模型上傳和部署到 Vertex AI。最後，它展示了如何使用部署的端點進行線上預測，並在完成後清理端點以避免不必要的費用。

總之，這個筆記本逐步引導您使用預先訓練的 BERT 模型進行情感分析，從資料準備、模型載入、微調訓練到模型部署，涵蓋了使用 BERT 進行文本分類的完整流程。

```Python
!saved_model_cli show \
    --tag_set serve \
    --signature_def serving_default \
    --dir {EXPORT_PATH}

!find {EXPORT_PATH}
os.environ['EXPORT_PATH'] = EXPORT_PATH

TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
PROJECT = !gcloud config list --format 'value(core.project)' 2>/dev/null
PROJECT = PROJECT[0]
BUCKET = PROJECT
REGION = "us-central1"
MODEL_DISPLAYNAME = f"classification-bert-{TIMESTAMP}"

print(f"MODEL_DISPLAYNAME: {MODEL_DISPLAYNAME}")

# from https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers
SERVING_CONTAINER_IMAGE_URI = (
    "us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest"
)

os.environ["BUCKET"] = BUCKET
os.environ["REGION"] = REGION

%%bash
# Create GCS bucket if it doesn't exist already...
exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)

if [ -n "$exists" ]; then
    echo -e "Bucket exists, let's not recreate it."
else
    echo "Creating a new GCS bucket."
    gsutil mb -l ${REGION} gs://${BUCKET}
    echo "\nHere are your current buckets:"
    gsutil ls
fi

!gsutil cp -r $EXPORT_PATH gs://$BUCKET/$MODEL_DISPLAYNAME

uploaded_model = aiplatform.Model.upload(
    display_name=MODEL_DISPLAYNAME,
    artifact_uri=f"gs://{BUCKET}/{MODEL_DISPLAYNAME}",
    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,
)

MACHINE_TYPE = "n1-standard-4"

endpoint = uploaded_model.deploy(
    machine_type=MACHINE_TYPE,
    accelerator_type=None,
    accelerator_count=None,
)

instances = [
    {"text": ["I loved the movie and highly recomment it"]},
    {"text": ["It was an okay movie in my opinion"]},
    {"text": ["I hated the movie"]},
]

response = endpoint.predict(instances=instances)

print(" prediction:", response.predictions)
```

