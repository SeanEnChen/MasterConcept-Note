# 7.6 Lab-Getting Started with Vector Search and Embeddings

首先一開始先取得 Compute Engine、Vertex AI、Cloud Storage、BigQuery 的權限，以利後續存取需要。第一步從 BigQuery 中取得 Stack Overflow 的 Public 資料集，總共有 2300 萬筆資料，而這邊是使用 1000 筆資料。接下來就是要把每筆資料都 Embedding，所以用 vertexai 的 API 可以直接使用 Embedding Model，這邊是用 `text-embedding-004`，這個模型最多可以同時處理五筆資料。

> [!note]
> 這邊有提到說要使用那種相似度比對的方式是取決於模型的訓練原因，然後提到以 `text-embedding-004` 這個模型的話要使用 dot product distance。

接下來就是算出隨機一筆資料跟其他所有資料的 similarity 是多少，然後因為用的方式是 dot product distance，所以就是算兩個向量的內積，越大就代表越相似 (距離越近)。目前只有 1000 筆資料，所以用 np.dot 看起來還好，如果有好幾萬比資料的話，那就會差很多。換成 GCP 的 Vertex AI Search 可以在 10 毫秒內從 800w 個 Embedding 中找到最相近的句子或問題。所以接下來把 dataframe 的每筆 id 和 embedding 存進一個 json 檔，然後寫進 Cloud Storage 中。好了之後就是要建立 index 索引，是使用 Tree-AH 演算法，然後基於剛剛的 json 檔建立索引，然後使用 dot product distance 作為相似度的衡量標準。Index 建立完成後，在 Deploy 到 Index Endpoint 上，都是 Vertex AI 的功能。待 Deploy 完成之後，就可以隨機輸入內容，然後就可以去 Query，然後透過 similarity search 找到最相近的幾筆結果。