# 7.2 Generate Embeddings

**核心目標：解決如何將資料（以文字為例）編碼成能捕捉語意、又能被機器學習模型使用的數值表示法。**

**主要挑戰：**

1. **保留意義**：數字表示法需能反映字詞間的關係（如相似度、差異性）。
2. **適用於機器學習**：表示法需要是相對**密集 (dense)** 的向量或矩陣，避免高維度、**稀疏 (sparse)** 的表示（可能導致模型過度配適）。

**方法一：基本向量化 - One-Hot 編碼**

- **作法**：為詞彙表中的每個字詞創建一個獨立的向量。向量長度等於詞彙表大小，在對應字詞的位置填 1，其餘填 0。
- **舉例**：句子「一隻狗在追人」 -> 斷詞預處理後可能是「狗」、「追」、「人」。假設詞彙表有 6 個詞（狗, 追, 人, 我的, 貓, 跑步）。
    - `狗` 的 One-Hot 向量: `[1, 0, 0, 0, 0, 0]`
    - `追` 的 One-Hot 向量: `[0, 1, 0, 0, 0, 0]`
    - `人` 的 One-Hot 向量: `[0, 0, 1, 0, 0, 0]`
    - 整個句子會變成由這三個向量堆疊成的矩陣。
- **優點**：直觀、易於實作。
- **缺點**：
    1. **無法表達語意關係**：所有向量彼此間的距離都一樣，無法體現「狗」和「貓」比「狗」和「跑步」更相似。
    2. **高維度且稀疏**：向量維度等於詞彙量（可能達數萬維），且向量中絕大多數是 0。例如，一萬個詞的詞彙表，矩陣中 99.99% 是 0，浪費計算資源且易導致模型 Overfitting。

**方法二：進階方法 - 字詞嵌入 (Word Embeddings)**

- **核心概念**：將字詞表示為一個**低維度**、**密集**的向量。這個向量空間中的維度可以想像成捕捉了字詞的不同「面向」或「屬性」（類似用很多特徵描述一隻狗或「巴黎」這個詞）。
- **語意體現**：
    - **相似度**：意義相近的字詞，其向量在空間中的**距離也相近**（例如，「巴黎」和「東京」的向量距離近，但都離「蘋果」的向量很遠）。
    - **關係/類比**：向量之間的**差異（方向和距離）** 可以表示字詞間的關係。例如，「巴黎」向量減去「法國」向量，得到的差異向量，會和「東京」向量減去「日本」向量得到的差異向量很相似。這使得 `向量("巴黎") - 向量("法國") + 向量("日本") ≈ 向量("東京")` 這種數學類比成為可能。
- **特性**：
    - **低維度**：維度通常介於幾十到幾千之間，遠小於 One-Hot 編碼的數萬維。
    - **密集**：向量中的值大多非 0。
    - **捕捉語意**：向量本身及其相對位置蘊含了豐富的語意資訊。
    - **分散式表示法 (Distributed Representation)**：字詞的意義分散（分佈）在向量的各個維度中。
- **生成方式**：
    - 不是手動設定，而是透過**訓練神經網路**來學習這些向量值。
    - 常用算法/模型：Word2vec (Google), GloVe (Stanford), FastText (Facebook)。
    - 訓練通常需要大量資料和計算資源。
- **實際應用：使用預先訓練的模型**：
    - 多數情況下**不需要自己訓練**。
    - 可以直接呼叫 API 使用 Google 等機構提供的**預訓練嵌入模型**。
    - **使用步驟（概念）**：
        1. 匯入相關函式庫。
        2. 指定要使用的預訓練模型名稱（例如 Google 的 `textembedding-gecko` 或 `multimodalembedding`）。
        3. 定義要轉換的文字輸入。
        4. 呼叫 API 取得嵌入向量。
- **擴展至多模態**：同樣的嵌入思想也可用於圖片、音訊等其他媒體，使得不同類型的資料可以在同一個向量空間中進行比較（例如，文字「狗」的向量和圖片「狗」的向量在空間中會很接近）。

**總結：**

字詞嵌入技術解決了 One-Hot 編碼無法表達語意且稀疏高維的問題，生成了能捕捉豐富語意關係的低維度密集向量。實務上，通常直接使用預訓練好的嵌入模型 API 來將文字或其他資料轉換成嵌入向量，為後續的向量搜尋奠定基礎。