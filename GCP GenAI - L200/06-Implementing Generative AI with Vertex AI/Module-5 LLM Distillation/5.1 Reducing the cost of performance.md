# 6.5.1 Reducing the cost of performance

這段內容主要探討了大型語言模型（LLM）的發展趨勢、面臨的挑戰以及可能的解決方案，特別聚焦於模型壓縮技術中的「蒸餾」。

1. **LLM 的強勁表現與趨勢：**
    - 大型語言模型在各種任務上（如：排名、評分、內容生成）表現越來越好，品質顯著提升。
    - **舉例：** SuperGLUE 基準測試顯示，隨著模型規模增大（例如 T5 架構的模型），性能也隨之提升。像 LaMDA 這樣更大型的模型甚至在某些任務上超越了人類的表現。
    - LLM 不僅在標準任務上表現好，也展現出處理少樣本學習（Few-Shot Learning，只需少量範例即可學習）和需要大量世界知識的任務的能力。
	
2. **大型模型的挑戰：**
    - 雖然模型越大性能越好，但在實際應用中，較小的模型更受歡迎。
    - **舉例：** Llama 模型允許在普通的消費級硬體上運行類似 GPT-3 的大型語言模型，因此受到從業者的歡迎。
    - 主要挑戰在於如何高效且經濟地部署（Serving）這些大型模型，涉及成本和資源問題。
    - **舉例：** 從參數數量來看，模型規模差異巨大，從較小的 PaLM 系列到比其大 4000 倍的 LaMDA 等。雖然參數數量不完全等於計算成本或延遲，但它反映了規模的巨大差異。
    - 推論成本（Inference Cost）是大型模型普及的主要障礙。
    - 低延遲（Latency）要求對生成式推論（Generative Inference）尤其具有挑戰性。
	
3. **推論挑戰的技術原因（基於 Transformer 架構）：**
    - **巨大的記憶體佔用（Large Memory Footprint）：**
        - 推論時，模型的參數（Parameters）和中間狀態（Intermediate States）都需要載入記憶體。
        - **舉例：** 隨著輸入序列變長，由於 Transformer 中的注意力機制（Attention Mechanism），記憶體和計算成本會呈二次方增長。
    - **低平行化能力（Low Parallelizability）：**
        - 生成式推論通常以自迴歸（Auto-regressive）方式執行（一次生成一個詞/token），這使得解碼過程難以有效平行化處理，影響速度。
	
4. **解決方案：網路壓縮技術（Network Compression）：**
    - 雖然有改進硬體或模型架構的方法，但常見的是網路壓縮技術。
    - 基本假設：如果模型尺寸更小（參數更少或精度更低），則記憶體需求更低，運行速度更快。
    - **主要技術：**
        - **剪枝（Pruning）：** 移除模型中不重要的權重或連接，以減小模型大小，但可能保持模型容量不變。
        - **量化（Quantization）：** 將模型的權重從高精度（如 32 位元浮點數）近似為低精度（如 8 位元整數）。特別是訓練後量化（Post-Training Quantization, PTQ），通常成本較低，無需額外訓練。
        - **蒸餾（Distillation）：**
            - **核心思想：** 訓練一個較小的、便宜的「學生」（Student）模型，去模仿一個預訓練好的、大型的、昂貴的「老師」（Teacher）模型的行為，從而將老師的能力轉移給學生，達到加速推論的目的。
            - **舉例：** 可以將一個原始的全精度、密集（Dense）的大模型作為老師，訓練一個經過量化、剪枝或稀疏化（Sparsified）的小模型作為學生，使其在保持較高性能的同時，大幅降低部署成本。蒸餾可以與量化、剪枝等技術結合使用。
	
5. **壓縮技術的普遍問題：**
    - 無論是 Pruning、Quantization 還是 Distillation，通常都會面臨一個共同問題：性能損失（Performance Loss）。
    - **舉例：** 在蒸餾中，學生模型的準確率通常會比老師模型低，存在一個「師生準確率差距」（Teacher-Student Accuracy Gap）。如何縮小這個差距是研究的重點。