# 6.5.3 The knowledge distillation system

這是有關知識蒸餾（Knowledge Distillation）不同組成部分和技術的整理摘要，並盡量加上例子：

知識蒸餾的核心概念是將一個大型、複雜的「老師模型」（Teacher Model）所學習到的知識，遷移到一個較小、較輕量的「學生模型」（Student Model）上，讓學生模型在保持較小體積和較快速度的同時，也能達到接近老師模型的效能。這主要涉及三個關鍵部分：

1. **知識的種類 (Knowledge Types)**：指的是從老師模型轉移到學生模型的「知識」具體是什麼。
    - **基於回應 (Response-based)**：
        - **說明**：主要關注老師模型 **最後輸出層** 的結果。學生模型學習模仿老師模型的最終預測結果（例如分類機率分佈）。蒸餾的目標是最小化學生和老師在預測結果上的差異。
        - **例子**：在圖像分類任務中，老師模型對於一張貓的圖片，可能輸出 {貓: 0.9, 狗: 0.08, 其他: 0.02} 的機率分佈。學生模型就學習輸出與這個分佈盡可能相似的結果，而不僅僅是學習預測正確答案「貓」。這種「軟目標」(soft targets) 包含了類別間的相似性資訊（例如，模型覺得這張圖有點像狗，但更像貓）。
    - **基於特徵 (Feature-based)**：
        - **說明**：關注老師模型 **中間層** 所捕捉到的特徵資訊。學生模型學習模仿老師模型在處理資料時，其中間層產生的特徵表示（feature map 或 activations）。
        - **例子**：在人臉辨識任務中，老師模型的某個中間層可能學會了如何有效地提取「眼睛」、「鼻子」等部位的特徵。學生模型的對應中間層就學習產生與老師模型相似的特徵圖，即使它們的網路結構不完全相同。
    - **基於關係 (Relation-based)**：
        - **說明**：關注老師模型不同層或特徵圖之間的 **關係**。學生模型學習模仿這些關係，例如特徵圖之間的相似度、關聯性等。
        - **例子**：不直接模仿特徵圖本身，而是學習老師模型中不同神經元激活模式之間的相關性。例如，可以計算老師模型中某一層不同特徵圖之間的相似度矩陣，然後讓學生模型的對應層學習產生具有相似關係結構的特徵圖。
	
2. **蒸餾方案 (Distillation Schemes)**：指的是進行知識蒸餾的過程或策略。
    - **離線蒸餾 (Offline)**：
        - **說明**：這是最常見且相對容易實作的方式。先將老師模型在訓練資料集上**完整訓練好**並**固定**其參數，然後再利用這個預訓練好的老師模型來指導學生模型的訓練。
        - **例子**：先用大量的資料訓練一個非常大的圖像辨識模型（如 ResNet-152）直到收斂。然後凍結這個 ResNet-152 的權重，用它的預測結果或中間層特徵作為目標，來訓練一個小的模型（如 MobileNet）。
    - **線上蒸餾 (Online)**：
        - **說明**：老師和學生模型**同時進行訓練**。它們在訓練過程中互相學習、共同演進。這在沒有預訓練好的老師模型可用時很有用。
        - **例子**：想像老師和學生一起上課（看到訓練資料），老師邊學邊把心得（知識）教給學生，學生也邊學邊可能給老師一些反饋（雖然比較少見）。兩者可能同時從零開始訓練。
    - **自我蒸餾 (Self-distillation)**：
        - **說明**：老師和學生是**同一個模型**或**相同架構**。知識從模型的較深層轉移到較淺層，或者從訓練早期階段的模型（當作老師）轉移到訓練後期階段的模型（當作學生）。
        - **例子**：一個 10 層的網路，在訓練時，讓第 5 層的輸出學習模仿第 10 層（最終輸出層）的行為；或者，用模型在第 10 個訓練週期 (epoch) 時的狀態作為老師，指導第 20 個週期時的模型。
	
3. **師生架構 (Teacher-Student Architecture)**：指的是老師和學生模型的結構關係，這對知識轉移的效率至關重要。
    - **說明**：通常老師模型很大，學生模型很小，存在「模型容量差距」（capacity gap）。優化師生架構可以幫助縮小這個差距，讓知識更有效地轉移。
    - **不同架構關係舉例**：
        - **簡化/更淺版本**：學生模型是老師模型的簡化版，層數較少或每層的神經元較少。（例如：老師是 ResNet-50，學生是 ResNet-18）。
        - **量化版本 (Quantized)**：學生模型使用較低精度的權重（例如 8 位元整數 INT8），而老師模型使用較高精度（例如 32 位元浮點數 FP32）。
        - **相同架構**：在自我蒸餾中使用。
        - **優化架構**：學生模型採用了針對效率設計的操作（如 MobileNet 中的深度可分離卷積），即使老師模型是標準的卷積網路。

**蒸餾演算法舉例 (Distillation Algorithms Examples)**：

- **對抗式蒸餾 (Adversarial Distillation)**：
    - **概念**：借鑒生成對抗網路 (GANs) 的思想。引入一個「判別器」(Discriminator)，用來區分樣本是來自老師模型還是學生模型（基於輸出、特徵圖等）。學生模型的目標是「欺騙」判別器，使其無法分辨，從而讓學生的輸出更接近老師。
    - **例子**：判別器學習判斷一個特徵圖是來自預訓練好的老師模型（真實樣本）還是正在訓練的學生模型（生成樣本）。學生模型則努力生成讓判別器誤認為是來自老師模型的特徵圖。
- **多教師蒸餾 (Multi-Teacher Distillation)**：
    - **概念**：學生模型同時向**多個**不同的老師模型學習，相當於學習一個老師「專家團」的集體智慧。
    - **例子**：訓練三個不同架構但表現都很好的模型（例如 ResNet、Inception、DenseNet）作為老師，讓一個學生模型學習這三個老師預測結果的平均值或綜合特徵。
- **多步驟知識蒸餾 (Multi-Step Knowledge Distillation)**：
    - **概念**：引入一個中等大小的「助教模型」(Teacher Assistant, TA)。先由老師模型教助教模型，再由助教模型教學生模型，以此來逐步縮小師生之間過大的模型容量差距。
    - **例子**：老師是 ResNet-152，學生是 MobileNet。先用 ResNet-152 訓練一個 ResNet-50 作為助教，然後再用訓練好的 ResNet-50 作為老師來訓練 MobileNet。
- **基於圖的蒸餾 (Graph-based Distillation)**：
    - **概念**：將特徵之間的關係建模成圖結構，讓學生模型學習模仿老師模型中的這種圖關係。

總之，知識蒸餾是一個強大的技術框架，透過選擇不同的知識類型、蒸餾方案、師生架構和具體演算法，可以有效地將大型模型的智慧轉移到小型模型上，實現模型壓縮和加速，同時盡可能保持原有的性能。