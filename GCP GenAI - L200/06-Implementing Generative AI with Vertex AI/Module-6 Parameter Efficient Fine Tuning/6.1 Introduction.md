# 6.6.1 Introduction

**參數高效微調 (Parameter-Efficient Fine-Tuning, PEFT)**

1. **主要內容**：今天會集中討論一系列被稱為「參數高效微調」(Parameter-Efficient Fine-Tuning, PEFT) 的方法。這些方法旨在用更少的參數來微調大型預訓練模型，以適應特定任務。
2. **討論範圍**：
    - 會先**概略性地介紹幾種**不同的 PEFT 方法。
    - 接著會**深入探討**其中一種具體的演算法，稱為「**提示微調 (Prompt Tuning)**」。
3. **深入探討 Prompt Tuning 的原因**：
    - 目前 (根據文本所述時間點)，Vertex AI 提供的模型**「微調 API」(Tuning API)** 底層實作的**正是 Prompt Tuning** 技術。
    - 因此，理解 Prompt Tuning 的運作原理對於內部人員來說非常重要。
4. **重要提醒 (對外溝通)**：
    - **免責聲明**：在與客戶溝通時，當提到 Vertex AI 的「微調 API」時，**應該避免**直接說明其底層使用的是 Prompt Tuning 或任何其他的具體演算法名稱。
    - **原因**：底層使用的具體技術**未來可能會更換**。對客戶只需稱其為「微調 API」。
    - **例子**：就像我們介紹一款智慧手機的「拍照優化功能」，我們通常不會跟一般使用者詳細解釋背後是用了哪種 HDR 演算法或降噪技術，因為這些技術細節可能會更新，而且使用者關心的是最終效果。這裡的「微調 API」也是類似的概念。
5. **深入探討的目的 (對內理解)**：
    - 儘管不對外透露具體演算法，但**內部團隊需要深入理解 Prompt Tuning 的原理**。
    - **目的**：這樣才能為客戶提供關於如何**正確、有效地使用**這個「微調 API」的**最佳實踐和指導建議**。

總結來說，先廣泛介紹 PEFT，然後聚焦於目前 Vertex AI 微調 API 底層使用的 Prompt Tuning 技術，目的是為了讓內部人員充分理解其原理以便更好地指導客戶，但同時要注意在對外溝通時保持技術細節的抽象性，只提「微調 API」。