# 6.6.8 Walkthrough of example code

**重點：展示 Prompt Tuning 在程式碼層面的核心操作**

1. **背景**：這是在看 T5X (一個用於訓練大型 Transformer 模型的框架) 的開源代碼，了解 Prompt Tuning 是如何被實際加到模型中的。
    - **注意**：這未必是 Google Cloud Vertex AI 中模型的實際運行代碼（那些可能是用 PaLM 等模型實現的），但**概念上是相通的**。
	
2. **實現方式概覽**：
    - 從一個基礎的模型架構（例如 T5 的編碼器-解碼器模型）出發。
    - 創建一個專門用於 Prompt Tuning 的新模型類別 (Class)。
    - 在這個新模型的 **前向傳播 (Forward Pass)**  過程中（也就是 `call` 函數執行時），會呼叫一個名為 `embed_and_combine_inputs` 的函數。
    - 這個函數經過一系列呼叫，最終會執行到一個關鍵的函數，（根據描述，可能類似）名為 `concat_prompt`（意為串接提示）。
	
3. **核心操作：陣列串接 (Array Concatenation)**：
    - 在那個關鍵的 `concat_prompt` 函數裡，Prompt Tuning 的核心實現其實非常 **簡單**。
    - 它基本上就是把**兩個陣列 (Arrays)**（在 T5X 中通常是 JAX 陣列）**連接（串接 / Concatenate）** 在一起。
    - **第一個陣列**：就是我們之前訓練好的**軟提示嵌入向量** (`prompt`)。
    - **第二個陣列**：是原始輸入文本轉換成的**嵌入向量** (`inputs_embeds`)。
    - **操作**：把這兩個陣列**頭尾相接**，變成一個更長的陣列（序列）。
    - **舉例（概念上）**：如果軟提示有 10 個向量，輸入句子有 50 個向量，這個操作就是把它們變成一個包含 10 + 50 = 60 個向量的新序列，然後再把這個新序列送入模型的後續層進行處理。
	
4. **結論：實作上的簡潔性**：
    - 講者強調，在程式碼層面，實現 Prompt Tuning 最關鍵的這一步（即組合軟提示和輸入嵌入）的操作，真的就是這麼**直接和簡單**——僅僅是把兩個代表向量序列的陣列合併起來。

總之，這段是透過 T5X 的例子展示，Prompt Tuning 雖然概念巧妙，但在程式碼中與模型結合的核心步驟（把學習到的提示向量加到輸入向量前面）是相當直觀的陣列操作。