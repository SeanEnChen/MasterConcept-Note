# 6.4.2 What is a language model

**什麼是語言模型？**

語言模型是一種學習給定詞彙表中一組詞語或標記的聯合機率分佈的模型。這意味著，對於一個特定的標記序列（標記可以是完整的詞語，也可以是詞語的片段），模型會分配一個機率值來表示該序列存在的可能性。這個機率的基礎是詞彙表中的所有可能的標記。

- **詞彙表舉例：** 在T5和ULM等模型中，詞彙表通常使用稱為WordPiece的技術，包含約32,000個標記。這些標記可以是完整的詞語，也可以是詞語的片段。例如，“unbelievable”這個詞可能被拆分成“un”、“believ”、“able”三個標記。

語言模型會為詞彙表中任何特定順序的這些標記分配一個機率，表示這個順序出現的可能性。

**語言模型如何工作？**

從機率學的基本規則來看，這種聯合機率分佈也可以建模為條件機率的乘積。這正是預訓練的基礎。如果我們能夠構建一個模型，它可以根據一個詞語序列來預測下一個詞語，那麼這個模型實際上等同於可以對任何標記的聯合機率進行建模的模型。這也意味著，如果我們有這樣一個模型，我們就可以擁有一個可以根據一個詞語序列來預測另一個詞語序列的模型，也就是說，我們可以擁有一個可以響應提示的模型。

**模型在推理時如何工作？**

理解模型在推理時的工作方式非常重要，因為這有助於理解像 top-K、top-P 或 temperature 這樣的概念。許多語言模型都是僅包含解碼器（Decoder-only）的模型，但即使對於包含編碼器和解碼器（Encoder-Decoder）的模型，在生成序列的最後階段，解碼器的推理過程也是非常相似的。

假設我們有一個輸入序列 "all you"。我們希望模型生成這個序列的補全。我們將這個序列輸入到解碼器中。解碼器會生成下一個詞的條件機率分佈。

- **條件機率分佈舉例：** 假設我們的詞彙表有32,000個標記。對於每一個標記，模型都會給出在 "all you" 這兩個詞之後出現該標記的機率。例如，"need" 這個詞可能有一個較高的機率，而 "elephant" 這個詞的機率則會很低。

為了選擇下一個詞，我們需要從這個機率分佈中進行抽樣。這就是像 top-K 和 top-P 這樣的參數發揮作用的地方。

- **貪婪搜索 (Greedy Search)：** 一種簡單的方法是直接選擇機率最高的標記作為下一個詞。但這種方法可能會導致生成的文本非常單調，甚至難以理解。
- **Top-K 抽樣：** 我們首先選取機率最高的K個標記（例如，前10個）。然後，我們根據這10個標記各自的機率進行抽樣。機率越高的標記被選中的可能性就越大，但機率較低的標記仍然有機會被選中，這就引入了一定的隨機性。
- **Top-P（Nucleus）抽樣：** 我們選擇一個可變數量的標記，這些標記的機率之和達到一個特定的閾值P（例如，0.7）。然後，我們從這些選定的標記中進行抽樣。

假設我們通過抽樣選擇了詞語 "need"。下一步，我們將 "need" 這個標記添加到我們的序列中，現在我們有了 "all you need"。我們重複這個過程，將 "all you need" 輸入到解碼器中，它會生成下一個標記的機率分佈，然後我們再次進行抽樣。這個過程會一直重複，直到模型生成一個句子結束標記，或者直到我們達到預設的輸出長度限制。輸出長度的限制是我們在推理時配置的另一個參數。

這個過程是循序漸進的。在推理時，解碼器是按順序使用的。這就是為什麼生成文本的長度是影響大型語言模型延遲的一個重要因素。因為對於生成的每一個標記，模型都需要執行一次完整的前向傳播，如果我們要生成1000個標記，就需要執行1000次前向傳播，這既耗費計算資源，也需要時間。

另一種抽樣策略是束搜索（Beam Search），它會考慮多個可能的路徑，這可能會導致計算成本更高，耗時更長。

總結來說，我們了解了什麼是語言模型，它在推理時是如何工作的，以及訓練一個能夠預測下一個詞的模型如何使其能夠處理許多其他的條件機率問題，從而實現提示功能。