# 6.4.6 Transformer Architecture Variants

這段內容主要涵蓋了兩個重點：**大型語言模型常見的架構**以及**不同的自我監督預訓練目標（如何從無標註資料產生訓練範例）**。以下為您整理摘要：

**一、 常見的模型架構（Architectures）**

現代大型語言模型（LLM）大多基於 Transformer 架構，主要有以下幾種：

1. **編碼器-解碼器（Encoder-Decoder）：**
    - 這是原始 Transformer 的完整架構，包含獨立的編碼器（處理輸入）和解碼器（生成輸出）。
    - 適用於序列到序列任務（如翻譯、摘要）。
    - **具體模型範例：**
		- **T5 (Text-to-Text Transfer Transformer) (Google):** 一個代表性的模型，它將所有自然語言處理任務都視為「文本到文本」的轉換，並使用完整的 Encoder-Decoder 架構來實現。
		- **BART (Bidirectional and Auto-Regressive Transformers) (Meta AI / Facebook AI):** 也是使用完整 Encoder-Decoder 架構的模型，常用於文本生成任務（如摘要），預訓練時常結合去噪目標（例如，輸入被破壞的文本讓模型還原）。
	
2. **僅解碼器（Decoder-Only）：**
    - 只使用 Transformer 的解碼器部分。這是目前許多生成式 LLM（Generative LLM）的主流架構。
    - **主要類型：**
        - **因果解碼器 / 完全語言模型（Causal Decoder / Full Language Model）：**
            - 模型在預測下一個詞元（token）時，只能看到前面的詞元（不能看到後面的）。這是透過特定的自注意力遮罩（self-attention mask）實現的。
            - **範例：** 
	            - **GPT 系列 (OpenAI):** 如 GPT-2, GPT-3, GPT-4，這是此類架構最著名的例子。
	            - **LLaMA / Llama 2 / Llama 3 (Meta AI):** Meta 開發的開源（或開放權重）模型系列。
            - 訓練目標通常是預測序列中的下一個詞元。
	
3. **僅編碼器（Encoder-Only）：**
    - 只使用 Transformer 的編碼器部分。
    - 主要用於理解任務（NLU），而非生成任務（NLG）。
    - **範例：** 
	    - **BERT (Google):** 最經典的僅編碼器模型，開啟了預訓練模型的新時代，常用於文本分類、命名實體識別等理解任務。
	    - **領域特定 BERT:** 如 **BioBERT** (生物醫學), **SciBERT** (科學文獻), **ClinicalBERT** (臨床記錄) 等，在特定領域資料上進一步訓練或微調的 BERT 模型。
    - 在生成式 AI 或大型語言模型場景中較少直接使用（文本中提到 UL2 論文對此有討論）。

**重點：** 當聽到模型是「僅解碼器」或「因果解碼器」時，通常意味著它只有解碼器部分，並且很可能是透過預測下一個詞元的方式進行預訓練的。

**二、 自我監督預訓練目標（Self-supervised Objectives）**

這是指如何 **將無標註的文本序列轉換成模型可以學習的 (輸入, 目標) 訓練範例**，常見的方法有：

1. **（完全/因果）語言模型目標（Full / Causal Language Modeling Objective）：**
    - **方法：** 預測序列中的「下一個詞元」。將原始序列向右移動一個位置作為目標。
    - **範例：** 對於序列 `[A, B, C]`
        - 輸入 `[A]`，目標 `[B]`
        - 輸入 `[A, B]`，目標 `[C]`
        - 輸入 `[A, B, C]`，目標（可能是序列結束符號）
    - **效果：** 一個無標註序列可以生成多個訓練範例。這是訓練 GPT-3 這類模型的主要方式。
	
2. **前綴語言模型目標（Prefix Language Modeling Objective）：**
    - **方法：** 將序列分成「前綴（prefix）」和「目標（target）」兩部分。模型使用前綴來預測目標部分。與完全語言模型目標相比，像是將序列向右移動了「多個」詞元來產生目標。
    - **範例：** 對於序列 `[A, B, C, D]`
        - 可能將 `[A, B]` 作為前綴（輸入），`[C, D]` 作為目標（預測）
    - **差異：** 主要在於如何分割輸入和目標，以及哪些詞元用於計算損失。
	
3. **遮蔽語言模型 / 區段預測 / 去噪目標（Masked LM / Span Prediction / Denoising Objective）：**
    - **方法：** 在輸入序列中隨機「遮蔽（mask）」掉一些詞元或連續的「區段（span）」，或者以其他方式「破壞」輸入。模型的任務是預測或還原這些被遮蔽/破壞的部分。
    - **範例（遮蔽）：** 輸入 `[A, [MASK], C, [MASK], E]`，目標可能是 `[B, D]`。
    - **範例（區段/去噪，類似 T5）：** 輸入 `[A, X, E]` (其中 X 代表被破壞的 `[B, C, D]`)，目標可能是 `[X --> B, C, D]`。
    - **變體：** 存在許多變體（例如 BERT 的單詞遮蔽、T5 的區段替換）。
    - **UL2 論文觀點：** 該論文認為許多不同的自監督目標，本質上都可以看作是不同類型遮蔽/去噪目標的組合。但文本也提到，像 PaLM、ULM 這樣的模型實際使用的目標可能不完全符合這種解釋。

**核心思想：** 無論使用哪種自監督目標，其根本目的都是 **巧妙地將無標註的序列轉換為 (輸入, 目標) 的形式**，創造出一個「偽」監督學習問題，讓模型能夠從中學習語言的結構和模式。
