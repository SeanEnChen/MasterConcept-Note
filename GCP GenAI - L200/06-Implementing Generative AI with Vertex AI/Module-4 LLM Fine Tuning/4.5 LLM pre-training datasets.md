# 6.4.5 LLM pre-training datasets

這段內容主要介紹了用於 **預訓練（Pre-training）** 和 **指令微調（Instruction Tuning）** 的具體 **資料集（Corpora / Datasets）** 範例。以下為您整理摘要：

**一、 預訓練語料庫（Pre-training Corpora）**

預訓練階段使用的資料通常非常龐大，可以分為兩大類：

1. **通用語料庫（Generic Corpora）：** 包含廣泛主題和來源的文本。
    - **公開（Open Source）範例：**
        - **C4 (Colossal Clean Crawled Corpus):** 從 Common Crawl 網頁爬蟲資料中清洗和去重後得到的大型文本資料集。
        - **OpenWebText:** 類似於 OpenAI 用於訓練 GPT-2 的 WebText 資料集的開源版本，來源於網路連結。
        - **Wikipedia:** 維基百科，包含大量結構化和非結構化的知識。
        - **Books:** 各種類型的書籍文本。
        - **Common Crawl:** 龐大的原始網頁爬蟲資料庫（GPT-3 主要使用此資料來源）。
    - **私有（Proprietary）範例：**
        - **MassiveWeb / MassiveText (DeepMind):** DeepMind 公司內部使用的大規模、更多樣化的網頁和文本資料集，用於訓練其模型。
        - **Google 內部資料（用於 PaLM）:**
            - 社交媒體對話（Social media conversations）
            - 經過濾的網頁（Filtered web pages）
            - 這些是 Google 私有的，未公開。
	
2. **特定領域語料庫（Domain-specific Corpora）：** 專注於特定專業領域的文本。
    - **公開範例：**
        - **MIMIC-III:** 關於重症監護（Healthcare）的醫療記錄資料庫。
        - **RealNews:** 新聞文章集合。
        - **S2ORC (Semantic Scholar Open Research Corpus):** 大量的科學研究論文（Scientific documents）。

**模型訓練的資料組合範例：**

- **GPT-3:** 主要基於 **Common Crawl**，並輔以 **Wikipedia** 和 **Books** 資料集。
- **DeepMind 的模型:** 常使用其私有的 **MassiveWeb/MassiveText**，追求資料的多元性。
- **PaLM (Google):** 使用了大量的 **Google 私有資料**（如社交媒體對話、過濾網頁），也混合了部分公開資料集。這顯示大型模型常混合使用公開和私有資料。

**二、 指令微調資料集（Instruction Tuning Datasets）**

指令微調是在預訓練之後，用來讓模型更好地理解和遵循指令的階段。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1743416010000odb63l.png)

- **範例：FLAN 2021 資料集 (來自 FLAN 2022 論文)**
    - **來源：** 這 **不是** 一個全新的資料集，而是將許多 **現有的、標準的、有標註的監督式 NLP 資料集**（Supervised NLP datasets）**集合** 起來。
    - **組成：**
        - 包含 **自然語言理解（NLU）** 任務的資料集（例如：情感分析、問題回答、文本分類）。(文本中提到圖示左側的藍色部分)
        - 包含 **自然語言生成（NLG）** 任務的資料集（例如：摘要、翻譯）。(文本中提到圖示右側的部分)
    - **轉換方式：** 這些監督式資料集會經過 **特殊的格式轉換**（文本中提到稍後會解釋），變成「指令」的形式，用來微調模型。
    - **目的：** 透過這種方式微調，可以讓經過自我監督預訓練的模型，更擅長進行 **情境學習（In-context learning，即根據提示中給出的少量範例來完成任務）**，以及在某些情況下直接遵循監督式的指令。

**總結：**

模型的訓練通常涉及混合使用多種來源的資料。預訓練階段會用大量通用或特定領域的文本（可能是公開的或私有的）來建立基礎語言能力。之後，可以利用像 FLAN 這樣由多個監督式任務轉換而來的指令微調資料集，進一步提升模型遵循指令和解決具體任務的能力。