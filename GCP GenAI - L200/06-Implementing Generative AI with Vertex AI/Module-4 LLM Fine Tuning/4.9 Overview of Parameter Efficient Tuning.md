# 6.4.9 Overview of Parameter Efficient Tuning

這段內容總結了前面討論的預訓練和完全微調，並重點介紹了 **參數高效微調（Parameter-Efficient Fine-tuning, PEFT）** 這一大類技術。以下為您整理摘要：

**回顧重點：**

- **預訓練（Pre-training）：** 核心是**自我監督學習（Self-supervision）**，但可能是一個多步驟過程，包含像 **指令微調（Instruction Tuning）** 這樣的技術。
- **完全微調（Full Fine-tuning）：** 涉及到修改預訓練模型 **所有** 的參數，甚至可能增加更多參數。

**介紹：參數高效微調（Parameter-Efficient Fine-tuning, PEFT）**

1. **核心思想：**
    - 與完全微調不同，PEFT 在調整模型以適應下游任務時，**只修改（訓練）一小部分參數**。
    - 預訓練模型的 **絕大部分參數保持凍結（Frozen）** 不變。
	
2. **動機與優點：**
    - **避免過度擬合（Overfitting）：** 因為只調整少量參數來適應單一特定任務，可以減少模型在新任務的小數據集上過度擬合的風險。
    - **避免災難性遺忘（Catastrophic Forgetting）：** 完全微調整個模型時，模型可能會忘記預訓練階段學到的通用知識。PEFT 因為大部分模型被凍結，能更好地保留通用能力。
    - **數據效率：** 即使下游任務的有標註範例較少，PEFT 也能有效運作。
    - **知識壓縮：** 這種方法可以看作是強迫模型將特定任務的知識「壓縮」到那一小部分可訓練的參數中。
	
3. **多樣化的技術（PEFT 是一個廣泛的集合）：**
    - **注入額外層（Inject additional layers）：** 在現有模型架構中插入一些新的、小型可訓練層（例如 Adapter Tuning）。
    - **修改權重矩陣（Modify weight matrices）：** 不直接修改原始權重，而是在旁邊添加一些小的、可訓練的數值或矩陣，訓練時只更新這些添加的部分（例如 LoRA - Low-Rank Adaptation）。
    - **附加前綴（Append a prefix - 嵌入層面）：** 在輸入序列的嵌入（embedding）前面加上一小段可訓練的「軟提示（soft prompt）」向量。模型本身的權重不變。
        - **範例：** Prompt Tuning、Prefix Tuning。
    - **只改變部分現有參數（Change a subset of existing parameters）：** 只更新模型中特定類型的一小部分參數。
        - **範例：** BitFit 方法，它只更新模型中的偏置項（bias terms）。
	
4. **效率體現（參數規模對比）：**
    - PEFT 大大減少了需要訓練的參數數量。
    - **範例：** 使用 Prompt Tuning 時，可能只需要訓練**幾萬到幾十萬**個參數。
    - **對比：** 而完全微調一個大型模型，則需要訓練**數十億（billions）**個參數。

**總結：PEFT 提供了一系列方法，允許我們用更少的計算資源、更少的數據、更低的風險（過擬合、災難性遺忘）來有效地將大型預訓練模型適應到特定的下游任務，其核心是凍結大部分模型參數，僅訓練一小部分新增或選定的參數。**