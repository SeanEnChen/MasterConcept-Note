# 8.4.1 Using RAG for longer memory in conversations

**核心主題：深入探討檢索增強生成 (RAG) 的應用，特別是如何解決 LLM 的記憶限制問題，以及在 Google Cloud 上實作 RAG 的不同方法比較。**

**RAG 應用回顧與擴展：**

1. **解決幻覺 (Fact-Checking Recap)**：
    - **問題**：LLM 因知識僅限於訓練資料和提示，可能捏造事實。
    - **RAG 解法**：透過向量搜尋即時檢索相關資訊，將其注入提示中，讓 LLM 基於提供的上下文生成更真實的回應。（再次提及研究論文聊天機器人的例子）。
2. **賦予聊天機器人記憶 (Chatbot Memory)**：
    - **問題**：LLM 本身是**無狀態 (stateless)** 的，會忘記之前的對話內容。若將全部歷史對話放入提示，很快會超過模型的上下文長度限制。
    - **舉例**：使用者詢問先前討論過的銷售目標，聊天機器人表示不記得。
    - **RAG 解法**：
        1. 將過去的對話（使用者提問+機器人回答）**分塊**並生成**嵌入向量**，儲存到向量搜尋資料庫中。
        2. 當使用者提出新問題時，系統（或 LLM 本身）判斷是否需要參考歷史對話。
        3. 若是，則將**當前問題**轉換為查詢向量，去**搜尋對話歷史的嵌入向量**。
        4. 檢索到最相關的幾段**過去對話片段**。
        5. 將「**當前問題**」與「**檢索到的歷史對話片段**」**組合**成增強提示，送給 LLM。
        6. LLM 根據當前問題和回溯的歷史上下文，生成連貫的回應。
    - **優點**：為 LLM 提供了相關的「記憶」，使其能夠無縫地接續先前的對話，無需無限的上下文視窗。

**在 Google Cloud 上實作 RAG 的流程 (DIY - Do It Yourself 方式)：**

1. **資料準備與嵌入生成**：
    - **來源**：文件、對話記錄等。
    - **分塊 (Chunking)**：將大型文件或長對話切成較小的、有意義的片段（因為嵌入模型有單次處理的 Token 限制）。
    - **生成嵌入**：為每一個「塊」生成嵌入向量（例如使用 Vertex AI Embedding API）。
2. **儲存與索引 (使用 Vertex AI Vector Search)**：
    - 將生成的嵌入向量載入到 **Vertex AI Vector Search** 中建立索引。
    - **加入元數據 (Metadata)**：在儲存嵌入時，同時儲存相關的元數據（例如來源文件 ID、區塊編號、關鍵字、時間戳等），通常以鍵值對 (Key-Value Pair) 形式儲存。這對於後續的**過濾 (Filtering)** 非常重要。
3. **檢索 (Retrieval)**：
    - 使用者提問。
    - **嵌入查詢**：將使用者的問題文字轉換成嵌入向量。
    - **執行向量搜尋**：將查詢向量發送到 Vector Search。**可同時帶上元數據過濾條件**（例如，只搜尋來源為 'manual_v2' 的文件塊）。
    - **獲得結果**：Vector Search 回傳最相似的幾個區塊的 ID 和嵌入向量（以及距離分數）。
4. **增強與生成 (Augmentation & Generation)**：
    - **提取內容**：根據回傳的 ID，取得對應區塊的原始文字內容。
    - **建構提示**：將「使用者的原始問題」、「檢索到的區塊文字內容」以及可能的「額外指示」（例如："請根據以下資訊回答..."）組合成最終的「增強提示」。
    - **呼叫 LLM**：將這個增強提示傳送給 LLM（例如 PaLM 或 Gemini 模型）。
    - **生成回應**：LLM 基於提示中提供的所有資訊（原始問題+檢索到的上下文）生成最終答案。

**實作 RAG 的方案比較：DIY vs. Vertex AI Agent Builder (開箱即用方案)**

| 特性            | DIY (自建方案) (Embeddings + Vector Search + [Document AI])                                      | Vertex AI Agent Builder (開箱即用) (整合 Dialogflow CX Gen AI 功能)                                  |
| :------------ | :------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------- |
| **建立速度/易用性**  | 較慢（可能需數天），較複雜                                                                                | **非常快**（可能數分鐘），透過 UI 或 API 即可完成，自動化程度高                                                       |
| **彈性/控制權**    | **高**                                                                                        | **較低**                                                                                       |
| **資料更新/新鮮度**  | 可支援**串流索引 (Streaming Indexing)**，即時性好                                                        | 可能**僅支援批次刷新 (Batch Refresh)**（截至錄製時），即時性較差                                                   |
| **文件解析能力**    | 需自行實作或整合 **Document AI**。Document AI 功能強大（OCR, 版面, 表格, 圖像, 自訂解析器），可處理複雜文件，可能產生**更高品質**的區塊/嵌入 | 內建解析，支援常見格式 (網站, BigQuery, HTML, PDF-文字, TXT; PPTX/DOCX 預覽)，但解析能力相對**有限**，不支援圖像/視訊/音訊（截至錄製時） |
| **提示工程/風格控制** | **完全控制**，可自訂提示模板 (Prompt Template)                                                           | 提供提示模板功能（預覽狀態），可設定 LLM 回應語氣                                                                  |
| **發展趨勢**      | -                                                                                            | 從「黑盒子」逐漸走向更可配置的「白盒子」平台，未來可能提供更多自訂選項（如自訂嵌入）                                                   |

**總結：**

RAG 是利用向量搜尋來克服 LLM 在事實準確性和記憶連續性方面限制的強大模式。在 Google Cloud 上，開發者可以選擇：

- **DIY 方式**：使用 Vertex AI Embedding API、Vector Search 以及可能的 Document AI，獲得最大的客製化彈性和對資料新鮮度、解析品質的控制。
- **Vertex AI Agent Builder**：一個開箱即用的方案，能極大地加速 RAG 應用（如聊天機器人）的開發速度，但彈性相對受限。