# 8.4.5 Lab - Multimodal Retrieval Augmented Generation (RAG) using the Gemini API in Vertex AI

這個跟前一個 Lab 很像，只是是 MultiModel 的練習，使用的模型是 Gemini 2.0 Flash，然後一樣從 Lab 給連結中拿到 Data，然後 Data 是 PDF 檔，是透過 `get_document_metadata` 來取得 Image 和 Text 的資料。他把資料做一些轉換或處理，像是 Text 的部分的話，如果文字太多就會切 chunk，然後也會把 Text 的內容進行 Embedding；圖片的部分也是，如果 PDF 檔裡面有圖片的話，因為有給他 prompt 加上 Gemini 是 MultiModel，所以會給圖片一個 description，並把 desc 進行 Embedding，以利後續 Query 的部分。

```Python
# Specify the PDF folder with multiple PDF

# pdf_folder_path = "/content/data/" # if running in Google Colab/Colab Enterprise
pdf_folder_path = "data/"  # if running in Vertex AI Workbench.

# Specify the image description prompt. Change it
image_description_prompt = """Explain what is going on in the image.
If it's a table, extract all elements of the table.
If it's a graph, explain the findings in the graph.
Do not include any numbers that are not mentioned in the image.
"""

# Extract text and image metadata from the PDF document
text_metadata_df, image_metadata_df = get_document_metadata(
    multimodal_model,  # we are passing Gemini 2.0 Flash model
    pdf_folder_path,
    image_save_dir="images",
    image_description_prompt=image_description_prompt,
    embedding_size=1408,
    add_sleep_after_page = True,
    sleep_time_after_page = 5,
    # generation_config = # see next cell
    # safety_settings =  # see next cell
)

# # Parameters for Gemini API call.
# # reference for parameters: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini

# generation_config=  GenerationConfig(temperature=0.2, max_output_tokens=2048)

# # Set the safety settings if Gemini is blocking your content or you are facing "ValueError("Content has no parts")" error or "Exception occurred" in your data.
# # ref for settings and thresholds: https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes

# safety_settings = {
#                   HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
#                   HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
#                   HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
#                   HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
#                   }

# # You can also pass parameters and safety_setting to "get_gemini_response" function

print("\n\n --- Completed processing. ---")
```

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1743682476000lgvwd7.png)

如上圖可以看到回傳的結果是 DataFrame 的格式。

接下來就是進行 Query 的部分，很簡單的方式就可以呼叫方法。

```Python
# Matching user text query with "chunk_embedding" to find relevant chunks.
matching_results_text = get_similar_text_from_query(
    query,
    text_metadata_df,
    column_name="text_embedding_chunk",
    top_n=3,
    chunk_text=True,
)

# Print the matched text citations
print_text_to_text_citation(matching_results_text, print_top=False, chunk_text=True)

# Citation 1: Matched text: 
# score:  0.76
# file_name:  google-10k-sample-part2.pdf
# page_number:  4
# chunk_number:  1
# chunk_text:  liquidation and dividend rights are identical, the undistributed earnings are
# allocated on a proportionate basis.
# In the years ended December 31, 2019, 2020 and 2021, the net income per
# share amounts are the same for Class A, Class B, and Class C stock because
# the holders of each class are entitled to equal per share dividends or distributions
# in liquidation in accordance with the Amended and Restated Certificate of
# Incorporation of Alphabet Inc.
# The following tables set forth the computation of basic and diluted net income per
# share of Class A, Class B, and Class C stock (in millions, except share amounts
# which are reflected in thousands and per share amounts):
```

但這個只有 retrieve 的結果，把這些 retrieve 的結果放到要給 LLM 的 Prompt 裡面，請他根據這些內容回答 User 的問題。

```Python
print("\n **** Result: ***** \n")

# All relevant text chunk found across documents based on user query
context = "\n".join(
    [value["chunk_text"] for key, value in matching_results_text.items()]
)

instruction = f"""Answer the question with the given context.
If the information is not available in the context, just return "not available in the context".
Question: {query}
Context: {context}
Answer:
"""

# Prepare the model input
model_input = instruction

# Generate Gemini response with streaming output
get_gemini_response(
    text_model,  # we are passing Gemini 2.0 Flash
    model_input=model_input,
    stream=True,
    generation_config=GenerationConfig(temperature=0.2),
)
```

搜尋圖片的部分也一樣，透過搜尋 image description 的欄位，可以找到符合 User Query 的圖片。

```Python
matching_results_image = get_similar_image_from_query(
    text_metadata_df,
    image_metadata_df,
    query=query,
    column_name="text_embedding_from_image_description",  # Use image description text embedding
    image_emb=False,  # Use text embedding instead of image embedding
    top_n=3,
    embedding_size=1408,
)

# Markdown(print_text_to_image_citation(matching_results_image, print_top=True))
print("\n **** Result: ***** \n")

# Display the top matching image
display(matching_results_image[0]["image_object"])
```

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1743683088000rd81b6.png)

上面一樣只是 retrieve 的部分，找到符合 User 的那個圖片，接下來一樣把這些東西放入給 LLM 的 Prompt，實現 RAG 這個技術。

```Python
print("\n **** Result: ***** \n")

# All relevant text chunk found across documents based on user query
context = f"""Image: {matching_results_image[0]['image_object']}
Description: {matching_results_image[0]['image_description']}
"""

instruction = f"""Answer the question in JSON format with the given context of Image and its Description. Only include value.
Question: {query}
Context: {context}
Answer:
"""

# Prepare the model input
model_input = instruction

# Generate Gemini response with streaming output
Markdown(
    get_gemini_response(
        multimodal_model_flash,  # we are passing Gemini 2.0 Flash
        model_input=model_input,
        stream=True,
        generation_config=GenerationConfig(temperature=1),
    )
)
```

接下來要實現 image Query 的部分，也就是透過圖片的 Embedding 比對，來實現透過輸入的圖片找到最相近的圖片結果。

```Python
# Search for Similar Images Based on Input Image and Image Embedding

matching_results_image = get_similar_image_from_query(
    text_metadata_df,
    image_metadata_df,
    query=query,  # Use query text for additional filtering (optional)
    column_name="mm_embedding_from_img_only",  # Use image embedding for similarity calculation
    image_emb=True,
    image_query_path=image_query_path,  # Use input image for similarity calculation
    top_n=3,  # Retrieve top 3 matching images
    embedding_size=1408,  # Use embedding size of 1408
)

print("\n **** Result: ***** \n")

# Display the Top Matching Image
display(
    matching_results_image[0]["image_object"]
)  # Display the top matching image object (Pillow Image)
```

後面也做到可以同時問 Gemini 很多問題，關於圖片也好、圖片描述也好

```Python
matching_results_image_query_1 = get_similar_image_from_query(
    text_metadata_df,
    image_metadata_df,
    query="Show me all the graphs that shows Google Class A cumulative 5-year total return",
    column_name="text_embedding_from_image_description",  # Use image description text embedding # mm_embedding_from_img_only text_embedding_from_image_description
    image_emb=False,  # Use text embedding instead of image embedding
    top_n=3,
    embedding_size=1408,
)

prompt = f""" Instructions: Compare the images and the Gemini extracted text provided as Context: to answer Question:
Make sure to think thoroughly before answering the question and put the necessary steps to arrive at the answer in bullet points for easy explainability.

Context:
Image_1: {matching_results_image_query_1[0]["image_object"]}
gemini_extracted_text_1: {matching_results_image_query_1[0]['image_description']}
Image_2: {matching_results_image_query_1[1]["image_object"]}
gemini_extracted_text_2: {matching_results_image_query_1[2]['image_description']}

Question:
 - Key findings of Class A share?
 - What are the critical differences between the graphs for Class A Share?
 - What are the key findings of Class A shares concerning the S&P 500?
 - Which index best matches Class A share performance closely where Google is not already a part? Explain the reasoning.
 - Identify key chart patterns in both graphs.
 - Which index best matches Class A share performance closely where Google is not already a part? Explain the reasoning.
"""

# Generate Gemini response with streaming output
rich_Markdown(
    get_gemini_response(
        multimodal_model,  # we are passing Gemini 2.0 Flash
        model_input=[prompt],
        stream=True,
        generation_config=GenerationConfig(temperature=1),
    )
)
```

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/17436835580000x3p6m.png)
![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1743683576000u0yex9.png)

Lab 最後有給一個 Multimodal retrieval augmented generation (RAG) 的步驟

- **Step 1:** The user gives a query in text format where the expected information is available in the document and is embedded in images and text.
- **Step 2:** Find all text chunks from the pages in the documents using a method similar to the one you explored in `Text Search`.
- **Step 3:** Find all similar images from the pages based on the user query matched with `image_description` using a method identical to the one you explored in `Image Search`.
- **Step 4:** Combine all similar text and images found in steps 2 and 3 as `context_text` and `context_images`.
- **Step 5:** With the help of Gemini, we can pass the user query with text and image context found in steps 2 & 3. You can also add a specific instruction the model should remember while answering the user query.
- **Step 6:** Gemini produces the answer, and you can print the citations to check all relevant text and images used to address the query.

