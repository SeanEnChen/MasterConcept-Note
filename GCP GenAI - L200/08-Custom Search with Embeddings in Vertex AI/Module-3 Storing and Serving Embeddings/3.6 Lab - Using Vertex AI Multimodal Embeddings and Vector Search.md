# 8.3.6 Lab - Using Vertex AI Multimodal Embeddings and Vector Search

一開始一樣先 Enable Vertex AI API 和 Service Networking API，然後開啟 Jupyter Lab，安裝一些會用到的套件，然後建立一個 GCS 的 Bucket。然後從 DiffusionDB 取得圖像資料，並且解壓縮全部存到某個資料夾中，然後用 Cloud Vision 執行安全搜尋檢測。檢測完之後，創建一個 `EmbeddingPredictionClient`，它封裝了調用 Embedding API 的邏輯。

---

### **Task 1. Enable APIs (啟用 API)**

- **目的：** 這個任務的目的是在您的 Google Cloud 專案中啟用實驗所需的特定服務 API。在 Google Cloud 中，您必須先啟用一個服務的 API，才能開始使用該服務的功能。
- **說明：**
    - **Vertex AI API：** 這是使用 Vertex AI 平台所有功能的基礎，包括我們將要用到的多模態嵌入模型 (Multimodal Embeddings) 和向量搜索 (Vector Search)。您需要透過 Google Cloud 控制台的搜尋列找到 "Vertex AI API" 並啟用它。
    - **Service Networking API：** Vertex AI 向量搜索需要一個安全的網路環境來運作，通常是透過 VPC 網路對等互連 (VPC Network Peering)。啟用 Service Networking API 是建立和管理這些網路連接的前提。同樣地，您需要透過控制台搜尋 "Service Networking API" 並啟用。
    - **介面操作：** 這部分的步驟都是在 Google Cloud Console (雲端主控台) 的圖形介面中完成，主要是點擊按鈕來啟用服務，因此沒有直接對應的程式碼區塊。

---

### **Task 2. Open a Jupyter notebook in Vertex AI Workbench (在 Vertex AI Workbench 中開啟 Jupyter 筆記本)**

- **目的：** 準備執行實驗程式碼的環境。Vertex AI Workbench 提供了一個代管的 JupyterLab 環境，預裝了許多數據科學和機器學習相關的函式庫，方便我們撰寫和執行 Python 程式碼。
- **說明：**
    - **導航至 Workbench：** 您需要透過 Cloud Console 找到 Vertex AI Workbench 服務。
    - **開啟 JupyterLab：** 在 Workbench 介面中，找到名為 `generative-ai-jupyterlab` 的執行個體 (Instance)，並點擊 "Open JupyterLab"。這會在瀏覽器新分頁中開啟 JupyterLab 介面。
    - **建立新筆記本：** 在 JupyterLab 的啟動器 (Launcher) 中，點擊 "Python 3" 圖示，建立一個新的空白 Jupyter 筆記本檔案 (.ipynb)。所有的實驗程式碼都將在這個筆記本中執行。
    - **介面操作：** 這部分也是在 Cloud Console 和 JupyterLab 介面中操作，沒有直接對應的程式碼。

---

### **Task 3. Set up the Jupyter notebook environment (設定 Jupyter 筆記本環境)**

- **目的：** 安裝實驗所需的 Python 函式庫，並設定一些基本的環境變數，例如您的 Google Cloud 專案 ID 和區域。
    
- **程式碼 1: 安裝套件 (`pip3 install ...`)**
    
    ```Python
    # Install the packages
    ! pip3 install --upgrade google-cloud-aiplatform \
                             google-cloud-storage
    ```
    
    - **作用：** 使用 `pip` (Python 的套件安裝程式) 來安裝或更新兩個重要的 Google Cloud Python 客戶端函式庫。
    - `google-cloud-aiplatform`: 這個函式庫讓您能透過 Python 程式碼與 Vertex AI 服務互動 (例如呼叫嵌入模型、建立向量索引等)。`--upgrade` 確保安裝的是最新版本。
    - `google-cloud-storage`: 這個函式庫用於與 Google Cloud Storage (GCS) 互動，我們將用它來讀寫儲存在 GCS 儲存桶 (Bucket) 中的資料 (例如嵌入向量檔案)。
    - `!` 符號：在 Jupyter Notebook 中，`!` 符號允許您直接執行 shell (終端機) 命令。
- **程式碼 2: 安裝 Cloud Vision 套件 (`pip install google-cloud-vision`)**
    
    ```Python
    # Install the packages
    ! pip install google-cloud-vision
    ```
    
    - **作用：** 安裝 Google Cloud Vision API 的 Python 客戶端函式庫。
    - **目的：** 在後續步驟 (Task 5) 中，我們將使用 Cloud Vision API 的 SafeSearch 功能來過濾掉資料集中可能包含不適當內容 (如暴力、成人內容) 的圖片。
- **程式碼 3: 重啟核心 (`import IPython...`)**
    
    ```Python
    # Automatically restart kernel after installs so that your environment can access the new packages
    import IPython
    
    app = IPython.Application.instance()
    app.kernel.do_shutdown(True)
    ```
    
    - **作用：** 在安裝完新的 Python 函式庫後，Jupyter Notebook 的「核心」(Kernel，實際執行程式碼的後端程序) 需要重新啟動，才能載入並使用這些新安裝的函式庫。這段程式碼會自動觸發核心重啟。
- **程式碼 4: 設定環境變數 (`PROJECT = ...`, `REGION = ...`)**
    
    ```Python
    PROJECT = !gcloud config get-value project
    PROJECT_ID = PROJECT[0]
    REGION = "<var>Lab Default Region</var>" # 您需要將 <var>Lab Default Region</var> 替換成實驗指定的區域，例如 'us-central1'
    ```
    
    - **作用：** 設定後續程式碼會用到的變數。
    - `!gcloud config get-value project`: 執行 `gcloud` 命令 (Google Cloud 的命令列工具) 來獲取當前設定的 Google Cloud 專案 ID。結果會被存入 `PROJECT` 列表。
    - `PROJECT_ID = PROJECT[0]`: 從 `PROJECT` 列表中取出專案 ID 字串，存入 `PROJECT_ID` 變數。
    - `REGION = ...`: 設定 Google Cloud 的區域 (Region)。許多 Google Cloud 服務都是區域性的，您需要指定在哪個區域操作。您需要將 `<var>Lab Default Region</var>` 手動替換成 Qwiklabs 實驗介面提供的預設區域 (例如 `us-central1`)。

---

### **Task 4. Prepare the data (準備資料)**

- **目的：** 下載並整理實驗要用的資料集。這個實驗使用 DiffusionDB 資料集，其中包含由 AI (Stable Diffusion) 生成的圖片以及生成這些圖片所使用的文字提示 (prompt)。
    
- **程式碼 1: 克隆 DiffusionDB 儲存庫 (`git clone ...`)**
    
    ```Python
    ! git clone https://github.com/poloclub/diffusiondb
    ```
    
    - **作用：** 使用 `git` 命令從 GitHub 下載 DiffusionDB 專案的程式碼儲存庫。這個儲存庫裡包含了下載資料集所需的腳本 (script)。
- **程式碼 2: 安裝下載腳本的依賴項 (`pip install -r ...`)**
    
    ```Python
    ! pip install -r diffusiondb/requirements.txt
    ```
    
    - **作用：** DiffusionDB 的下載腳本依賴一些特定的 Python 函式庫。這個命令會讀取 `diffusiondb` 資料夾下的 `requirements.txt` 文件，並使用 `pip` 安裝所有列出的依賴函式庫。
- **程式碼 3: 下載圖片檔案 (`python diffusiondb/scripts/download.py ...`)**
    
    ```Python
    # Download image files from 1 to 5. Each file is 1000 images.
    ! python diffusiondb/scripts/download.py -i 1 -r 5
    ```
    
    - **作用：** 執行剛才下載的 `download.py` 腳本來下載實際的圖片資料。
    - `-i 1 -r 5`: 參數指定下載資料集的第 1 到第 5 部分 (part)。DiffusionDB 資料集很大，被分成了很多部分。這裡只下載一小部分作為範例。腳本會將下載的壓縮檔存放在名為 `images` 的資料夾中。
- **程式碼 4: 解壓縮圖片檔案 (`unzip ...`)**
    
    ```Python
    # Unzip all image files
    image_directory = "extracted"
    ! unzip -n 'images/*.zip' -d '{image_directory}'
    ```
    
    - **作用：** 將下載好的圖片壓縮檔解壓縮。
    - `image_directory = "extracted"`: 設定一個變數來儲存解壓縮後檔案要存放的資料夾名稱。
    - `! unzip -n 'images/*.zip' -d '{image_directory}'`: 執行 `unzip` 命令。
        - `-n`: 表示如果目標資料夾中已存在同名檔案，不要覆蓋它。
        - `'images/*.zip'`: 指定要解壓縮的檔案是 `images` 資料夾下所有結尾是 `.zip` 的檔案。
        - `-d '{image_directory}'`: 指定將檔案解壓縮到 `extracted` 資料夾中。
- **程式碼 5: 載入圖片元資料 (`import json...`, `metadatas = {}...`)**
    
    ```Python
    import json
    import os
    
    metadatas = {}
    for file_name in os.listdir(image_directory):
        if file_name.endswith(".json"):
            with open(os.path.join(image_directory, file_name)) as f:
                metadata = json.load(f)
                metadatas.update(metadata)
    
    image_names = list(metadatas.keys())
    image_paths = [os.path.join(image_directory, image_name) for image_name in image_names]
    len(metadatas) # 輸出載入的元資料數量
    ```
    
    - **作用：** 讀取解壓縮後的 JSON 檔案，這些檔案包含了每張圖片的元資料 (例如生成圖片的文字提示)。
    - `import json, os`: 匯入處理 JSON 檔案和操作檔案系統所需的模組。
    - `metadatas = {}`: 初始化一個空的字典，用來儲存所有圖片的元資料。
    - `for file_name in os.listdir(image_directory):`: 遍歷 `extracted` 資料夾中的所有檔案和資料夾。
    - `if file_name.endswith(".json"):`: 檢查檔案名稱是否以 `.json` 結尾。
    - `with open(...) as f: ... metadata = json.load(f)`: 如果是 JSON 檔，就打開它，並使用 `json.load()` 將其內容解析為 Python 字典。
    - `metadatas.update(metadata)`: 將從單一 JSON 檔案讀取的元資料 (通常一個 JSON 檔包含多張圖片的資訊) 合併到主要的 `metadatas` 字典中。字典的鍵 (key) 通常是圖片的檔名 (例如 `image123.png`)，值 (value) 是包含提示等資訊的另一個字典。
    - `image_names = list(metadatas.keys())`: 從 `metadatas` 字典中取出所有的鍵 (圖片檔名)，轉換成一個列表。
    - `image_paths = [...]`: 使用列表推導式 (list comprehension)，為 `image_names` 中的每個檔名，建立完整的檔案路徑 (例如 `extracted/image123.png`)，並存入 `image_paths` 列表。
    - `len(metadatas)`: 計算並輸出成功載入元資料的圖片數量。

---

### **Task 5. Define function to detect explicit images (定義檢測不雅圖片的函數)**

- **目的：** 使用 Google Cloud Vision API 的 SafeSearch 功能，檢查下載的圖片是否包含不適當內容 (例如成人、暴力、醫療、諷刺內容)，並過濾掉不安全的圖片。這是確保資料品質和合規性的重要步驟。
    
- **程式碼 1: 定義 SafeSearch 檢測函數 (`detect_safe_search`)**
    
    ```Python
    from typing import Optional
    from google.cloud import vision
    from google.cloud.vision_v1.types.image_annotator import SafeSearchAnnotation
    
    client = vision.ImageAnnotatorClient() # 初始化 Vision API 客戶端
    
    def detect_safe_search(path: str) -> Optional[SafeSearchAnnotation]:
        """Detects unsafe features in the file."""
        with open(path, "rb") as image_file: # 以二進位讀取模式開啟圖片檔
            content = image_file.read() # 讀取圖片的位元組內容
    
        image = vision.Image(content=content) # 建立 Vision API 的圖片物件
    
        response = client.safe_search_detection(image=image) # 呼叫 SafeSearch API
    
        if response.error.message: # 檢查是否有錯誤
            print(response.error.message)
            return None
    
        return response.safe_search_annotation # 回傳 SafeSearch 的註解結果
    ```
    
    - **作用：** 這個函數封裝了呼叫 Cloud Vision API 來對單一圖片執行 SafeSearch 檢測的邏輯。
    - 它接收一個圖片檔案路徑 `path` 作為輸入。
    - 讀取圖片檔案的二進位內容。
    - 建立 Vision API 客戶端 (`client`)。
    - 呼叫 `client.safe_search_detection()` 方法，將圖片內容傳送給 API。
    - 回傳 API 回應中的 `safe_search_annotation` 部分，其中包含了對各類不安全內容的評級 (例如 `adult`, `violence` 的可能性)。如果 API 呼叫出錯，則回傳 `None`。
- **程式碼 2: 定義轉換註解結果的函數 (`convert_annotation_to_safety`)**
    
    ```Python
    from google.cloud.vision_v1.types.image_annotator import Likelihood
    
    # Returns true if some annotations have a potential safety issues
    def convert_annotation_to_safety(safe_search_annotation: SafeSearchAnnotation) -> bool:
        return all( # 檢查是否所有條件都為 True
            [
                (safe_level == Likelihood.VERY_UNLIKELY) # 可能性為 非常不可能 或
                or (safe_level == Likelihood.UNLIKELY)   # 不可能
                for safe_level in [
                    safe_search_annotation.adult,     # 成人內容
                    safe_search_annotation.medical,   # 醫療內容
                    safe_search_annotation.violence,  # 暴力內容
                    safe_search_annotation.racy,      # 諷刺/挑逗內容
                ]
            ]
        )
    ```
    
    - **作用：** 這個函數接收 `SafeSearchAnnotation` 物件，並根據預設的標準判斷圖片是否「安全」。
    - 它檢查 `adult`, `medical`, `violence`, `racy` 這四個類別的評級 (`Likelihood`)。
    - 只有當 **所有** 這四個類別的評級都是 `VERY_UNLIKELY` (非常不可能) 或 `UNLIKELY` (不可能) 時，函數才回傳 `True` (表示安全)。只要有任何一個類別的評級高於 `UNLIKELY`，就回傳 `False` (表示可能不安全)。
- **程式碼 3: 執行速率受限的圖片檢測 (`ThreadPoolExecutor`)**
    
    ```Python
    import time
    from concurrent.futures import ThreadPoolExecutor
    from typing import Optional
    import numpy as np
    from tqdm import tqdm # 用於顯示進度條
    
    # Create a rate limiter with a limit of 1800 requests per minute
    seconds_per_job = 1 / (1800 / 60) # 計算每個請求之間需要間隔多少秒
    
    def process_image(image_path: str) -> Optional[bool]:
        try:
            annotation = detect_safe_search(image_path) # 呼叫之前的函數進行檢測
    
            if annotation:
                return convert_annotation_to_safety(safe_search_annotation=annotation) # 轉換結果為 True/False
            else:
                return None # 如果檢測失敗，回傳 None
        except Exception:
            return None # 處理其他可能的異常
    
    # Process images using ThreadPool
    is_safe_values_cloud_vision = []
    with ThreadPoolExecutor() as executor: # 建立一個線程池來並行處理
        futures = []
        for img_url in tqdm(image_paths, total=len(image_paths), position=0): # 遍歷所有圖片路徑，並顯示進度條
            futures.append(executor.submit(process_image, img_url)) # 將 process_image 任務提交給線程池
            time.sleep(seconds_per_job) # 每次提交後暫停一小段時間，以符合 API 速率限制
    
        for future in futures: # 等待所有任務完成並收集結果
            is_safe_values_cloud_vision.append(future.result())
    
    # Set Nones to False (將檢測失敗的結果視為不安全)
    is_safe_values_cloud_vision = [
        is_safe or False for is_safe in is_safe_values_cloud_vision
    ]
    
    # Print number of safe images found
    print(
        f"Safe images = {np.array(is_safe_values_cloud_vision).sum()} out of {len(is_safe_values_cloud_vision)} images"
    )
    ```
    
    - **作用：** 這段程式碼使用多線程 (ThreadPoolExecutor) 來並行處理大量的圖片檢測任務，以提高效率。同時，它也加入了速率限制 (`time.sleep`)，避免因為請求過於頻繁而被 Cloud Vision API 拒絕服務。
    - `seconds_per_job`: 計算出基於每分鐘 1800 次請求的限制，每次請求之間應該等待多少秒。
    - `process_image`: 包裝了 `detect_safe_search` 和 `convert_annotation_to_safety` 的呼叫，並增加了錯誤處理。
    - `ThreadPoolExecutor`: 建立一個線程池，可以同時執行多個 `process_image` 任務。
    - `executor.submit()`: 將圖片路徑傳遞給 `process_image` 函數，並將這個任務提交到線程池中異步執行。
    - `time.sleep()`: 在每次提交任務後，強制程式暫停一小段時間，以控制請求速率。
    - `future.result()`: 獲取每個異步任務的執行結果 (True, False 或 None)。
    - 最後，將所有 `None` (表示檢測失敗) 的結果替換為 `False` (視為不安全)，並統計和印出被判定為安全的圖片數量。
- **程式碼 4: 過濾圖片列表 (`metadatas = [...`, `image_names = [...`, `image_paths = [...]`)**
    
    ```Python
    # Filter images by safety
    metadatas_dict = dict(zip(original_image_names, metadatas)) # 假設 metadatas 是列表，先轉回字典方便查找
                                                                # 如果 metadatas 本身就是字典，則不需要這行
    
    safe_image_names = [
        name for name, is_safe in zip(image_names, is_safe_values_cloud_vision) if is_safe
    ]
    
    # 根據 safe_image_names 過濾 metadatas 和 image_paths
    # 注意：這裡假設原始的 image_names 和 metadatas/image_paths 是一一對應的
    filtered_metadatas = [metadatas_dict[name] for name in safe_image_names] # 假設 metadatas_dict 已建立
    filtered_image_paths = [path for path, name in zip(image_paths, image_names) if name in safe_image_names]
    
    
    # 更新變數 (注意：原始程式碼直接修改了列表，更安全的方式是創建新列表)
    metadatas = filtered_metadatas
    image_names = safe_image_names
    image_paths = filtered_image_paths
    
    # (原始程式碼的寫法，更簡潔但可讀性稍差)
    # metadatas = [
    #     metadata
    #     for metadata, is_safe in zip(metadatas, is_safe_values_cloud_vision)
    #     if is_safe
    # ]
    # image_names = [
    #     image_name
    #     for image_name, is_safe in zip(image_names, is_safe_values_cloud_vision)
    #     if is_safe
    # ]
    # image_paths = [
    #     image_path
    #     for image_path, is_safe in zip(image_paths, is_safe_values_cloud_vision)
    #     if is_safe
    # ]
    
    ```
    
    - **作用：** 根據上一步得到的 `is_safe_values_cloud_vision` 列表 (包含 True/False 的安全性判斷)，過濾掉不安全的圖片。
    - 使用列表推導式，重新建立 `metadatas`, `image_names`, `image_paths` 這三個列表，只保留那些 `is_safe` 值為 `True` 的對應項目。這樣，後續的處理就只會針對這些被認為安全的圖片。
    - _(註解中的程式碼展示了更安全的做法，先根據安全的檔名篩選出需要的元數據和路徑，再更新原始變數，避免在迭代過程中修改列表可能產生的問題，雖然原始實驗的寫法在這種情況下通常也能正常工作)_

---

### **Task 6. Create helper functions to process data in batches (建立輔助函數以批次處理資料)**

- **目的：** 定義一系列函數來更有效率地處理資料，特別是呼叫 Vertex AI 的多模態嵌入 API。這包括將資料分批、並行處理、加入重試機制以及處理 API 的速率限制。
    
- **程式碼 1: 定義 EmbeddingPredictionClient (`import base64...`, `class EmbeddingPredictionClient...`)**
    
    ```Python
    import base64
    import time
    import typing
    from google.cloud import aiplatform
    from google.protobuf import struct_pb2
    import requests # 需要匯入 requests 來處理 URL 圖片
    
    class EmbeddingResponse(typing.NamedTuple): # 定義一個具名元組來儲存回應
        text_embedding: typing.Sequence[float]
        image_embedding: typing.Sequence[float]
    
    def load_image_bytes(image_uri: str) -> bytes: # 函數：從本地或 URL 載入圖片位元組
        """Load image bytes from a remote or local URI."""
        image_bytes = None
        if image_uri.startswith("http://") or image_uri.startswith("https://"): # 如果是 URL
            response = requests.get(image_uri, stream=True) # 下載圖片
            if response.status_code == 200:
                image_bytes = response.content
        else: # 如果是本地路徑
            image_bytes = open(image_uri, "rb").read() # 開啟並讀取檔案
        return image_bytes
    
    class EmbeddingPredictionClient: # 主要的客戶端類別
        """Wrapper around Prediction Service Client."""
    
        def __init__(
            self,
            project: str,
            location: str = "us-central1", # 預設位置
            api_regional_endpoint: str = "us-central1-aiplatform.googleapis.com", # API 端點
        ):
            client_options = {"api_endpoint": api_regional_endpoint}
            # 初始化底層的 PredictionServiceClient
            self.client = aiplatform.gapic.PredictionServiceClient(
                client_options=client_options
            )
            self.location = location
            self.project = project
    
        def get_embedding(self, text: str = None, image_file: str = None): # 獲取嵌入向量的方法
            if not text and not image_file: # 必須提供文字或圖片其中之一
                raise ValueError("At least one of text or image_file must be specified.")
    
            # 載入圖片位元組 (如果提供了 image_file)
            image_bytes = None
            if image_file:
                image_bytes = load_image_bytes(image_file) # 使用上面定義的函數
    
            instance = struct_pb2.Struct() # 建立請求的 instance 結構
            if text: # 如果提供了文字
                instance.fields["text"].string_value = text
    
            if image_bytes: # 如果提供了圖片
                encoded_content = base64.b64encode(image_bytes).decode("utf-8") # 將圖片位元組進行 Base64 編碼
                image_struct = instance.fields["image"].struct_value
                image_struct.fields["bytesBase64Encoded"].string_value = encoded_content
    
            instances = [instance] # 將 instance 包裝成列表
            # 設定要呼叫的模型端點 (endpoint)
            endpoint = (
                f"projects/{self.project}/locations/{self.location}"
                "/publishers/google/models/multimodalembedding@001" # 指向 multimodalembedding 模型版本 001
            )
            # 呼叫 predict API
            response = self.client.predict(endpoint=endpoint, instances=instances)
    
            # 解析回應，提取嵌入向量
            text_embedding = None
            if text:
                text_emb_value = response.predictions[0]["textEmbedding"]
                text_embedding = [v for v in text_emb_value] # 轉換為 Python 列表
    
            image_embedding = None
            if image_bytes:
                image_emb_value = response.predictions[0]["imageEmbedding"]
                image_embedding = [v for v in image_emb_value] # 轉換為 Python 列表
    
            # 回傳包含文字和/或圖片嵌入的 NamedTuple
            return EmbeddingResponse(
                text_embedding=text_embedding, image_embedding=image_embedding
            )
    
    ```
    
    - **作用：** 這個類別 `EmbeddingPredictionClient` 封裝了與 Vertex AI Multimodal Embedding 模型互動的邏輯。
    - `EmbeddingResponse`: 定義了一個簡單的資料結構，用來清晰地回傳文字嵌入和圖片嵌入。
    - `load_image_bytes`: 一個輔助函數，可以從本地檔案路徑或網路 URL 讀取圖片，並回傳其二進位內容 (bytes)。它使用了 `requests` 函式庫來處理 URL。
    - `__init__`: 類別的初始化方法。它接收專案 ID 和位置，並建立一個底層的 `aiplatform.gapic.PredictionServiceClient` 物件，這是實際與 Vertex AI API 通訊的客戶端。
    - `get_embedding`: 這是核心方法，用來獲取嵌入向量。
        - 它接收可選的 `text` (文字字串) 和 `image_file` (圖片路徑或 URL)。
        - 如果提供了 `image_file`，它會呼叫 `load_image_bytes` 來取得圖片內容，然後進行 Base64 編碼 (這是 API 要求圖片的傳輸格式)。
        - 它建立一個符合 API 要求的請求結構 (`instance`)，包含文字和/或 Base64 編碼的圖片。
        - 它指定要呼叫的模型端點 (`multimodalembedding@001`)。
        - 它呼叫 `self.client.predict()` 將請求發送到 Vertex AI。
        - 它解析 API 的回應 (`response.predictions`)，提取出 `textEmbedding` 和/或 `imageEmbedding` 向量，並將它們轉換為 Python 的浮點數列表。
        - 最後，它回傳一個 `EmbeddingResponse` 物件。
- **程式碼 2: 定義批次產生器和分塊處理函數 (`generate_batches`, `encode_to_embeddings_chunked`)**
    
    ```Python
    import time
    from concurrent.futures import ThreadPoolExecutor
    from typing import Callable, Generator, List, Optional # 增加 Optional
    from tqdm.auto import tqdm
    
    def generate_batches( # 函數：產生批次
        inputs: List[str], batch_size: int) -> Generator[List[str], None, None]:
        """
        Generator function that takes a list of strings and a batch size, and yields batches of the specified size.
        """
        for i in range(0, len(inputs), batch_size): # 以 batch_size 為步長進行迭代
            yield inputs[i : i + batch_size] # 每次回傳一個切片 (一個批次)
    
    API_IMAGES_PER_SECOND = 2 # 假設 API 每秒能處理 2 張圖片 (速率限制)
    
    def encode_to_embeddings_chunked( # 函數：分塊並行處理嵌入生成
        process_function: Callable[[List[str]], List[Optional[List[float]]]], # 接收一個處理函數作為參數
        items: List[str], # 要處理的項目列表 (例如圖片路徑)
        batch_size: int = 1, # 批次大小，預設為 1
    ) -> List[Optional[List[float]]]: # 回傳嵌入列表 (可能包含 None)
        """
        Function that encodes a list of strings into embeddings using a process function.
        It takes a list of strings and returns a list of optional lists of floats.
        The data is processed in chunks to prevent out-of-memory errors.
        """
        embeddings_list: List[Optional[List[float]]] = [] # 初始化結果列表
    
        # Prepare the batches using a generator
        batches = generate_batches(items, batch_size) # 產生批次生成器
    
        seconds_per_job = batch_size / API_IMAGES_PER_SECOND # 計算每個批次處理間的延遲
    
        with ThreadPoolExecutor() as executor: # 建立線程池
            futures = []
            # 遍歷批次，顯示進度條
            for batch in tqdm(batches, total=len(items) // batch_size, position=0):
                # 將處理函數 (process_function) 和當前批次提交給線程池
                futures.append(executor.submit(process_function, batch))
                time.sleep(seconds_per_job) # 延遲以符合速率限制
    
            for future in futures: # 收集所有批次的結果
                embeddings_list.extend(future.result()) # 將批次結果追加到主列表
        return embeddings_list
    ```
    
    - **作用：** 這兩個函數協同工作，以有效且安全的方式處理大量資料的嵌入生成。
    - `generate_batches`: 這是一個 Python 生成器 (Generator)。它接收一個列表 (`inputs`) 和一個批次大小 (`batch_size`)。它不會一次性把所有批次都創建出來，而是每次被請求時才產生 (yield) 下一個批次。這對於處理非常大的列表非常有用，可以節省記憶體。
    - `encode_to_embeddings_chunked`: 這個函數是核心的批次處理邏輯。
        - 它接收一個 `process_function` 作為參數，這個函數是實際執行嵌入生成任務的函數 (例如後面會定義的 `encode_images_to_embeddings`)。
        - 它接收 `items` (要處理的資料列表，如圖片路徑) 和 `batch_size`。
        - 它使用 `generate_batches` 來獲取資料批次。
        - `API_IMAGES_PER_SECOND` 和 `seconds_per_job` 用來計算每次處理一個批次後需要等待多久，以遵守 API 的速率限制 (這裡假設每秒只能處理 2 個請求，所以如果 `batch_size` 是 1，就要等 0.5 秒)。
        - 它使用 `ThreadPoolExecutor` 來並行處理不同的批次。對於每個批次，它呼叫 `executor.submit(process_function, batch)`，將處理任務交給線程池。
        - `time.sleep(seconds_per_job)` 在每次提交後強制暫停，實現速率限制。
        - 最後，它收集所有線程的執行結果 (嵌入向量列表，可能包含 `None` 表示錯誤) 並將它們合併成一個大列表回傳。
- **程式碼 3: 定義帶重試邏輯的嵌入函數 (`encode_texts_to_embeddings_with_retry`, `encode_images_to_embeddings_with_retry`, etc.)**
    
    ```Python
    import copy
    from typing import List, Optional
    import numpy as np
    import requests # 確保匯入
    from tenacity import retry, stop_after_attempt # 匯入重試函式庫
    
    client = EmbeddingPredictionClient(project=PROJECT_ID, location=REGION) # 初始化客戶端，使用之前設定的 PROJECT_ID 和 REGION
    
    # Use a retry handler in case of failure
    @retry(reraise=True, stop=stop_after_attempt(3)) # 設定重試裝飾器：最多重試 3 次，如果最終失敗則重新拋出異常
    def encode_texts_to_embeddings_with_retry(text: List[str]) -> List[List[float]]:
        assert len(text) == 1 # 斷言：確保輸入列表只有一個文字 (因為 API 一次處理一個)
    
        try:
            # 呼叫客戶端的 get_embedding，只傳遞 text
            return [client.get_embedding(text=text[0], image_file=None).text_embedding]
        except Exception:
            raise RuntimeError("Error getting embedding.") # 拋出通用運行時錯誤
    
    def encode_texts_to_embeddings(text: List[str]) -> List[Optional[List[float]]]: # 包裝函數
        try:
            return encode_texts_to_embeddings_with_retry(text=text) # 呼叫帶重試的函數
        except Exception:
            return [None for _ in range(len(text))] # 如果重試最終失敗，回傳 [None]
    
    @retry(reraise=True, stop=stop_after_attempt(3)) # 同樣的重試邏輯
    def encode_images_to_embeddings_with_retry(image_uris: List[str]) -> List[List[float]]:
        assert len(image_uris) == 1 # 斷言：確保輸入列表只有一個圖片 URI
    
        try:
            # 呼叫客戶端的 get_embedding，只傳遞 image_file
            return [
                client.get_embedding(text=None, image_file=image_uris[0]).image_embedding
            ]
        except Exception as ex:
            print(ex) # 印出具體的異常信息
            raise RuntimeError("Error getting embedding.") # 拋出通用運行時錯誤
    
    def encode_images_to_embeddings(image_uris: List[str]) -> List[Optional[List[float]]]: # 包裝函數
        try:
            return encode_images_to_embeddings_with_retry(image_uris=image_uris) # 呼叫帶重試的函數
        except Exception as ex:
            print(ex) # 印出異常信息
            return [None for _ in range(len(image_uris))] # 如果重試最終失敗，回傳 [None]
    ```
    
    - **作用：** 這些函數為文字和圖片嵌入的生成提供了更健壯的接口。它們使用了 `tenacity` 函式庫提供的 `@retry` 裝飾器，可以在 API 呼叫失敗時自動重試。
    - `client = ...`: 首先，初始化之前定義的 `EmbeddingPredictionClient`。
    - `@retry(reraise=True, stop=stop_after_attempt(3))`: 這個裝飾器應用於 `_with_retry` 函數。`stop_after_attempt(3)` 表示最多嘗試 3 次 (包括第一次)。`reraise=True` 表示如果 3 次嘗試都失敗了，就把最後一次的異常重新拋出來。
    - `encode_texts_to_embeddings_with_retry` / `encode_images_to_embeddings_with_retry`: 這些是實際呼叫 `client.get_embedding` 的函數。它們包含一個 `assert len(...) == 1`，因為根據實驗說明，這個特定的嵌入模型 API 一次只能處理一個項目 (一個文字或一張圖片)。如果 API 呼叫成功，它們回傳包含嵌入向量的列表；如果失敗（即使重試後），它們會拋出 `RuntimeError`。
    - `encode_texts_to_embeddings` / `encode_images_to_embeddings`: 這些是最終提供給 `encode_to_embeddings_chunked` 函數使用的包裝函數。它們呼叫對應的 `_with_retry` 函數。關鍵在於它們使用了 `try...except` 塊：如果 `_with_retry` 函數成功，它們就回傳嵌入向量列表；但如果 `_with_retry` 函數最終拋出了異常 (表示重試也失敗了)，它們會捕捉這個異常，並回傳一個包含 `None` 的列表 (`[None]`)。這樣可以讓批次處理繼續進行，而不會因為單個項目的失敗而中斷，同時也標記了哪些項目處理失敗。
- **程式碼 4: 測試圖片編碼函數 (`%%time...`, `image_paths_filtered = ...`)**
    
    ```Python
    %%time
    # Encode a sample subset of images
    image_paths_filtered = list(image_paths)[:1000] # 取前 1000 張安全圖片的路徑作為樣本
    image_embeddings = encode_to_embeddings_chunked( # 使用分塊處理函數
        process_function=encode_images_to_embeddings, # 指定使用帶重試和錯誤處理的圖片編碼函數
        items=image_paths_filtered, # 傳入圖片路徑樣本
        batch_size=1 # 批次大小設為 1 (因為 API 一次處理一張)
    )
    
    # Keep only non-None embeddings (過濾掉處理失敗的結果)
    indexes_to_keep, image_embeddings_filtered = zip( # 使用 zip(*) 來同時獲取索引和非 None 的嵌入
        *[
            (index, embedding)
            for index, embedding in enumerate(image_embeddings) # 遍歷結果和索引
            if embedding is not None # 只保留嵌入不是 None 的項目
        ]
    )
    image_embeddings = list(image_embeddings_filtered) # 將過濾後的嵌入轉回列表 (如果後續需要列表形式)
    
    print(f"Processed {len(image_embeddings)} embeddings successfully") # 印出成功處理的數量
    ```
    
    - **作用：** 這段程式碼對一小部分資料 (前 1000 張過濾後的安全圖片) 執行圖片嵌入生成，以測試前面定義的函數是否正常工作，並大致了解其性能。
    - `%%time`: Jupyter Notebook 的魔法命令，用來測量整個儲存格 (cell) 的執行時間。
    - `image_paths_filtered = list(image_paths)[:1000]`: 從之前過濾得到的安全圖片路徑列表 `image_paths` 中，取出前 1000 個作為測試樣本。
    - `encode_to_embeddings_chunked(...)`: 呼叫批次處理函數。
        - `process_function=encode_images_to_embeddings`: 告訴批次處理函數，每個批次應該使用 `encode_images_to_embeddings` 這個函數來處理 (這個函數內部包含了重試和錯誤處理邏輯)。
        - `items=image_paths_filtered`: 傳入要處理的圖片路徑樣本。
        - `batch_size=1`: 設定批次大小為 1，因為底層 API 一次只處理一張圖片。
    - 過濾 `None` 值：`encode_to_embeddings_chunked` 回傳的 `image_embeddings` 列表中可能包含 `None` (如果某張圖片處理失敗)。這部分程式碼使用列表推導式和 `enumerate` 來找出那些嵌入值不是 `None` 的項目及其原始索引，然後使用 `zip(*...)` 的技巧將索引和對應的非 `None` 嵌入分開。最後將過濾後的嵌入向量存回 `image_embeddings` (覆蓋了原始帶有 None 的列表)。
    - `print(...)`: 輸出成功生成嵌入向量的圖片數量。
- **程式碼 5: 定義點積距離函數 (`dot_product_distance`)**
    
    ```Python
    import numpy as np
    
    def dot_product_distance(
        text_embedding: np.ndarray, image_embeddings: np.ndarray) -> np.ndarray:
        """Compute dot-product distance between text and image embeddings by taking the dot product"""
        # 計算文字嵌入向量與所有圖片嵌入向量矩陣的點積
        # text_embedding 是一維向量 (D,)
        # image_embeddings 是二維矩陣 (N, D)，N 是圖片數量，D 是維度
        # np.dot 會自動處理成 text_embedding * image_embeddings.T
        # 結果是一個一維向量 (N,)，每個元素是文字與對應圖片的點積相似度
        return np.dot(text_embedding, image_embeddings.T)
    ```
    
    - **作用：** 定義一個函數來計算一個文字嵌入向量和一組圖片嵌入向量之間的相似度。
    - **原理：** 在向量空間中，如果向量是標準化的 (長度為 1)，它們之間的點積 (dot product) 等於它們之間夾角的餘弦值 (cosine similarity)。餘弦相似度是衡量向量方向相似性的常用指標，值越接近 1 表示越相似。即使向量沒有完全標準化，點積通常也能很好地反映相似性，特別是對於像 Vertex AI Multimodal Embedding 這樣經過優化訓練的模型。
    - `import numpy as np`: 匯入 NumPy 函式庫，用於高效的數值計算。
    - 函數接收一個 NumPy 陣列 `text_embedding` (代表單個文字查詢的嵌入) 和一個 NumPy 陣列 `image_embeddings` (代表多個圖片的嵌入，每行是一個圖片的嵌入)。
    - `np.dot(text_embedding, image_embeddings.T)`: 使用 NumPy 的 `dot` 函數計算點積。`image_embeddings.T` 是將圖片嵌入矩陣轉置，這樣 `dot` 函數就能計算文字向量與每個圖片向量的點積。結果是一個 NumPy 陣列，其中每個元素代表文字查詢與對應圖片的點積相似度分數。分數越高通常表示越相似。
- **程式碼 6: 測試文字到圖片的相似度並視覺化 (`import math...`, `plt.subplots...`)**
    
    ```Python
    import math
    from io import BytesIO # 用於處理來自 URL 的圖片數據流
    import matplotlib.pyplot as plt
    from PIL import Image # Python Imaging Library，用於處理圖片
    import requests # 需要匯入才能從 URL 下載圖片
    
    text_query = "Birds in flight" # 設定文字查詢
    # Calculate text embedding of query
    text_embedding_list = encode_texts_to_embeddings(text=[text_query]) # 獲取文字嵌入 (回傳是列表)
    if text_embedding_list and text_embedding_list[0] is not None: # 確保成功獲取嵌入
        text_embedding = text_embedding_list[0] # 取出嵌入向量
    
        try:
            # (用於除錯，檢查類型和維度)
            # print(type(text_embedding))
            # print(type(text_embedding[0]))
            # print(type(image_embeddings)) # 確保 image_embeddings 是列表或可以轉換為 NumPy 陣列
            # print(type(image_embeddings[0]))
            # print(np.array(image_embeddings).shape) # 檢查圖片嵌入的形狀
            # print(np.array(text_embedding).shape) # 檢查文字嵌入的形狀
    
            # Calculate distance (計算相似度分數)
            distances = dot_product_distance(
                text_embedding=np.array(text_embedding), # 轉換為 NumPy 陣列
                image_embeddings=np.array(image_embeddings) # 將圖片嵌入列表轉換為 NumPy 陣列
            )
    
            # Set the maximum number of images to display
            MAX_IMAGES = 20 # 設定最多顯示 20 張圖片
    
            # Sort images and scores by descending order of scores and select the top max_images
            # 將圖片路徑和對應的相似度分數打包，並根據分數降序排序
            sorted_data = sorted(
                zip(image_paths_filtered, distances), # 使用過濾後的樣本圖片路徑
                key=lambda x: x[1], # 按元組的第二個元素 (分數) 排序
                reverse=True # 降序排列 (分數越高越相似)
            )[:MAX_IMAGES] # 取前 MAX_IMAGES 個結果
    
            # Calculate the number of rows and columns needed to display the images
            num_cols = 4 # 設定每行顯示 4 張圖片
            num_rows = math.ceil(len(sorted_data) / num_cols) # 計算需要的行數
    
            # Create a grid of subplots to display the images
            fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 12)) # 建立子圖網格
    
            # Loop through the top max_images images and display them in the subplots
            for i, (image_path, distance) in enumerate(sorted_data): # 遍歷排序後的結果
                # Calculate the row and column index for the current image
                row_idx = i // num_cols # 計算行索引
                col_idx = i % num_cols # 計算列索引
    
                # Check if image_path is a remote URL or local file
                try: # 加入錯誤處理，防止圖片載入失敗導致整個過程中斷
                    if image_path.startswith("http://") or image_path.startswith("https://"):
                        response = requests.get(image_path) # 從 URL 下載
                        response.raise_for_status() # 檢查請求是否成功
                        image = Image.open(BytesIO(response.content)) # 從下載的內容開啟圖片
                    else:
                        image = Image.open(image_path) # 從本地檔案開啟圖片
    
                    # Display the image in the current subplot
                    # 如果 axs 是一維的 (只有一行)，需要調整索引方式
                    if num_rows == 1:
                       ax = axs[col_idx]
                    else:
                       ax = axs[row_idx, col_idx]
    
                    ax.imshow(image, cmap="gray") # 顯示圖片
    
                    # Set the title of the subplot to the image index and score
                    ax.set_title(f"Rank {i+1}, Distance = {distance:.2f}") # 設定標題，顯示排名和分數
    
                    # Remove ticks from the subplot
                    ax.set_xticks([]) # 移除 X 軸刻度
                    ax.set_yticks([]) # 移除 Y 軸刻度
                except Exception as e:
                    print(f"Error loading/displaying image {image_path}: {e}")
                    # 如果圖片載入失敗，可以在對應位置顯示錯誤訊息或留白
                    if num_rows == 1:
                       ax = axs[col_idx]
                    else:
                       ax = axs[row_idx, col_idx]
                    ax.set_title(f"Rank {i+1}\nError")
                    ax.set_xticks([])
                    ax.set_yticks([])
    
    
            # Adjust the spacing between subplots and display the plot
            plt.tight_layout() # 自動調整子圖間距
            # plt.subplots_adjust(hspace=0.3, wspace=0.1) # 或者手動調整
            plt.show() # 顯示圖像
        except Exception as e:
            print(f"An error occurred during visualization: {e}") # 捕捉其他可能的錯誤
    else:
        print("Failed to get text embedding for the query.") # 如果文字嵌入獲取失敗
    
    ```
    
    - **作用：** 這段程式碼演示了一個簡單的文字到圖片搜索的過程，並將結果視覺化。
    - `text_query`: 設定要搜索的文字。
    - `encode_texts_to_embeddings`: 呼叫之前定義的函數，獲取 `text_query` 的嵌入向量。
    - `dot_product_distance`: 使用點積函數計算文字嵌入與樣本圖片嵌入之間的相似度分數。
    - `sorted(...)`: 將圖片路徑和它們對應的分數打包 (`zip`)，然後根據分數 (`key=lambda x: x[1]`) 進行降序排序 (`reverse=True`)，最後取出分數最高的前 `MAX_IMAGES` 個結果。
    - `matplotlib.pyplot` 和 `PIL.Image`: 使用這兩個函式庫來建立一個圖片網格。
    - 迴圈遍歷排序後的結果，對於每個結果：
        - 計算它在網格中的位置 (`row_idx`, `col_idx`)。
        - 根據 `image_path` 是 URL 還是本地路徑，使用 `requests` 或直接 `Image.open` 來載入圖片。增加了 `try...except` 來處理可能的圖片載入錯誤。
        - 使用 `axs[row_idx, col_idx].imshow()` 在對應的子圖位置顯示圖片。
        - 設定子圖的標題，顯示圖片的排名和相似度分數。
        - 移除座標軸刻度，使圖像更清晰。
    - `plt.tight_layout()` 或 `plt.subplots_adjust()`: 調整子圖之間的間距。
    - `plt.show()`: 將繪製好的圖像顯示出來。
- **程式碼 7: 儲存嵌入維度 (`DIMENSIONS = ...`)**
    
    ```Python
    # Assuming text_embedding was successfully calculated in the previous step
    if 'text_embedding' in locals() and text_embedding is not None:
        DIMENSIONS = len(text_embedding)
        print(DIMENSIONS)
    else:
        print("Text embedding not available to determine dimensions.")
        # DIMENSIONS = 1408
    ```
    
    - **作用：** 獲取嵌入向量的維度 (dimensionality)，並將其儲存起來。這個維度值在後續創建 Vector Search 索引時是必需的參數。
    - `len(text_embedding)`: 計算之前生成的 `text_embedding` 向量包含多少個元素 (浮點數)。這個數量就是模型的輸出維度 (例如，可能是 768, 1024 或 1408，取決於所用的 `multimodalembedding@001` 模型的具體版本)。
    - `print(DIMENSIONS)`: 將獲取的維度值印出來。
- **程式碼 8: 創建臨時檔案 (`import tempfile...`)**
    
    ```Python
    import tempfile
    # Create temporary file to write embeddings to
    embeddings_file = tempfile.NamedTemporaryFile(suffix=".jsonl", delete=False, mode='w') # 使用 .jsonl 後綴，設定寫入模式
    
    print(embeddings_file.name) # 印出臨時檔案的路徑
    # 記得在使用完畢後關閉檔案: embeddings_file.close() (或者使用 with open)
    ```
    
    - **作用：** 創建一個臨時檔案，用來存放接下來要為 **整個資料集** 生成的嵌入向量。這些向量需要以特定的格式寫入檔案，然後上傳到 Google Cloud Storage (GCS)，才能被 Vector Search 用來建立索引。
    - `import tempfile`: 匯入 Python 的 `tempfile` 模組。
    - `tempfile.NamedTemporaryFile(...)`: 創建一個帶有名稱的臨時檔案。
        - `suffix=".jsonl"`: 指定檔案的後綴名為 `.jsonl`。JSONL (JSON Lines) 格式要求檔案中的每一行都是一個獨立的、合法的 JSON 物件。這是 Vector Search 接受的輸入格式之一。
        - `delete=False`: 預設情況下，`NamedTemporaryFile` 創建的檔案在關閉後會被自動刪除。設定為 `False` 可以讓檔案保留下來，以便後續上傳。
        - `mode='w'`: 以寫入模式開啟檔案。
    - `embeddings_file.name`: 獲取這個臨時檔案在系統中的完整路徑。
- **程式碼 9: 生成所有圖片的嵌入並存檔 (`import json...`, `with open(...)`)**
    
    ```Python
    import json
    
    BATCH_SIZE = 1000 # 設定每次處理的批次大小
    # 使用 with open 可以確保檔案在使用完畢後自動關閉
    with open(embeddings_file.name, "w") as f: # 以寫入模式 ('w') 開啟臨時檔案
        # 遍歷所有過濾後的安全圖片名稱和路徑，以 BATCH_SIZE 為步長
        for i in tqdm(range(0, len(image_names), BATCH_SIZE)):
            # 獲取當前批次的圖片名稱和路徑
            image_names_chunk = image_names[i : i + BATCH_SIZE]
            image_paths_chunk = image_paths[i : i + BATCH_SIZE]
    
            # 為當前批次的圖片生成嵌入向量
            embeddings = encode_to_embeddings_chunked(
                process_function=encode_images_to_embeddings, # 使用圖片嵌入函數
                items=image_paths_chunk, # 傳入圖片路徑批次
                batch_size=1 # API 限制，一次一個
            )
    
            # Prepare lines to write to file (準備要寫入檔案的 JSONL 行)
            lines_to_write = []
            for img_name, embedding in zip(image_names_chunk, embeddings): # 將圖片名稱和對應的嵌入配對
                if embedding is not None: # 只處理成功生成的嵌入
                    # 建立符合 Vector Search 要求的 JSON 物件
                    record = {
                        "id": str(img_name), # id 必須是字串，使用圖片名稱作為唯一標識
                        # embedding 欄位需要是浮點數列表 (Vector Search 通常支援)
                        # 或者像原始碼一樣轉成字串列表 (某些舊版本或特定設定可能需要)
                        "embedding": [float(value) for value in embedding], # 確保是浮點數列表
                        # "embedding": [str(value) for value in embedding], # 原始碼的寫法
                    }
                    lines_to_write.append(json.dumps(record) + "\n") # 將 JSON 物件轉換為字串，並在末尾加上換行符
    
            # Append to file (將格式化好的行寫入檔案)
            f.writelines(lines_to_write)
    
    print(f"Embeddings saved to: {embeddings_file.name}")
    
    # 注意：這一步會花費相當長的時間，因為需要為所有圖片呼叫嵌入 API
    ```
    
    - **作用：** 這段程式碼是實驗中計算量最大的部分之一。它會遍歷 **所有** 經過安全過濾的圖片，為它們生成嵌入向量，並將結果以 JSONL 格式寫入之前創建的臨時檔案中。
    - `BATCH_SIZE = 1000`: 這裡的 `BATCH_SIZE` 指的是一次從 `image_names` 和 `image_paths` 中取多少個項目來調用 `encode_to_embeddings_chunked`。注意，`encode_to_embeddings_chunked` 內部的 `batch_size` 仍然是 1，因為 API 限制。這個外層的 `BATCH_SIZE` 主要是為了控制單次迴圈處理的項目數量和記憶體使用。
    - `with open(embeddings_file.name, "w") as f:`: 使用 `with` 語句來開啟臨時檔案進行寫入 (`"w"`)。這樣可以確保檔案在區塊執行完畢後被正確關閉，即使發生錯誤。
    - `for i in tqdm(...)`: 使用 `tqdm` 顯示進度條，遍歷所有圖片，每次處理 `BATCH_SIZE` 個。
    - `image_names_chunk`, `image_paths_chunk`: 獲取當前批次的圖片名稱和路徑。
    - `embeddings = encode_to_embeddings_chunked(...)`: 呼叫批次處理函數，為當前批次的圖片生成嵌入向量。結果 `embeddings` 是一個列表，其中可能包含 `None`。
    - `for img_name, embedding in zip(...)`: 遍歷當前批次的圖片名稱和它們對應的嵌入向量。
    - `if embedding is not None:`: 只處理那些成功生成了嵌入向量的圖片。
    - `record = {...}`: 創建一個 Python 字典，其結構符合 Vector Search 對 JSONL 格式的要求：
        - `"id"`: 每個向量的唯一標識符，必須是字串。這裡使用圖片的檔名作為 ID。
        - `"embedding"`: 嵌入向量本身，是一個浮點數列表。_(原始程式碼中將浮點數轉成了字串列表 `[str(value) for value in embedding]`，雖然某些情況下 Vector Search 可能接受，但通常直接使用浮點數列表更標準且不易出錯)_。
    - `json.dumps(record) + "\n"`: 使用 `json.dumps()` 將 Python 字典轉換為 JSON 格式的字串，並在末尾加上一個換行符 (`\n`)，以符合 JSONL 格式的要求 (每行一個 JSON 物件)。
    - `f.writelines(lines_to_write)`: 將準備好的多行 JSONL 字串一次性寫入檔案。
- **程式碼 10: 上傳嵌入檔案到 GCS (`! gsutil cp ...`)**
    
    ```Python
    # Make sure BUCKET_URI is defined from Task 3
    # Example: BUCKET_URI = f"gs://artifacts-{PROJECT_ID}-unique"
    UNIQUE_FOLDER_NAME = "embeddings_folder_unique" # GCS 中的資料夾名稱
    EMBEDDINGS_INITIAL_URI = f"{BUCKET_URI}/{UNIQUE_FOLDER_NAME}/" # 完整的 GCS 目錄路徑
    
    # Copy the local embeddings file to Google Cloud Storage
    ! gsutil cp {embeddings_file.name} {EMBEDDINGS_INITIAL_URI}
    ```
    
    - **作用：** 將本地儲存著所有嵌入向量的 JSONL 臨時檔案，複製到 Google Cloud Storage (GCS) 儲存桶中。Vector Search 服務會從這個 GCS 位置讀取資料來建立索引。
    - `UNIQUE_FOLDER_NAME`: 設定一個在 GCS 儲存桶內用於存放嵌入檔案的資料夾名稱。
    - `EMBEDDINGS_INITIAL_URI`: 組合出完整的 GCS 路徑，格式為 `gs://<bucket_name>/<folder_name>/`。`<bucket_name>` 是在 Task 3 中創建的儲存桶名稱 (存於 `BUCKET_URI` 變數)。
    - `! gsutil cp ... ...`: 使用 `gsutil` (Google Cloud Storage 的命令列工具) 的 `cp` (複製) 命令。
        - `{embeddings_file.name}`: 源檔案，即本地的臨時 JSONL 檔案路徑。
        - `{EMBEDDINGS_INITIAL_URI}`: 目標位置，即 GCS 上的目錄路徑。`gsutil` 會將檔案複製到這個 GCS 目錄下。

---

### **Task 7. Create MatchingEngineIndex (建立 MatchingEngineIndex)**

- **目的：** 在 Vertex AI 中實際創建向量搜索索引 (Vector Search Index，在 API 中稱為 Matching Engine Index)。這個索引是基於您上傳到 GCS 的嵌入向量建立的，它使用了近似最近鄰 (Approximate Nearest Neighbor, ANN) 算法，能夠快速地在大量向量中找到與查詢向量最相似的向量。
    
- **程式碼 1: 設定索引的顯示名稱和描述 (`DISPLAY_NAME = ...`)**
    
    ```Python
    DISPLAY_NAME = "multimodal_diffusiondb" # 設定索引在控制台顯示的名稱
    DESCRIPTION = "Multimodal DiffusionDB Embeddings" # 設定索引的描述
    ```
    
    - **作用：** 定義兩個字串變數，用於在 Vertex AI 控制台或 API 中識別這個索引。
- **程式碼 2: 初始化 Vertex AI SDK (`aiplatform.init(...)`)**
    
    ```Python
    aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)
    ```
    
    - **作用：** 再次初始化 Vertex AI Python SDK。這確保 SDK 使用正確的專案 ID、區域，並指定了一個 GCS 儲存桶 (`staging_bucket`)，SDK 可能會在創建雲端資源 (如索引) 的過程中，使用這個桶來存放臨時檔案。
- **程式碼 3: 創建 Tree-AH 索引 (`aiplatform.MatchingEngineIndex.create_tree_ah_index(...)`)**
    
    ```Python
    tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(
        display_name=DISPLAY_NAME, # 使用之前設定的顯示名稱
        contents_delta_uri=EMBEDDINGS_INITIAL_URI, # 指定包含嵌入向量的 GCS 目錄路徑
        dimensions=DIMENSIONS, # 指定嵌入向量的維度 (來自 Task 6)
        approximate_neighbors_count=150, # ANN 演算法參數：構建索引時考慮的近似鄰居數量
        distance_measure_type="COSINE_DISTANCE", # 指定距離度量類型
        leaf_node_embedding_count=500, # Tree-AH 演算法參數：葉節點包含的向量數量上限
        leaf_nodes_to_search_percent=7, # Tree-AH 演算法參數：查詢時搜索的葉節點百分比
        description=DESCRIPTION, # 使用之前設定的描述
    )
    ```
    
    - **作用：** 這是創建向量索引的核心指令。它呼叫了 Vertex AI SDK 中創建特定類型 ANN 索引的方法 (`create_tree_ah_index`)。Tree-AH 是 Google 開發的一種高效 ANN 演算法。
    - `display_name`, `description`: 設定索引的元資料。
    - `contents_delta_uri`: **非常重要**，指向您在 Task 6 中上傳嵌入向量 JSONL 檔案的那個 GCS 目錄 (`gs://.../embeddings_folder_unique/`)。Vertex AI 會從這裡讀取資料來建立索引。
    - `dimensions`: **必需參數**，指定嵌入向量的維度 (例如 1408)，必須與 `contents_delta_uri` 中向量的實際維度一致。
    - `approximate_neighbors_count`: 影響索引構建時的精度和時間。
    - `distance_measure_type`: 指定索引內部計算向量之間距離的方式。可選項通常包括：
        - `"COSINE_DISTANCE"`: 餘弦距離 (1 - cosine similarity)。適用於方向很重要的歸一化向量。值越小越相似。
        - `"DOT_PRODUCT_DISTANCE"`: 點積距離 (-dot product 或 1 - dot product，取決於實現)。適用於點積大小直接反映相似度的情況。通常值越小越相似。
        - `"SQUARED_L2_DISTANCE"`: 歐氏距離的平方。適用於向量的實際位置差異很重要的情況。值越小越相似。 實驗中前面使用點積計算相似度，如果向量是歸一化的，選擇 `COSINE_DISTANCE` 是合適的。如果未歸一化且點積越大越好，則 `DOT_PRODUCT_DISTANCE` 可能更直觀（但要注意查詢時返回的距離值大小代表的意義）。這裡選擇了 `COSINE_DISTANCE`。
    - `leaf_node_embedding_count`, `leaf_nodes_to_search_percent`: Tree-AH 演算法的調優參數。`leaf_node_embedding_count` 控制索引樹結構中每個葉節點儲存多少個向量。`leaf_nodes_to_search_percent` 控制在執行查詢時，需要檢查多少比例的葉節點，這會影響查詢的延遲和召回率（找到真正最近鄰的比例）之間的權衡。百分比越高，越準確但越慢。
    - **執行時間：** 創建索引是一個耗時的操作，因為 Vertex AI 需要讀取所有向量，並構建複雜的 ANN 索引結構。實驗提示這可能需要大約 50 分鐘。
    - **返回值：** 這個函數呼叫會啟動一個長時間運行的操作，並回傳一個 `MatchingEngineIndex` 物件，代表了您正在創建的索引資源。您可以稍後使用這個物件來檢查索引狀態或獲取其資訊。
- **程式碼 4: 獲取索引資源名稱 (`INDEX_RESOURCE_NAME = ...`)**
    
    ```Python
    INDEX_RESOURCE_NAME = tree_ah_index.resource_name # 從返回的物件中獲取索引的完整資源名稱
    
    print(INDEX_RESOURCE_NAME) # 印出資源名稱
    ```
    
    - **作用：** 從上一步創建索引操作返回的 `tree_ah_index` 物件中，獲取該索引在 Google Cloud 中的唯一資源名稱。這個名稱的格式通常是 `projects/<PROJECT_NUMBER>/locations/<REGION>/indexes/<INDEX_ID>`。
    - 這個資源名稱在後續操作中（例如將索引部署到端點，或在其他地方引用這個索引）非常有用。
- **程式碼 5: 通過資源名稱獲取索引物件 (`tree_ah_index = aiplatform.MatchingEngineIndex(...)`)**
    
    ```Python
    # 如果你需要重新獲取索引物件 (例如在不同的程式或核心重啟後)
    tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)
    ```
    
    - **作用：** 展示了如何僅使用索引的資源名稱 (`INDEX_RESOURCE_NAME`) 來重新獲取代表該索引的 Python 物件 (`MatchingEngineIndex`)。如果您丟失了之前創建時返回的 `tree_ah_index` 變數，或者想在不同的程式碼段中操作同一個已創建的索引，這個方法很有用。

---

### **Task 8. Create an IndexEndpoint with VPC Network (使用 VPC 網路建立 IndexEndpoint)**

- **目的：** 創建一個索引端點 (Index Endpoint)。索引本身只是數據結構，您需要將它「部署」到一個端點上，才能夠接收實時的查詢請求。為了安全和網路性能，這個實驗將端點創建在您的專案的 VPC (虛擬私有雲) 網路內，而不是直接暴露在公共網路上。
    
- **程式碼 1: 獲取 VPC 網路路徑 (`PROJECT_NUMBER = ...`, `VPC_NETWORK_FULL = ...`)**
    
    ```Python
    # Retrieve the project number (獲取專案編號)
    PROJECT_NUMBER = !gcloud projects list --filter="PROJECT_ID:'{PROJECT_ID}'" --format='value(PROJECT_NUMBER)'
    PROJECT_NUMBER = PROJECT_NUMBER[0] # gcloud 命令返回的是列表，取出第一個元素
    
    VPC_NETWORK = "default" # 指定要使用的 VPC 網路名稱 (通常每個專案都有一個名為 'default' 的網路)
    # Construct the full VPC network path (構建完整的 VPC 網路資源名稱)
    VPC_NETWORK_FULL = "projects/{}/global/networks/{}".format(PROJECT_NUMBER, VPC_NETWORK)
    print(VPC_NETWORK_FULL) # 印出完整的網路路徑
    ```
    
    - **作用：** 準備創建 Index Endpoint 所需的 VPC 網路資訊。
    - `!gcloud projects list ...`: 使用 `gcloud` 命令獲取與您的 `PROJECT_ID` 對應的專案編號 (`PROJECT_NUMBER`)。專案編號是 Google Cloud 內部使用的唯一數字標識符。
    - `VPC_NETWORK = "default"`: 指定要使用的 VPC 網路的名稱。除非您有特殊配置，否則 Google Cloud 專案通常會自帶一個名為 `default` 的 VPC 網路。
    - `VPC_NETWORK_FULL = ...`: 將專案編號和 VPC 網路名稱組合成 Google Cloud API 所需的完整 VPC 網路資源路徑格式：`projects/<PROJECT_NUMBER>/global/networks/<NETWORK_NAME>`。注意網路通常是 `global` 資源。
    - Vector Search 端點需要與這個指定的 VPC 網路建立連接 (通常是通過 VPC 網路對等互連)，以便能夠安全地接收來自該網路內部的查詢請求。
- **程式碼 2: 創建 MatchingEngineIndexEndpoint (`my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(...)`)**
    
    ```Python
    my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(
        display_name=DISPLAY_NAME, # 端點的顯示名稱 (可以使用和索引相同的名稱)
        description=DISPLAY_NAME, # 端點的描述
        public_endpoint_enabled=False, # 重要：設定為 False，表示創建私有端點
        network=VPC_NETWORK_FULL, # 指定端點要連接的 VPC 網路
    )
    ```
    
    - **作用：** 在 Vertex AI 中創建索引端點資源。
    - `display_name`, `description`: 設定端點的元資料。
    - `public_endpoint_enabled=False`: **關鍵參數**。設定為 `False` 表示創建的是一個**私有端點**。這意味著該端點只能從您指定的 VPC 網路 (`network` 參數) 內部訪問，不能從公共網際網路直接訪問，提高了安全性。如果設為 `True`，則會創建一個公共端點，可以直接從網際網路訪問（需要適當的身份驗證）。
    - `network=VPC_NETWORK_FULL`: 將此端點與之前構建的完整 VPC 網路路徑關聯起來。Vertex AI 會在後台處理將服務連接到您的 VPC 所需的網路配置（例如 VPC Peering）。
    - **返回值：** 這個函數呼叫會創建端點資源，並回傳一個 `MatchingEngineIndexEndpoint` 物件，代表了這個端點。

---

### **Task 9. Deploy Indexes (部署索引)**

- **目的：** 將之前創建好的向量索引 (Matching Engine Index) 部署到剛剛創建的索引端點 (Index Endpoint) 上。只有部署完成後，索引才能真正開始處理查詢請求。
    
- **程式碼 1: 設定部署 ID (`DEPLOYED_INDEX_ID = ...`)**
    
    ```Python
    DEPLOYED_INDEX_ID = "deployed_index_id_unique" # 為這次部署設定一個唯一的 ID
    ```
    
    - **作用：** 為這次特定的部署操作指定一個唯一的標識符。一個索引端點理論上可以部署多個不同的索引（或同一個索引的不同版本），這個 ID 用於區分它們。
- **程式碼 2: 將索引部署到端點 (`my_index_endpoint = my_index_endpoint.deploy_index(...)`)**
    
    ```Python
    my_index_endpoint = my_index_endpoint.deploy_index(
        index=tree_ah_index, # 指定要部署的索引物件 (來自 Task 7)
        deployed_index_id=DEPLOYED_INDEX_ID # 指定這次部署的唯一 ID
    )
    
    print(my_index_endpoint.deployed_indexes) # 印出端點上已部署的索引信息
    ```
    
    - **作用：** 執行部署操作。
    - `my_index_endpoint.deploy_index()`: 呼叫 `MatchingEngineIndexEndpoint` 物件的 `deploy_index` 方法。
    - `index=tree_ah_index`: 傳遞在 Task 7 中創建並表示向量索引的 `MatchingEngineIndex` 物件 (`tree_ah_index`)。
    - `deployed_index_id=DEPLOYED_INDEX_ID`: 傳遞為這次部署設定的唯一 ID。
    - **執行時間：** 部署索引也需要一些時間，因為 Vertex AI 需要分配計算資源來託管索引，並將索引數據加載到這些資源上。
    - **更新物件/返回值：** 這個方法呼叫可能會更新 `my_index_endpoint` 物件的狀態，或者回傳一個更新後的物件。
    - `my_index_endpoint.deployed_indexes`: 部署操作完成後（可能是異步的，需要等待），可以訪問端點物件的 `deployed_indexes` 屬性，查看哪些索引已經成功部署到這個端點上及其狀態和 ID。

---

### **Task 10. Create Online Queries (建立線上查詢)**

- **目的：** 測試部署好的索引和端點是否能夠正常工作。這一步會發送一個查詢向量到端點，並取回與查詢最相似的向量的 ID。
    
- **程式碼 1: 編碼查詢文字 (`text_embeddings = encode_texts_to_embeddings(...)`)**
    
    ```Python
    # Encode query (將查詢文字轉換為嵌入向量)
    query_text = "New York skyline"
    text_embeddings = encode_texts_to_embeddings(text=[query_text]) # 使用與索引相同的編碼函數
    ```
    
    - **作用：** 將您想要查詢的文字 (例如 "New York skyline") 轉換成嵌入向量。
    - **關鍵：** 必須使用與創建索引時 **完全相同** 的嵌入模型和處理方式 (`encode_texts_to_embeddings` 函數) 來生成查詢向量。如果查詢向量和索引中的向量是用不同方式生成的，那麼相似度計算就沒有意義了。
- **程式碼 2: 執行向量匹配查詢 (`NUM_NEIGHBORS = ...`, `response = my_index_endpoint.match(...)`)**
    
    ```Python
    # Define number of neighbors to return (設定要返回的最近鄰數量)
    NUM_NEIGHBORS = 20
    
    # Ensure text_embeddings is not None and contains a valid embedding
    if text_embeddings and text_embeddings[0] is not None:
        query_embedding = text_embeddings[0] # Get the actual embedding vector
    
        # Perform the match query against the deployed index
        response = my_index_endpoint.match(
            deployed_index_id=DEPLOYED_INDEX_ID, # 指定要查詢哪個已部署的索引
            queries=[query_embedding], # 傳遞查詢向量列表 (即使只有一個也要放在列表裡)
            num_neighbors=NUM_NEIGHBORS, # 指定要返回多少個最近鄰
        )
    
        print(response) # 印出查詢結果
    else:
        print(f"Could not generate embedding for query: {query_text}")
        response = None # Set response to None if query failed
    ```
    
    - **作用：** 向部署好的索引端點發送查詢請求。
    - `NUM_NEIGHBORS`: 設定希望查詢返回多少個最相似的結果。
    - `if text_embeddings and text_embeddings[0] is not None:`: 檢查查詢文字是否成功轉換為嵌入向量。
    - `my_index_endpoint.match()`: 呼叫索引端點物件的 `match` 方法 (在某些舊版 SDK 中可能叫做 `find_neighbors`) 來執行查詢。
        - `deployed_index_id=DEPLOYED_INDEX_ID`: 指定要查詢的是端點上部署 ID 為 `DEPLOYED_INDEX_ID` 的那個索引。
        - `queries=[query_embedding]`: 提供一個包含查詢向量的列表。即使只查詢一個向量，也需要將其放入列表中。如果您想一次查詢多個向量，可以將它們都放入這個列表。
        - `num_neighbors=NUM_NEIGHBORS`: 告訴 Vector Search 您希望為每個查詢向量返回多少個最近鄰結果。
    - **返回值 `response`：** `match` 方法的回應通常是一個列表，列表中的每個元素對應一個查詢向量的結果。對於單個查詢，`response` 會是類似 `[[MatchNeighbor(id='...', distance=...), MatchNeighbor(id='...', distance=...), ...]]` 的結構。
        - `MatchNeighbor` 物件包含了找到的鄰居的資訊。
        - `id`: 鄰居向量的 ID (即您在創建索引時提供的 ID，這裡應該是圖片的檔名)。
        - `distance`: 查詢向量與這個鄰居向量之間的距離/相似度分數。**注意：** 這個值的意義取決於您在創建索引時設定的 `distance_measure_type`。
            - 如果是 `COSINE_DISTANCE`，值越小表示越相似 (0 表示完全相同，2 表示完全相反)。
            - 如果是 `DOT_PRODUCT_DISTANCE` 或原始點積，值的解釋可能不同（可能值越大越相似，或者是一個轉換後的值）。您需要根據 API 文件或實驗結果來確定。從後面的視覺化程式碼按 `distance` 降序排序來看，這裡的 `distance` 很可能是一個 **相似度分數** (越高越好)，而不是嚴格意義上的距離。
- **程式碼 3: 繪製查詢結果 (`sorted_data = ...`, `plt.show()`)**
    
    ```Python
    import os # 需要 os 模組來檢查檔案是否存在
    import copy # 可能需要複製圖像物件
    
    # Check if the response was successful
    if response and response[0]: # 確保 response 不是 None 且包含結果
        # Get the neighbors for the first query (因為我們只查詢了一個)
        neighbors = response[0]
    
        # Sort images and scores by descending order of scores (假設 distance 是相似度分數)
        sorted_data = sorted(neighbors, key=lambda x: x.distance, reverse=True)
    
        # --- (接下來的繪圖代碼與 Task 6 中的視覺化代碼非常相似) ---
        num_cols = 4
        # Ensure we don't try to create more rows/cols than needed
        num_results = len(sorted_data)
        num_rows = math.ceil(num_results / num_cols)
    
        if num_results == 0:
            print("No neighbors found.")
        else:
            fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, max(3, 3 * num_rows))) # 動態調整高度
    
            # Loop through the sorted neighbors
            for i, neighbor in enumerate(sorted_data):
                image_id = neighbor.id # 獲取圖片 ID (檔名)
                score = neighbor.distance # 獲取相似度分數
                # Construct the full image path using the directory defined in Task 4
                image_path = os.path.join(image_directory, image_id) # 使用 os.path.join 更安全
    
                # Calculate the row and column index
                row_idx = i // num_cols
                col_idx = i % num_cols
    
                # Get the correct subplot axis
                if num_rows == 1:
                    # Handle case with only one row explicitly if axs is 1D array
                    ax = axs[col_idx] if num_cols > 1 else axs
                else:
                    ax = axs[row_idx, col_idx]
    
                # Display the image if it exists
                if os.path.exists(image_path): # 檢查圖片檔案是否存在
                    try:
                        image = Image.open(image_path)
                        # image = copy.deepcopy(Image.open(image_path)) # Deepcopy 通常不需要
                        ax.imshow(image, cmap="gray")
                        # Set the title with Rank and Score
                        ax.set_title(f"Rank {i+1}, Score = {score:.2f}") # noqa
                    except Exception as e:
                        print(f"Error displaying image {image_path}: {e}")
                        ax.set_title(f"Rank {i+1}\nLoad Error")
                else:
                    # If image doesn't exist, show placeholder/text
                    ax.set_title(f"Rank {i+1}, Score = {score:.2f}\n(Image not found)")
                    print(f"Image file not found: {image_path}")
    
    
                # Remove ticks
                ax.set_xticks([])
                ax.set_yticks([])
    
            # Hide any unused subplots
            for j in range(i + 1, num_rows * num_cols):
                row_idx = j // num_cols
                col_idx = j % num_cols
                if num_rows == 1:
                    ax = axs[col_idx] if num_cols > 1 else axs
                else:
                    ax = axs[row_idx, col_idx]
                ax.axis('off') # Turn off axis for unused subplots
    
    
            # Adjust layout and display
            plt.tight_layout()
            #plt.subplots_adjust(hspace=0.3, wspace=0.1)
            plt.show()
    else:
        print("Query failed or returned no results.")
    
    ```
    
    - **作用：** 將從 Vector Search 返回的最近鄰結果（圖片 ID）視覺化，以便直觀地判斷搜索效果。
    - 這段程式碼的邏輯與 Task 6 中測試相似度時的視覺化程式碼非常相似。
    - `if response and response[0]:`: 檢查查詢是否成功並返回了結果。
    - `neighbors = response[0]`: 獲取第一個（也是唯一一個）查詢的結果列表。
    - `sorted(...)`: 根據 `neighbor.distance`（假設是相似度分數）對結果進行降序排序。
    - 迴圈遍歷排序後的 `neighbors`：
        - 獲取圖片 ID (`neighbor.id`) 和分數 (`neighbor.distance`)。
        - 使用 Task 4 中定義的 `image_directory` 和圖片 ID 構建本地圖片檔案的完整路徑 (`os.path.join(image_directory, image_id)`)。
        - **`if os.path.exists(image_path):`**: **重要檢查**，確認對應 ID 的圖片檔案確實存在於本地的 `extracted` 資料夾中。因為索引建立可能在雲端進行，或者本地檔案可能已變動，這個檢查是必要的。
        - 如果檔案存在，則使用 PIL 開啟圖片並在對應的子圖位置顯示，同時顯示排名和分數。
        - 如果檔案不存在，則在子圖位置給出提示。
    - `plt.tight_layout()` 和 `plt.show()`: 調整佈局並顯示圖像。

---

**Vertex AI 多模態嵌入與向量搜索實驗：工作流程**

這個實驗引導您完成建立一個**文字到圖片相似度搜索系統**的端到端流程。

1. **環境設定 (Setup & Environment):**
    
    - **啟用雲端服務:** 在 Google Cloud 專案中啟用必要的 API (Vertex AI API, Service Networking API)，這是使用雲端資源的前提。
    - **準備開發環境:** 啟動 Vertex AI Workbench 中的 JupyterLab 執行個體，提供一個整合的程式碼開發與執行平台。
    - **安裝依賴:** 安裝實驗所需的 Python 函式庫 (Vertex AI SDK, Cloud Storage SDK, Cloud Vision SDK 等)。
    - **配置基本參數:** 設定 Google Cloud 專案 ID、區域 (Region) 等環境變數。
    - **建立儲存空間:** 創建一個 Google Cloud Storage (GCS) 儲存桶，用於存放後續產生的資料 (如嵌入向量檔案)。
2. **資料獲取與準備 (Data Acquisition & Preparation):**
    
    - **下載資料:** 從網路來源 (DiffusionDB) 下載圖片資料集及其對應的元資料 (包含生成圖片的文字提示)。
    - **解壓與整理:** 將下載的壓縮檔解開，並讀取元資料，建立圖片檔案路徑與其資訊的對應關係。
3. **資料過濾與品質控制 (Data Filtering & Quality Control):**
    
    - **安全內容檢測:** (選用但建議) 使用 Cloud Vision API 的 SafeSearch 功能，對每張圖片進行不適當內容檢測。
    - **篩選資料集:** 根據 SafeSearch 結果，過濾掉被標記為可能不安全的圖片，只保留安全的圖片進行後續處理。
4. **多模態嵌入生成 (Multimodal Embedding Generation):**
    
    - **定義 API 客戶端:** 建立一個 Python 類別或函數，封裝呼叫 Vertex AI Multimodal Embedding API 的邏輯。
    - **設計批次處理:** 實現輔助函數，將大量圖片分批處理，並加入 API 速率限制控制、錯誤處理及自動重試機制，以確保穩定高效地生成嵌入向量。
    - **生成向量:** **核心步驟** - 遍歷所有**已過濾**的圖片，呼叫多模態嵌入 API，將每張圖片轉換成一個高維度的數學向量 (Embedding)。
    - **格式化輸出:** 將生成的每個嵌入向量與其對應的圖片 ID (檔名) 配對，並格式化成 JSON Lines (JSONL) 格式，每一行代表一個圖片的 ID 和其嵌入向量。
    - **儲存嵌入:** 將格式化好的 JSONL 資料寫入本地臨時檔案。
5. **資料上傳至 GCS (Stage Embeddings to GCS):**
    
    - **上傳檔案:** 將包含所有嵌入向量的本地 JSONL 檔案，上傳到步驟 1 中創建的 GCS 儲存桶的指定目錄下。
6. **向量搜索索引創建 (Vector Search Index Creation):**
    
    - **定義索引配置:** 指定索引的名稱、描述、嵌入向量的**維度 (Dimensions)**、用於計算相似度的**距離度量類型 (Distance Measure)** (如餘弦距離 COSINE_DISTANCE)，以及選擇的近似最近鄰 (ANN) 演算法及其相關參數 (如 Tree-AH 參數)。
    - **創建索引資源:** 使用 Vertex AI SDK 呼叫 API，創建一個 Matching Engine Index (向量索引) 資源。API 會從您在 GCS 中指定的路徑讀取嵌入向量資料來建構索引。 **(此步驟耗時較長)**
7. **向量搜索端點建立 (Vector Search Endpoint Setup):**
    
    - **配置網路:** 獲取專案的 VPC 網路資訊。
    - **創建端點資源:** 創建一個 Matching Engine Index Endpoint (索引端點) 資源。**關鍵：** 將其配置為**私有端點 (Private Endpoint)**，並指定它連接到您的 VPC 網路，以確保查詢的安全性和網路效率。
8. **索引部署 (Index Deployment):**
    
    - **部署操作:** 將步驟 6 中創建好的向量索引，部署到步驟 7 中創建的索引端點上。 **(此步驟也需要時間)** 只有部署完成後，端點才能接收查詢。
9. **線上查詢與驗證 (Online Querying & Validation):**
    
    - **編碼查詢:** 輸入一個**文字**查詢語句 (例如 "飛翔的小鳥")。
    - **生成查詢向量:** **使用與建立索引時完全相同的多模態嵌入模型**，將文字查詢語句轉換成一個嵌入向量。
    - **發送查詢:** 將查詢向量發送到已部署的索引端點。
    - **獲取結果:** 索引端點利用 ANN 索引快速查找，回傳與查詢向量最相似的 N 個圖片的 ID 及其相似度分數。
    - **視覺化驗證:** 根據返回的圖片 ID，從本地或 GCS 取回對應的圖片並顯示出來，直觀地評估文字到圖片的搜索效果。