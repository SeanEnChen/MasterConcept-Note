# 8.2.7 Lab - Use the PaLM API to Cluster Products Based on Descriptions

首先就是啟用 Notebook 和 Vertex AI API，然後建立一個帶有最新 tensorflow 版本的虛擬機，接著就可以使用 Jupyter Lab 打開這個 notebook。好了之後打開 Jupyter Lab，然後透過 gcloud 指令，將原始碼從 GCS 抓下來放到這個 VM 上面。接著就是把 Notebook run 完就好，但是要看 ipynb 的說明。

在這個 Notebook 裡面，一開始就是一樣先安裝和 import 套件，而 dataset 是用 GCP 上的資料，總共有五種 Category 的資料，按 Category sorted 排序，每個 Category 2000 筆資料。這個 Lab 用的是 `text-embedding-004` 這個模型，然後接下來把這些東西寫成 function，因為每次 batch 只能五筆資料，但一樣設計成 batch 匯入。因為這個 Lab 用的 Data 有 10000 筆，然後 Lab 的關係 quota 有被限制，所以 Lab 把轉換成 embedding 的部分註解掉了，可以根據自己的配額率限制更改 api_calls_per_minute 參數。

接下來就是用這些資料透過 K-Means 將他們分群，cluster 設定成 5，因為總共有五個 Category。透過 `kmeans = KMeans(n_clusters=5, n_init="auto").fit(embeddings)` 就可以進行模型的訓練，然後 `predictions = kmeans.predict(embeddings)` 進行預測。接著可以將訓練出來的 kmeans 模型儲存成 pkl 檔，然後也可以存進 GCS 裡面。Notebook 中提到雖然存在 GCS 沒問題，但是每次使用都還要先從 GCS 拿到模型然後載入才能使用，所以提供了另一種方式就是存到 Vertex AI 的 Model Registry。然後最後就是給一些新的資料，然後讓 kmeans 模型進行預測。