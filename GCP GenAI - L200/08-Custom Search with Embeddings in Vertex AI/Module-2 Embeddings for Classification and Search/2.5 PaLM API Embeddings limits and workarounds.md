# 8.2.5 PaLM API Embeddings limits and workarounds

**核心問題：使用 Vertex AI PaLM Embedding API 生成嵌入時，存在速率限制 (Rate Limits)，需要了解如何應對。**

**API 的主要限制（以錄製當時為準）：**

1. **請求頻率**：每分鐘最多允許發送 **100 個**請求 (Queries Per Minute, QPM)。
2. **單次請求文件數**：每次 API 請求能處理的文件數量也有限制（具體數值未說明）。
3. **單文件 Token 數**：每個輸入文件有 Token 數量的上限（具體數值未說明）。
4. **超限處理**：超過 Token 限制的文件會被**自動截斷**。（文中提到 MapReduce 可解決處理超長文件的問題，但此處重點是速率限制）。

**應對策略：當需要生成大量嵌入時，必須加入速率限制邏輯。**

**方法一：自訂速率限制邏輯 (Custom Rate Limiting Code)**

- **目的**：編寫程式碼來控制 API 呼叫的頻率，確保不超過限制。
- **運作邏輯（概念）**：
    1. 根據 QPM 限制（如 100 QPM）計算出每次呼叫之間的最短間隔時間（例如 60秒 / 100次 = 0.6秒/次）。
    2. 記錄呼叫 API 前的時間。
    3. 執行 API 呼叫（生成嵌入）。
    4. 記錄呼叫完成後的時間，計算實際花費時間。
    5. 比較實際花費時間和最短間隔時間。
    6. 如果呼叫完成得太快（花費時間 < 間隔時間），則讓程式**暫停 (sleep)** 一小段時間（兩者之差），湊足間隔時間。
    7. 如果呼叫花費時間已足夠長，則可以立即進行下一次呼叫。
    8. 重複此流程。
- **實作方式（範例）**：
    1. 建立一個繼承自 `VertexAIEmbeddings`（很可能是 LangChain 的類別）的新類別 (`CustomVertexAIEmbeddings`)。
    2. 在這個新類別中，實作上述的速率限制函數邏輯。
    3. 在處理文件列表時，呼叫這個帶有速率限制的函數。
    4. 設定**批次處理 (Batching)**：在每次受速率限制的 API 呼叫中，實際處理多少份文件。
- **相關參數設定**：
    - `EMBEDDING_QPM`：設定為 API 允許的每分鐘最大查詢數（例如 `100`）。
    - `EMBEDDING_NUM_BATCH`：設定每次批次呼叫中處理的文件數量（例如，可從 `5` 開始嘗試，視情況調整）。
- **使用**：實例化這個自訂的 `CustomVertexAIEmbeddings` 類別，並傳入設定好的參數。

**方法二：使用批次嵌入 API (Batch Prediction API - 注意：當時處於預覽版 Preview)**

- **目的**：提供一種「開箱即用」的方式來處理大量的離線嵌入生成任務。
- **輸入來源**：可以是 **BigQuery 表格**，或是儲存在 **Cloud Storage** 中的 **JSON Lines (JSONL)** 檔案。
- **處理能力**：每個批次請求最多可以處理 **30,000 個**提示（輸入實例）。
- **輸出位置**：結果會儲存回指定的 **Cloud Storage** 儲存桶中。
- **格式範例**：文中展示了 Cloud Storage 輸入/輸出所需的 JSONL 格式，以及 BigQuery 表格中對應的格式範例。

**總結：**

當需要大量生成 PaLM 嵌入而可能觸及 API 速率限制時，主要有兩種應對方法：一是自行編寫程式碼，加入速率控制和批次處理邏輯（例如繼承 LangChain 的類別來實作）；二是利用 Google Cloud 提供的（當時為預覽版）批次預測 API，它可以直接處理來自 BigQuery 或 Cloud Storage 的大量資料，適合離線批次任務。