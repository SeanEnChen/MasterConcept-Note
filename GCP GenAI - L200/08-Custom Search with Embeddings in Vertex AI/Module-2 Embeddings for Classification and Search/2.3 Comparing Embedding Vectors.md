# 8.2.3 Comparing Embedding Vectors

**核心目標：理解有了嵌入向量後，如何利用它們來找出相似的項目，以及如何分析和視覺化這些向量。**

**使用嵌入的核心概念：尋找相似性而非完全匹配**

- 使用嵌入時，目標不是尋找一模一樣的內容（這用傳統的反向索引更有效率），而是尋找**語意上最接近**的項目。
- 在向量空間中，向量之間的**距離越近**，代表它們所代表的原始資料（如文字、圖片）**相似度越高**。

**測量向量間相似度/距離的方法：**

1. **餘弦相似度 (Cosine Similarity)**：
    - 測量兩個向量之間的**角度**，關注**方向**的一致性。
    - 輸出值介於 -1 (完全相反) 到 +1 (方向完全相同) 之間。
    - 計算方式：兩向量的點積除以它們各自長度（模長）的乘積。
    - **特點**：標準化過程**消除了向量長度（大小/Magnitude）的影響**。這在比較不同長度的文本時特別有用，因為文本長度不一定代表語意相似度。
2. **點積 (Dot Product)**：
    - 同時考慮向量的**方向和大小（幅度）**。可以理解為一個向量投影到另一個向量上的長度。
    - **應用場景**：當向量的大小本身也具有意義時（例如，內容的受歡迎程度應影響其相關性）。
    - **Google Cloud 文件強烈建議**在向量搜尋中使用點積距離。
    - **提示**：如果不希望大小影響距離，可以先將向量進行**正規化 (Normalize)**（例如 L2 正規化，使所有向量長度變為 1），這樣點積就等同於餘弦相似度了。
3. **歐氏距離 (Euclidean Distance / L2)**：計算兩向量端點間的直線距離（最短距離）。
4. **曼哈頓距離 (Manhattan Distance / L1)**：計算各維度差的絕對值總和（計算上通常比歐氏距離快）。

- **如何選擇？** 可能需要實驗來確定哪種最適合你的嵌入和任務。若不確定，**建議從點積開始嘗試**，並考慮是否需要先進行正規化。

**實際計算相似度範例 (使用 Scikit-learn)**

- **工具**：使用 Python 的 `scikit-learn` 函式庫。
- **情境**：比較三句話的嵌入向量：句子 1 和 2 都關於程式語言，句子 3 關於狗追車。
- **預期與結果**：預期句子 1 和 2 的相似度很高，而句子 3 與前兩者相似度很低。使用 `scikit-learn` 計算餘弦相似度可以驗證這一點。

**視覺化嵌入 (Visualization)**

- **挑戰**：嵌入向量通常維度很高（例如 768 維），難以直接視覺化。
- **方法：降維 (Dimensionality Reduction)**
    - 使用 **主成分分析 (Principal Component Analysis, PCA)** 等算法。
    - PCA 能找出資料中變異最大的方向（主成分），並用這些方向將高維向量（如 768 維）**投影到低維空間**（如 2 維），同時盡量保留原始資料的主要結構。
- **繪圖步驟 (使用 Python 函式庫)**：
    1. 取得多個文本片段的嵌入向量。
    2. 使用 PCA 將這些高維向量降至 2 維。
    3. 使用 `pandas` 建立一個 DataFrame，包含降維後的兩個維度 (x, y) 以及原始的文本句子。
    4. 使用 `Seaborn` (基於 `matplotlib`) 根據這個 DataFrame 繪製散佈圖 (Scatter Plot)。
- **結果**：在 2D 散佈圖上，可以看到**語意相似的句子會聚集在一起**，形成不同的**集群 (Clusters)**。
    - **舉例**：提到動物和水相關活動的句子會聚在一起；關於食物的句子聚在一起；關於程式語言的句子也聚在一起。

**總結：**

有了嵌入向量後，我們透過計算它們之間的距離（常用如餘弦相似度、點積）來衡量語意相似度，並找出最接近的項目。雖然嵌入是高維的，但可利用 PCA 等降維技術，再搭配視覺化工具（如 Seaborn）將其繪製在 2D 圖上，直觀地觀察相似內容的聚集情況。