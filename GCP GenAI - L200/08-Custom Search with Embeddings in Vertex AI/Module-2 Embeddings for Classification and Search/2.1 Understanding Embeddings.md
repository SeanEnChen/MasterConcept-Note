# 8.2.1 Understanding Embeddings

**模組核心目標：**

- 深入探討「嵌入 (Embeddings)」這項技術，特別是它在**文字分類**和**語意搜尋**等自然語言處理任務中的應用。
- 建立在先前「向量搜尋和嵌入」課程的基礎之上。

**回顧傳統搜尋方法的挑戰：**

- **範例**：使用傳統方法在文件中搜尋關鍵字，例如「peanuts」（花生）。
- **問題點**：
    1. **效率低**：搜尋時間隨文件大小呈指數級增長。
    2. **缺乏語意理解**：
        - 無法捕捉同義詞或相似詞（搜 "peanuts" 找不到其他種類的 "nuts"，甚至找不到拼寫錯誤）。
        - 需要手動建立「停用詞 (Stop Words)」列表（如 "the", "a", "is"）來排除常見但無意義的詞彙，減少誤報。
    3. **需要輸入清理**：需要額外處理單複數等變形（例如希望搜 "peanuts" 也能找到 "peanut"）。

**嵌入 (Embeddings) 作為解決方案 (複習與深入)：**

- **再次強調**：向量搜尋利用嵌入技術，透過理解語意、支援多模態、實現個人化等方式克服了傳統搜尋的挑戰。嵌入是 AI 和機器學習（如 LLM、生成式 AI）的基礎。
- **對比 One-Hot 編碼**：複習了 One-Hot 編碼的缺點（稀疏、高維、無語意）。
- **字詞嵌入 (Word Embeddings) 的特性**：
    1. **定義**：是詞彙的**數字陣列（向量）**，能捕捉其**語意或含義**。
    2. **提供上下文 (Context)**：一個詞的嵌入會受到句子中**周圍詞彙**的影響，從而更好地理解其在特定情境下的意義。
        - **舉例**：在查詢「什麼時候應該再塗一層油漆？(When should I apply another coat of paint?)」中，「coat」的嵌入會因「paint」的存在而偏向「塗層」的意義，而不是「外套」的意義，從而避免搜到關於「寒冷天氣穿的外套」的無關結果。
    3. **語意相似性**：語意上相似的詞彙會產生數值上**更接近**的嵌入向量。
        - **舉例**：如果將嵌入向量視覺化（例如降維到 2D 繪圖），意思相近的詞彙或句子會**聚集在一起**。
    4. **捕捉關係**：向量間的數學關係能反映詞彙間的邏輯關係（再次提及 `king - man + woman ≈ queen` 的例子）。
    5. **生成方式**：由機器學習模型（如 Word2Vec, BERT, GPT, PaLM）在大量資料上訓練學習而來，無需手動指定數值。

**視覺化嵌入：**

- **數字陣列**：最原始的呈現方式。
- **彩色網格**：一種更直觀的視覺輔助，但高維度下，詞彙間的關係可能不明顯。
- **二維/三維圖**：透過降維技術繪製，可以清晰地看到**相似句子或詞彙的集群 (Clustering)** 效果。

**課程銜接：**

- 提醒若想複習基礎概念，可參考 Cloud Skills Boost 上的「向量搜尋和嵌入」課程。

**總結：**

這個模組聚焦於嵌入技術如何解決傳統搜尋在語意理解上的不足。透過生成能捕捉上下文和語意相似性的密集向量，嵌入使得更智能的分類和搜尋成為可能。理解嵌入的特性、生成方式以及如何從視覺化中觀察其效果，是掌握其應用的基礎。