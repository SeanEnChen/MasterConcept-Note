# 1.1 Attention Mechanism Overview

這段內容主要介紹了注意力機制，解釋了它為什麼重要（特別是對像 LLM 這樣的模型），以及它如何改進傳統的序列到序列模型（如用於機器翻譯的編碼器-解碼器模型）。

1. **背景問題：傳統 Encoder-Decoder 的侷限**
    - 傳統的序列到序列模型（例如用 RNN 實現的 Encoder-Decoder）在處理輸入序列時，會將整個輸入序列的資訊壓縮成一個單一的、固定長度的「隱藏狀態」（Hidden State），也稱為「上下文向量」（Context Vector），然後才傳遞給解碼器（Decoder）進行處理。
    - **問題點：** 這種做法可能導致資訊損失，特別是當輸入序列很長時。同時，如果輸入和輸出序列的詞語對應關係不是簡單的一對一順序（例如語序不同），模型很難準確處理。
    - **舉例：** 想要將英文句子 "The cat ate mouse" 翻譯成法文 "Le chat a mangé la souris"。如果使用傳統模型，編碼器讀完 "The cat ate mouse" 後只產生一個最終狀態給解碼器。
    - **更複雜的例子：** 翻譯 "black cat ate the mouse"。英文第一個詞是 "black"，但對應的法文第一個詞卻是 "chat"（貓）。傳統模型很難在解碼第一步時就準確地「跳過」"black" 去關注 "cat"。
	
2. **Attention Mechanism 是什麼？**
    - 是一種讓神經網路在處理序列數據時，能夠 **動態地專注於輸入序列中特定部分** 的技術。
    - 核心思想是為輸入序列的不同部分分配不同的「注意力權重」（Attention Weights）。與當前輸出步驟最相關的部分會獲得較高的權重。
    - 模型會根據這些權重計算輸入序列的「加權和」（Weighted Sum），這個加權和更能代表當前步驟所需的上下文資訊，然後再用於神經網路的下一步計算。
    - 模型根據大量數據中學習和歸納出來的結果，**自己學會了** 如何判斷和分配注意力，而不是被直接告知。
	
3. **Attention Mechanism 如何改進 Encoder-Decoder 模型？**
    - **傳遞更豐富的資訊：** 與傳統模型只傳遞 **最後一個** 隱藏狀態不同，帶有 Attention Mechanism 的編碼器會將 **所有** 時間步的隱藏狀態（每個隱藏狀態大致對應輸入序列中的一個詞）都傳遞給解碼器。這讓解碼器擁有了關於整個輸入序列更全面的上下文。
    - **解碼器的額外注意力步驟：** 在生成每一個輸出詞之前，注意力解碼器會執行以下步驟：
        1. 檢視它收到的所有編碼器隱藏狀態（例如，`h1, h2, h3,...`）。
        2. 根據當前解碼器的狀態（例如，`hd4`，表示正在生成第 4 個輸出詞），給每一個編碼器隱藏狀態（`h`）打一個分數，判斷它與當前要生成的輸出詞的相關性。
        3. 使用 Softmax 函數將這些分數轉換成一組「注意力權重」（`alpha`），這些權重相加為 1，代表了注意力應該如何分配。分數高的隱藏狀態獲得高權重。
        4. 計算這些編碼器隱藏狀態的加權和，得到一個**針對當前解碼步驟**的上下文向量（Context Vector，例如 `A4`）。`A4 = sum(alpha_i * h_i)`。這與傳統模型那個固定的上下文向量不同。
	
4. **整合運作流程：**
    - 在解碼的某個時間步（例如第 4 步）：
        - 計算該步的上下文向量 `A4`（通過上述注意力步驟）。
        - 將該上下文向量 `A4` 與當前解碼器的隱藏狀態 `hd4` **拼接（concatenate）** 起來。
        - 將這個拼接後的向量輸入一個前饋神經網路（Feedforward Neural Network）。
        - 這個網路的輸出結果就用來預測當前時間步（第 4 步）應該輸出的詞。
    - 這個過程會一直重複，直到解碼器生成代表句子結束的特殊符號為止。
	
5. **效果與優勢：**
    - Attention Mechanism 使得模型能夠更好地處理輸入和輸出之間非線性的對應關係。
    - **舉例：** 在 "black cat ate the mouse" 的翻譯中，注意力機制可以讓模型在生成法文 "chat"（貓）時主要關注英文的 "cat"，生成 "noir"（黑色）時主要關注 "black"，處理了詞序顛倒的問題。同樣，它可以讓模型在生成法文 "a mangé"（吃了，兩個詞）時，持續關注英文的 "ate"（一個詞）。
    - 通過在每一步都動態地聚焦於最相關的輸入部分，注意力機制顯著提高了序列到序列任務（如機器翻譯）的性能和準確性。

---

總結來說，注意力機制通過允許模型在處理序列時動態地關注輸入的不同部分，克服了傳統 Encoder-Decoder 模型資訊瓶頸和處理非線性對齊的困難，是現代許多先進模型（包括大型語言模型）取得成功的關鍵技術之一。