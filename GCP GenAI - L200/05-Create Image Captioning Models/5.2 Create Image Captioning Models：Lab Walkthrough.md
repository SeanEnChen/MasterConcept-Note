# 5.2 Create Image Captioning Models：Lab Walkthrough

**1. Introduction and Setup**

```Python
import time
from textwrap import wrap
import matplotlib.pylab as plt
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_hub as hub
from tensorflow.keras import Input
from tensorflow.keras.layers import (
    GRU,
    Add,
    AdditiveAttention,
    Attention,
    Concatenate,
    Dense,
    Embedding,
    LayerNormalization,
    Reshape,
    StringLookup,
    TextVectorization,
)
print(tf.version.VERSION)
```

- **目的：** 導入必要的 Python 函式庫，包括 TensorFlow、Keras 和 TensorFlow Datasets。`print(tf.version.VERSION)` 用於確認 TensorFlow 的版本。
- **影片內容關聯：** Takumi 在影片的開頭提到了環境設定，並說明了需要安裝的依賴項，包括 TensorFlow 和 Keras。這裡的程式碼就是導入這些函式庫。

**2. Read and prepare dataset**

```Python
# Change these to control the accuracy/speed
VOCAB_SIZE = 20000  # use fewer words to speed up convergence
ATTENTION_DIM = 512  # size of dense layer in Attention
WORD_EMBEDDING_DIM = 128

# InceptionResNetV2 takes (299, 299, 3) image as inputs
# and return features in (8, 8, 1536) shape
FEATURE_EXTRACTOR = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(
    include_top=False, weights="imagenet")
IMG_HEIGHT = 299
IMG_WIDTH = 299
IMG_CHANNELS = 3
FEATURES_SHAPE = (8, 8, 1536)
```

- **目的：** 定義一些超參數和設定。
    - `VOCAB_SIZE`: 定義詞彙表的大小，用於限制模型學習的詞語數量，以加快收斂速度。
    - `ATTENTION_DIM`: 注意力機制中密集層的大小。
    - `WORD_EMBEDDING_DIM`: 詞嵌入向量的維度。
    - `FEATURE_EXTRACTOR`: 選擇使用預訓練的 InceptionResNetV2 模型作為圖像特徵提取器。`include_top=False` 表示不包含模型的頂部（分類）層，`weights="imagenet"` 表示使用在 ImageNet 資料集上預訓練的權重。
    - `IMG_HEIGHT`, `IMG_WIDTH`, `IMG_CHANNELS`: 定義輸入圖像的高度、寬度和通道數，這是 InceptionResNetV2 模型的要求。
    - `FEATURES_SHAPE`: 定義 InceptionResNetV2 模型輸出的特徵圖的形狀。
- **影片內容關聯：** Takumi 提到了使用 InceptionResNetV2 作為編碼器模型，並說明了這些常數是根據該模型的定義而來的。


```Python
GCS_DIR = "gs://asl-public/data/tensorflow_datasets/"
BUFFER_SIZE = 1000

def get_image_label(example):
    caption = example["captions"]["text"][0]  # only the first caption per image
    img = example["image"]
    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))
    img = img / 255
    return {"image_tensor": img, "caption": caption}

trainds = tfds.load("coco_captions", split="train", data_dir=GCS_DIR)
trainds = trainds.map(
    get_image_label, num_parallel_calls=tf.data.AUTOTUNE).shuffle(BUFFER_SIZE)
trainds = trainds.prefetch(buffer_size=tf.data.AUTOTUNE)
```

- **目的：** 從 TensorFlow Datasets 加載 COCO captions 資料集，並進行初步的預處理。
    - `GCS_DIR`: 指定資料集在 Google Cloud Storage 上的位置。
    - `BUFFER_SIZE`: 用於 shuffle 資料集的緩衝區大小。
    - `get_image_label` 函數：
        - 從每個範例中提取第一條文本描述 (`caption`) 和圖像 (`image`)。
        - 將圖像縮放到預定義的高度和寬度 (`IMG_HEIGHT`, `IMG_WIDTH`)。
        - 將像素值從 [0, 255] 縮放到 [0, 1]。
        - 返回包含處理後的圖像 (`image_tensor`) 和描述 (`caption`) 的字典。
    - `tfds.load("coco_captions", split="train", data_dir=GCS_DIR)`: 加載 COCO captions 資料集的訓練集。
    - `.map(get_image_label, num_parallel_calls=tf.data.AUTOTUNE)`: 使用 `get_image_label` 函數並行地處理資料集中的每個範例。
    - `.shuffle(BUFFER_SIZE)`: 隨機打亂資料集。
    - `.prefetch(buffer_size=tf.data.AUTOTUNE)`: 預先載入後續的資料批次，以提高效能。
- **影片內容關聯：** Takumi 說明了使用 TensorFlow Datasets 加載 COCO captions 資料集，並展示了一些圖像和對應的文本描述範例。

**3. Text Preprocessing**

```Python
def add_start_end_token(data):
    start = tf.convert_to_tensor("<start>")
    end = tf.convert_to_tensor("<end>")
    data["caption"] = tf.strings.join(
        [start, data["caption"], end], separator=" "
    )
    return data

trainds = trainds.map(add_start_end_token)
```

- **目的：** 在每個文本描述的開頭添加 `<start>` 標記，在結尾添加 `<end>` 標記。
- **影片內容關聯：** Takumi 強調了添加起始和結束標記的重要性，因為這是一個編碼器-解碼器模型，在預測時需要 `<start>` 標記來啟動生成過程，並使用 `<end>` 標記來標示句子的結束。

```Python
MAX_CAPTION_LEN = 64

def standardize(inputs):
    inputs = tf.strings.lower(inputs)
    return tf.strings.regex_replace(
        inputs, r"[!\"#$%&\(\)\*\+.,-/:;=?@\[\\\]^_`{|}~]?", ""
    )

tokenizer = TextVectorization(
    max_tokens=VOCAB_SIZE,
    standardize=standardize,
    output_sequence_length=MAX_CAPTION_LEN,
)
tokenizer.adapt(trainds.map(lambda x: x["caption"]))
```

- **目的：** 使用 `TextVectorization` 層將文本描述轉換為整數序列（token）。
    - `MAX_CAPTION_LEN`: 設定所有輸出序列的最大長度為 64。如果描述的長度超過這個值，將會被截斷；如果長度不足，將會被填充。
    - `standardize` 函數：將文本轉換為小寫，並移除標點符號。這裡覆蓋了 `TextVectorization` 的預設標準化方法，以保留 `<` 和 `>` 字符，確保起始和結束標記不被移除。
    - `TextVectorization`: 創建一個文本向量化器。
        - `max_tokens=VOCAB_SIZE`: 設定詞彙表的最大大小為之前定義的 `VOCAB_SIZE`。
        - `standardize=standardize`: 使用自定義的標準化函數。
        - `output_sequence_length=MAX_CAPTION_LEN`: 設定輸出序列的長度。
    - `tokenizer.adapt(trainds.map(lambda x: x["caption"]))`: 根據訓練資料集中的所有文本描述來建立詞彙表。這個過程會遍歷所有的描述，將其分割成單詞，並選擇最常見的 `VOCAB_SIZE` 個單詞作為詞彙表。
- **影片內容關聯：** Takumi 詳細解釋了創建 tokenizer 的過程，將單詞（包括起始和結束標記）映射到唯一的整數索引。

```Python
word_to_index = StringLookup(
    mask_token="", vocabulary=tokenizer.get_vocabulary())
index_to_word = StringLookup(
    mask_token="", vocabulary=tokenizer.get_vocabulary(), invert=True)
```

- **目的：** 創建詞語到索引和索引到詞語的查找表，方便後續的轉換。
    - `StringLookup`: Keras 提供的一個層，用於創建查找表。
    - `word_to_index`: 將詞彙表中的每個單詞映射到其對應的索引。`mask_token=""` 表示不使用遮罩標記。
    - `index_to_word`: 將索引映射回其對應的詞語。`invert=True` 表示進行反向查找。
- **影片內容關聯：** Takumi 提到了創建這些轉換器的實用性，方便在訓練和預測過程中進行詞語和索引之間的轉換。

**4. Create a tf.data dataset for training**

```Python
BATCH_SIZE = 32

def create_ds_fn(data):
    img_tensor = data["image_tensor"]
    caption = tokenizer(data["caption"])

    target = tf.roll(caption, -1, 0)
    zeros = tf.zeros([1], dtype=tf.int64)
    target = tf.concat((target[:-1], zeros), axis=-1)
    return (img_tensor, caption), target

batched_ds = (
    trainds.map(create_ds_fn)
    .batch(BATCH_SIZE, drop_remainder=True)
    .prefetch(buffer_size=tf.data.AUTOTUNE))
```

- **目的：** 創建用於訓練的 `tf.data.Dataset`。
    - `BATCH_SIZE`: 設定訓練的批次大小為 32。
    - `create_ds_fn` 函數：
        - 接收包含圖像張量和文本描述的資料。
        - 使用之前創建的 `tokenizer` 將文本描述轉換為整數序列。
        - 創建目標標籤 (`target`)。目標標籤是將輸入的描述向左移動一個位置，並在末尾填充 0。例如，如果輸入是 `<start> I love cats <end>`, 目標將是 `I love cats <end> <padding>`。這樣做的目的是讓模型學習根據當前詞語預測下一個詞語。
        - 返回一個 tuple -> `((img_tensor, caption), target)`，其中 `img_tensor` 是圖像特徵，`caption` 是輸入的詞語序列，`target` 是目標詞語序列 (label)。
    - `trainds.map(create_ds_fn)`: 將 `create_ds_fn` 函數應用於訓練資料集中的每個範例。
    - `.batch(BATCH_SIZE, drop_remainder=True)`: 將資料集分成大小為 `BATCH_SIZE` 的批次。`drop_remainder=True` 表示如果最後一個批次的樣本數量小於 `BATCH_SIZE`，則將其丟棄。
    - `.prefetch(buffer_size=tf.data.AUTOTUNE)`: 預先載入後續的資料批次，以提高效能。
- **影片內容關聯：** Takumi 詳細解釋了如何創建目標標籤，通過將輸入描述向左移動一個位置來預測下一個詞語。

**5. Model Architecture**

**Image Encoder**

```Python
FEATURE_EXTRACTOR.trainable = False
image_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))
image_features = FEATURE_EXTRACTOR(image_input)
x = Reshape((FEATURES_SHAPE[0] * FEATURES_SHAPE[1], FEATURES_SHAPE[2]))(
    image_features)
encoder_output = Dense(ATTENTION_DIM, activation="relu")(x)
```

- **目的：** 定義圖像編碼器模型。
    - `FEATURE_EXTRACTOR.trainable = False`: 凍結預訓練的 InceptionResNetV2 模型的權重，使其在訓練過程中不會被更新。這樣可以節省訓練時間並利用預訓練的知識。
    - `image_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))`: 定義編碼器的輸入層，接收形狀為 `(299, 299, 3)` 的圖像。
    - `image_features = FEATURE_EXTRACTOR(image_input)`: 將輸入圖像傳遞給預訓練的特徵提取器。
    - `x = Reshape((FEATURES_SHAPE[0] * FEATURES_SHAPE[1], FEATURES_SHAPE[2]))(image_features)`: 將特徵提取器輸出的形狀從 `(8, 8, 1536)` 重塑為 `(64, 1536)`。這裡將 8x8 的空間特徵圖展平成一個包含 64 個向量的序列，每個向量的維度是 1536。
    - `encoder_output = Dense(ATTENTION_DIM, activation="relu")(x)`: 使用一個全連接層將展平後的特徵向量的維度從 1536 降低到 `ATTENTION_DIM` (512)，並使用 ReLU 激活函數。
- **影片內容關聯：** Takumi 說明了編碼器的作用是提取圖像的特徵，並使用了預訓練的 InceptionResNetV2 模型。他還提到了凍結模型權重以利用預訓練知識。

```Python
encoder = tf.keras.Model(inputs=image_input, outputs=encoder_output)
encoder.summary()
```

- **目的：** 創建編碼器模型並顯示其結構。

**Caption Decoder**

```Python
word_input = Input(shape=(MAX_CAPTION_LEN,), name="words")
embed_x = Embedding(VOCAB_SIZE, ATTENTION_DIM)(word_input)
decoder_gru = GRU(
    ATTENTION_DIM,
    return_sequences=True,
    return_state=True,)
gru_output, gru_state = decoder_gru(embed_x)
decoder_attention = Attention()
context_vector = decoder_attention([gru_output, encoder_output])
addition = Add()([gru_output, context_vector])
layer_norm = LayerNormalization(axis=-1)
layer_norm_out = layer_norm(addition)
decoder_output_dense = Dense(VOCAB_SIZE)
decoder_output = decoder_output_dense(layer_norm_out)
```

- **目的：** 定義文本描述解碼器模型，該模型使用了注意力機制。
    - `word_input = Input(shape=(MAX_CAPTION_LEN,), name="words")`: 定義解碼器的輸入層，接收形狀為 `(MAX_CAPTION_LEN,)` 的詞語索引序列。
    - `embed_x = Embedding(VOCAB_SIZE, ATTENTION_DIM)(word_input)`: 使用 `Embedding` 層將輸入的詞語索引轉換為詞嵌入向量。`VOCAB_SIZE` 是詞彙表的大小，`ATTENTION_DIM` 是嵌入向量的維度。
    - `decoder_gru = GRU(...)`: 創建一個 GRU 層。
        - `ATTENTION_DIM`: GRU 單元的數量（輸出維度）。
        - `return_sequences=True`: 表示 GRU 將返回每個時間步的輸出序列。
        - `return_state=True`: 表示 GRU 將返回最後一個時間步的隱藏狀態。
    - `gru_output, gru_state = decoder_gru(embed_x)`: 將詞嵌入向量傳遞給 GRU 層，得到每個時間步的輸出 (`gru_output`) 和最後一個隱藏狀態 (`gru_state`)。
    - `decoder_attention = Attention()`: 創建一個注意力層。這裡使用的是 Luong 風格的注意力機制。
    - `context_vector = decoder_attention([gru_output, encoder_output])`: 將 GRU 的輸出 (`gru_output`) 作為查詢 (query)，編碼器的輸出 (`encoder_output`) 作為鍵 (key) 和值 (value) 傳遞給注意力層，計算上下文向量 (`context_vector`)。上下文向量表示模型在生成下一個詞語時應該關注的圖像區域的加權平均。
    - `addition = Add()([gru_output, context_vector])`: 將 GRU 的輸出和上下文向量相加（跳躍連接或殘差連接）。
    - `layer_norm = LayerNormalization(axis=-1)`: 對相加後的結果進行層標準化。
    - `layer_norm_out = layer_norm(addition)`: 應用層標準化。
    - `decoder_output_dense = Dense(VOCAB_SIZE)`: 使用一個全連接層將標準化後的輸出轉換為詞彙表大小的向量，表示每個詞語的預測 logits。
    - `decoder_output = decoder_output_dense(layer_norm_out)`: 應用全連接層。
- **影片內容關聯：** Takumi 詳細解釋了解碼器的架構，包括詞嵌入、GRU、注意力機制以及跳躍連接和層歸一化。他還提到了注意力機制如何讓模型關注圖像的不同部分。

```Python
decoder = tf.keras.Model(
    inputs=[word_input, encoder_output], outputs=decoder_output)
tf.keras.utils.plot_model(decoder)
decoder.summary()
```

- **目的：** 創建解碼器模型並顯示其結構。解碼器的輸入是詞語序列和編碼器的輸出（圖像特徵）。

**6. Training Model**

```Python
image_caption_train_model = tf.keras.Model(
    inputs=[image_input, word_input], outputs=decoder_output)
```

- **目的：** 將編碼器和解碼器組合在一起，創建最終的訓練模型。訓練模型的輸入是圖像和詞語序列，輸出是解碼器的預測。

**7. Loss Function**

```Python
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction="none")

def loss_function(real, pred):
    loss_ = loss_object(real, pred)

    # returns 1 to word index and 0 to padding (e.g. [1,1,1,1,1,0,0,0,0,...,0])
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    mask = tf.cast(mask, dtype=tf.int32)
    sentence_len = tf.reduce_sum(mask)
    loss_ = loss_[:sentence_len]

    return tf.reduce_mean(loss_, 1)
```

- **目的：** 定義訓練過程中使用的損失函數。
    - `loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction="none")`: 使用稀疏分類交叉熵作為損失函數。`from_logits=True` 表示模型的輸出是未經 softmax 激活的 logits。`reduction="none"` 表示對每個樣本的損失單獨計算，而不是求平均。
    - `loss_function(real, pred)` 函數：
        - 計算真實標籤 (`real`) 和模型預測 (`pred`) 之間的交叉熵損失。
        - 創建一個遮罩 (`mask`)，用於識別真實標籤中的非填充部分（非零值）。
        - 計算每個句子的實際長度 (`sentence_len`)。
        - 只保留每個句子中實際詞語的損失值，忽略填充部分的損失。
        - 返回每個句子的平均損失。
- **影片內容關聯：** Takumi 說明了使用稀疏分類交叉熵作為損失函數，並解釋了如何通過遮罩忽略填充部分對損失的影響。

**8. Training loop**

```Python
image_caption_train_model.compile(
    optimizer="adam",
    loss=loss_function,)
```

- **目的：** 編譯訓練模型，指定優化器 (`adam`) 和損失函數 (`loss_function`)。

```Python
%%time
history = image_caption_train_model.fit(batched_ds, epochs=1)
```

- **目的：** 使用 `model.fit` API 訓練模型一個 epoch。
- **影片內容關聯：** Takumi 提到了訓練一個 epoch 大約需要 15-20 分鐘。

**9. Inference Model**

```Python
gru_state_input = Input(shape=(ATTENTION_DIM,), name="gru_state_input")

gru_output, gru_state = decoder_gru(embed_x, initial_state=gru_state_input)

context_vector = decoder_attention([gru_output, encoder_output])
addition_output = Add()([gru_output, context_vector])
layer_norm_output = layer_norm(addition_output)
decoder_output = decoder_output_dense(layer_norm_output)

decoder_pred_model = tf.keras.Model(
    inputs=[word_input, gru_state_input, encoder_output],
    outputs=[decoder_output, gru_state],)
```

- **目的：** 創建一個專門用於預測的解碼器模型。這個模型與訓練時使用的解碼器共享權重，但它允許我們手動控制 GRU 的隱藏狀態，這在自迴歸生成文本時非常重要。
    - `gru_state_input = Input(shape=(ATTENTION_DIM,), name="gru_state_input")`: 為初始 GRU 狀態創建一個輸入層。
    - `gru_output, gru_state = decoder_gru(embed_x, initial_state=gru_state_input)`: 重用訓練好的 `decoder_gru` 層，但這次允許我們通過 `initial_state` 參數傳入初始隱藏狀態。
    - 後續的層（attention layer、add layer、normalization layer、dense layer）也重用了訓練好的權重。
    - `decoder_pred_model`: 定義預測模型，其輸入包括詞語序列 (`word_input`)、初始 GRU 狀態 (`gru_state_input`) 和編碼器的輸出 (`encoder_output`)。輸出包括下一個詞語的預測 (`decoder_output`) 和更新後的 GRU 狀態 (`gru_state`)。
- **影片內容關聯：** Takumi 強調了在推論階段需要手動控制 GRU 狀態，因此創建了一個單獨的預測模型。

**10. Prediction Function**

```Python
MINIMUM_SENTENCE_LENGTH = 5

def predict_caption(filename):
    gru_state = tf.zeros((1, ATTENTION_DIM))

    img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)
    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))
    img = img / 255

    features = encoder(tf.expand_dims(img, axis=0))
    dec_input = tf.expand_dims([word_to_index("<start>")], 1)
    result = []
    for i in range(MAX_CAPTION_LEN):
        predictions, gru_state = decoder_pred_model(
            [dec_input, gru_state, features]
        )

        # draws from log distribution given by predictions
        top_probs, top_idxs = tf.math.top_k(
            input=predictions[0][0], k=10, sorted=False
        )
        chosen_id = tf.random.categorical([top_probs], 1)[0].numpy()
        predicted_id = top_idxs.numpy()[chosen_id][0]

        result.append(tokenizer.get_vocabulary()[predicted_id])

        if predicted_id == word_to_index("<end>"):
            return img, result

        dec_input = tf.expand_dims([predicted_id], 1)

    return img, result
```

- **目的：** 定義一個函數 `predict_caption`，用於給定圖像檔案名生成圖像標註。
    - `MINIMUM_SENTENCE_LENGTH`: 設定生成標註的最小長度。
    - `gru_state = tf.zeros((1, ATTENTION_DIM))`: 將初始 GRU 狀態設置為零向量。
    - 讀取並預處理輸入圖像，包括解碼 JPEG、調整大小和縮放像素值。
    - 使用訓練好的 `encoder` 模型提取圖像特徵。
    - `dec_input = tf.expand_dims([word_to_index("<start>")], 1)`: 將起始標記 `<start>` 作為解碼器的初始輸入。
    - 使用一個迴圈迭代生成標註，直到達到最大長度 (`MAX_CAPTION_LEN`) 或生成結束標記 `<end>`。
        - 將當前輸入詞語 (`dec_input`)、GRU 狀態 (`gru_state`) 和圖像特徵 (`features`) 傳遞給 `decoder_pred_model`，得到下一個詞語的預測 (`predictions`) 和更新後的 GRU 狀態。
        - 從預測的 logits 中選擇下一個詞語。這裡使用了概率抽樣的方法，從 top-10 最可能的詞語中隨機選擇一個，以增加生成的多樣性。
        - 將預測的詞語 ID 轉換回詞語，並添加到結果列表 `result` 中。
        - 如果預測的詞語是結束標記 `<end>`, 則停止生成。
        - 將預測的詞語 ID 作為下一次迭代的輸入。
    - 返回原始圖像和生成的標註（詞語列表）。
- **影片內容關聯：** Takumi 詳細解釋了推論階段的步驟，包括初始化 GRU 狀態、使用起始標記開始生成、迭代地預測下一個詞語以及遇到結束標記時停止。他還提到了概率抽樣以增加生成的多樣性。

**11. Captioning Example**

```Python
filename = "../sample_images/baseball.jpeg"  # you can also try surf.jpeg
for i in range(5):
    image, caption = predict_caption(filename)
    print(" ".join(caption[:-1]) + ".")
img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)
plt.imshow(img)
plt.axis("off")
```

- **目的：** 使用 `predict_caption` 函數為一個範例圖像生成 5 個不同的標註，並顯示原始圖像。
- **影片內容關聯：** Takumi 展示了如何使用 `predict_caption` 函數來生成圖像標註，並展示了模型在 baseball.jpeg 上的生成結果。

**12. Summary**

這部分總結了建立圖像標註模型的關鍵步驟，包括創建圖像編碼器和文本解碼器。

希望這個詳細的解釋能夠幫助你理解影片內容和程式碼的每個部分。