# 8.1.2 Methods of testing

**探索 Virtual Agent 的測試方法**

本節旨在探討測試 **virtual agent (虛擬客服)** 的各種方法。目標包含：了解最常用的測試方法選項、它們的目的、在開發生命週期的哪個階段進行，以及由誰負責管理每個測試階段。

**考量系統互動：**

在思考測試方法時，需考量所有與 **Dialogflow CX** 及客戶互動的系統，通常包含：

- **Telephony Integration component (電話整合元件)：** 處理無縫通話轉換。
- **Client services application (客戶端服務應用)：** 如 web 或 mobile application，用於呈現使用者介面訊息。
- **Backend systems (後端系統)：** 提供更深入的客戶資訊，通常也協助執行特定動作 (如 payments 或更新資訊)。

_(注意：本課程主要關注與 Dialogflow CX 直接相關的測試方法，聚焦於 intent recognition, routing accuracy, response generation)_

**四層測試方法 (Four Tiered Approach)：**

對於 Dialogflow CX 中的 virtual agents，最成功的測試策略基於四層方法：

1. **Unit Tests (單元測試)**
2. **Integration tests (整合測試)**
3. **End to End tests (E2E - 端到端測試)**
4. **A/B tests (A/B 測試)**

- **職責劃分：**
    - **Development team (開發團隊)** 負責前兩種測試 (Unit, Integration)，發生在開發生命週期早期。
    - **Quality Assurance (QA) team (品質保證團隊)** 更專注於後兩種測試 (E2E, A/B)，發生在接近 production deployment (生產部署) 時。
- **開發者責任：** 確保在任何開發工作前後**沒有 test cases (測試案例) 失敗**；為新 feature (功能) 建立新測試以比較 bot behavior (機器人行為) 與定義的 requirements (需求)。
- **自動化程度：** **Unit** 和 **integration tests** 預期**大部分是自動化的 (mostly automated)**，以便確認 utterances 的完整覆蓋、無 regressions (回歸問題) 及 APIs 的正確運作。**End to end** 和 **A/B tests** 則需要**更高程度的手動投入 (manual effort)**，以更仔細地評估最複雜的 use cases。
- **環境要求：** 無論哪個階段，測試都應在**盡可能與 production environment (生產環境) 相似**的環境中進行。

**各測試類型詳解：**

1. **Unit Tests (單元測試)：**
    - **類型：**
        - **Routing Unit tests (路由單元測試)：** 確保 **customer user journey (客戶使用者旅程)** 中步驟之間的 **logical transitions (邏輯轉換)** 按預期運作。**所有 pages 和 transitions 都應至少有一個 unit test 覆蓋**。由建構這些 routes 的 development team 負責。
        - **Natural Language Understanding (NLU) Unit testing (NLU 單元測試)：** 確保 virtual agent 的 NLU model (模型) 表現最佳，能正確識別**多樣 user utterances (使用者語句)** 的 **intent (意圖)** (包含 phrasing, synonyms, informal language 的變化)。涵蓋 **intent recognition (意圖識別)** 和 **entity extraction (實體提取)** (如 dates, locations, product names)。應使用**大量範例 utterances** 來找出可透過增/移/刪 training phrases 來 **fine tuned (微調)** NLU 的地方。同時需確保 agent 對於在 CUJ 特定點**非預期的 user inputs** 有適當的回應 (例如，在 page level 客製化 **no-match responses** 來重複提示或選項)。(例：收集序號時，需處理用戶回覆 "I don't know", "None of them", "All of them" 等情況的 intents)。
    - **執行：** 大多數可在 **Dialogflow CX sandbox** 中執行。
    - **原則：** 遵循 **zero failing test cases policy (零失敗測試案例政策)**。有助於 **regression testing (回歸測試)**。
    - **依賴處理：** 若 webhook 在 dev 環境不可用，可用 **Webhook Parrot** 等工具進行 **mocked (模擬)**。
2. **Integration System Testing (整合系統測試)：**
    - **目標：** 發現 components 之間 **interfaces (介面)** 的錯誤，並驗證各層級的 modules (模組) 能協同工作。預期盡可能驗證 **end-to-end application flows (端到端應用流程)** 的完整範圍。
    - **範圍：** 通常包含對外部 (應用程式或組織外部) 的 services, applications, systems, components 的 **invocations (調用)**。
    - **環境：** 在**類似 production 的環境**中進行能顯著簡化測試。
    - **負責人：** 盡可能由 development team 執行。
    - **範例：** **Dual-tone multi-frequency (DTMF)** 或 numerical input, **barge in (插話)**, **webhook timeouts (超時)**, **telephony integration (電話整合)**。開發者需確保需要電話整合的 test cases 得到完整覆蓋。
3. **End to End Testing (E2E) / User Acceptance Testing (UAT - 使用者驗收測試)：**
    - **目標：** 由 **QA team** 確保所有 **customer experience requirements (客戶體驗需求)** 都被滿足，且 Virtual agent 能應對 **real world scenarios (真實世界場景)**。
    - **方法：** 測試完整的 **catalog of customer user journeys (CUJ 目錄)**，並與 virtual agent 進行 **adversarially (對抗性)** 互動。
    - **環境：** 同樣建議在 **production-like environment (類生產環境)** 中進行。
4. **A/B Testing (A/B 測試)：**
    - **定位：** 暴露給 **live customers (真實客戶)** 之前的**最終認證階段 (final phase of the certification process)**。
    - **方法：** 一種 **UX methodology (使用者體驗方法論)**，透過 **randomized experiment (隨機實驗)** 來比較兩個 systems 或 conversational experiences 的 performance，以驗證哪個版本更好地滿足專案範疇或規格中概述的 functional requirements。
    - **比較範例：**
        - **Legacy IVA experience (舊互動語音應答體驗)** vs. **新開發的 Dialogflow CX 體驗**。
        - **Live agent (真人客服)** 處理的客戶體驗 vs. **新開發的 Dialogflow CX 體驗**。(只要能證明在通話處理體驗上有改進即可)。
    - **負責人：** 通常由 QA team 管理。

**測試階段與環境對應：**

- **Development Environment (開發環境)：** Unit Testing, Integration Testing。
- **QA Environment (測試環境)：** End to End Testing (UAT)。
- **Pre-prod Environment (預生產環境)：** A/B Testing。
- **重要性：** 在盡可能接近 Production 的環境中執行測試非常重要 (尤其是 Integration 和 E2E)，以便獲得更準確的反饋。若無法達成，應有 **mitigation plan (緩解計畫)** (如使用 mock webhooks/systems 或 small batch releases - 小批量發布)。

**額外的測試策略 (常規或 Ad Hoc)：**

除了上述主要測試階段，還有其他策略可確保持續品質 (常使用 Dialogflow CX 外部的 custom tooling 管理)：

- **Design Testing (設計測試)：** 確保正確 intents 被預期 utterances 觸發，提供 UI experience 反饋 (由 Design team 在 Dev 環境執行)。
- **Performance Testing (效能測試 / Telco Infrastructure Test - 電信基礎設施測試)：** 評估 VA 和其他 infrastructure components 在不同負載下的效能 (response times, accuracy under load, simultaneous interactions) (由 Architects/Telco Engineers 在 QA/Pre-prod 執行)。
- **Regression Testing (回歸測試)：** 確保新增/更新 features 後，現有功能仍正常運作 (由 QA team 在 E2E/UAT 中負責評估)。
- **Adversarial Testing (對抗性測試)：** QA team 故意嘗試使 agent 失敗，以找出新的 vulnerabilities (漏洞) 或需改進以防 escalations 的地方 (在 QA/Pre-prod 環境)。
- **Failover and Recovery Testing (容錯移轉與恢復測試)：** 評估 agent 如何處理和從 system failures 中恢復，以及期間的 data integrity (資料完整性) (由 QA team 在 QA/Pre-prod 按需執行)。
- **Beta Testing (Beta 測試)：** 在全面 launch (上線) 前，讓 agent 在完整實施狀態下進行測試，但避免將 defects 暴露給大量受眾。

**評估與追蹤測試結果：**

- **評估：** 可利用 Dialogflow CX 的 **test case interface (測試案例介面)** 或開發自己的 test running scripts 和 visuals (視覺化儀表板)。
- **追蹤：** 定義 **tracking system (追蹤系統)** 很重要，包含：
    - **Granular record keeping (精細記錄保存)：** 維護 log，分類每次 release 的 test case results (pass/fail status, associated defects, relevant observations)。
    - **分析與反饋：** 分析結果以識別 trends, patterns, recurring issues，為 root cause analysis 和改進策略提供資訊。建立從 QA 到 development team 的 feedback loop process。
    - **知識共享：** 將結果彙編用於 root cause resolution forums，以透過 preventative measures (預防措施) 限制未來發生。
- **目標：** 穩健的測試策略能在開發生命週期中創造一個促進持續改進 agent quality throughput (品質產出) 的 **virtuous cycle (良性循環)**。

**總結來說，** 為 Dialogflow CX Virtual Agent 建立成功的測試策略需要一個多層次的方法，通常包含開發團隊負責的 Unit Tests (路由邏輯、NLU準確性) 和 Integration Tests (元件接口、端到端流程初步驗證，含外部依賴)，以及 QA 團隊負責的 End-to-End/UAT Testing (驗證完整 CUJ、對抗性測試) 和 A/B Testing (比較版本優劣)。這些測試應盡可能在類生產環境中執行，並輔以 Design, Performance, Regression, Failover, Beta 等其他測試策略。最後，必須建立有效的結果評估與追蹤機制，利用測試數據反饋開發流程，形成持續改進的良性循環。