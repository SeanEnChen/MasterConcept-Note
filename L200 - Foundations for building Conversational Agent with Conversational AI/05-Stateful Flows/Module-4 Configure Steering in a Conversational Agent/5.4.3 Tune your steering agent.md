# 5.4.3 Tune your steering agent

**調整 Steering Agent**

在配置好 steering agent 後，下一步是 **tune (調整)** 它。本節旨在探索識別 steering agent 潛在問題的各種方法，特別是在進入 production (生產環境) 之前，包含使用 **validation tool (驗證工具)**、**test sets (測試集)** 和 **scripts (腳本)**。

**1. 使用 Validation Tool 識別問題：**

- **定義：** **Conversational Agents (對話式代理程式)** 的 **native capability (原生功能)**，用於在 agent 到達 production 之前識別其中的問題。可在 **“Manage” tab (管理分頁)** 下找到。
- **作用：** Validation results (驗證結果) 僅供 **advisory purpose (建議性質)**，不影響 agent 的行為，但處理其指出的問題有助於提升 agent 品質 (確保有效理解並準確、及時回應)。
- **常見問題類型：**
    - 不同 intents 的 **training phrases (訓練語句)** 過於相似。
    - **Parameter value (參數值)** 沒有定義的 entity (實體)。
    - **Text (文本)** 在某些 training phrases 中被 annotated (標註)，但在其他中沒有。
    - 同一個 **synonym (同義詞)** 在一個 entity type 中被用於不同的 values (值)。
- **嚴重性級別 (Severity Types)：**
    - **Errors (錯誤)：** 阻礙處理使用者請求，**強烈建議**解決。
    - **Warning (警告)：** 不會阻止 agent 工作，但可能降低處理請求的準確性，**建議檢查**以確保觸發預期 intent。
    - **Information (資訊)：** 表明 agent 未遵循某個 best practice (最佳實務)，應檢查，但嚴重性最低。
- **建議：** 利用此工具提升 agent 開發的信心。

**2. 使用 Custom Scripts 進行驗證：**

- **目的：** 作為 Validation tool 的補充，滿足特定的驗證需求。
- **範例用途：**
    - 自動偵測不同 intents 下相似的 training phrases。
    - 確保 **intents tagging (意圖標記)** 成功 (例如，檢查被標記的 entity 是否出現在其 entity type 關聯的 synonym 中)。

**3. 利用 Test Sets 識別問題：**

- **目的：** 透過一組測試語句來識別 agent 中的問題。
- **最佳實務：**
    - **避免使用 training phrases：** 因為若 head intent 在 scope 內，使用 training phrase 通常會確保匹配，這無法反映系統在真實場景 (使用者不說訓練語句時) 的表現。
    - **使用真實或自然的語句：** 若可能，使用 production 中常見的 utterances (語句)。若從零開始建構，至少應使用預期真人使用者會說的自然片語。
    - **追求廣泛覆蓋 (Broad coverage)：** 確保測試片語能涵蓋大量 intents，而非只集中在一兩個。(例：若有 30 個 intents，測試應盡量覆蓋多數)。
    - **自動化執行：** 識別自動化運行 test set 的方法 (例如，使用 Conversational Agents **內建的 test cases** 或 **SCRAPI library**)。

**4. NLU Tuning (自然語言理解模型調整) 最佳實務：**

- **定義：** **NLU tuning** 是提高 **Natural Language Understanding (NLU) model** 準確性的過程。
- **核心原則：**
    - **謹慎添加失敗語句：** 當發現某個片語未能匹配您的 intent 時，**不要**直接將其添加為 training phrase。應小心操作，因為這可能導致 **overfitting (過度擬合)** intents，損害其性能。
    - **考慮其他解決方案：** 在添加前，先思考：是否需要更多 **entity tagging (實體標記)**？是否 **training phrase overlaps (訓練語句重疊)** 導致了 intent confusion (意圖混淆)？匹配這個特定片語**是否真的那麼重要**？(永遠不要期望 100% 的 intent 匹配率)。
    - **測試變更：** **務必**測試您所做的調整。不要僅僅假設您的 tuning 帶來了改進，而是要運行各種 test sets 以確保沒有發生 **regression (效能衰退)**。

**處理相似 Intents (常見 Tuning Use Case)：**

- **問題：** 具有相似 training phrases 的 Intents 會導致大量 **no-matches** (因為 NLU 無法足夠自信地確定匹配哪個 intent)。
- **解決選項：**
    1. **Merge different intents (合併不同意圖)：** 若它們代表的 user intention (使用者意圖) 足夠相似。
        - **做法：** 將想移除的 intent 中的相關 phrases 加入目標 intent (避免機械式複製導致 bloated intents - 意圖臃腫 和 overfitting) -> 替換指向舊 intent 的 routes -> 刪除舊 intent。
        - **工具：** 對於 phrases 數量少的 intents 可在 Conversational Agents 內完成；對於數量多的，建議下載到 **spreadsheet (電子表格)** 處理後再上傳 (可使用 custom script)。
    2. **Update the training phrases (更新訓練語句)：** 修改兩個 intents 的 training phrases，使它們足夠**區分 (distinct)**。
    3. **(若需拆分) Split intents (拆分意圖)：** 逐一檢查 training phrases，決定哪個 phrase 應歸屬哪個 intent。(處理方式同合併，大量 phrases 建議使用 spreadsheet/script)。

**開發完成標誌：**

- 至此，您已建構了 taxonomy，在 Conversational Agents 中實施了它，並進行了 tuning。您的開發工作已完成。

**總結來說，** 在將 Steering Agent 投入生產前進行 Tuning 至關重要。可以利用平台內建的 **Validation tool** 找出常見的配置問題 (按嚴重性 Error/Warning/Info 分級處理)，並可透過 **Custom Scripts** 進行補充驗證。設計 **Test Sets** 時應避免使用訓練語句，力求真實與廣泛覆蓋，並考慮自動化執行。在進行 **NLU Tuning** 時，切忌盲目添加失敗語句以防過度擬合，應先考慮其他優化方案，並務必測試變更以防效能衰退。對於因訓練語句相似而導致混淆的 Intents，可根據情況選擇 **Merge (合併)** 或 **Update/Split (更新/拆分)** 的策略來解決。完成這些 Tuning 工作後，開發階段便告一段落。