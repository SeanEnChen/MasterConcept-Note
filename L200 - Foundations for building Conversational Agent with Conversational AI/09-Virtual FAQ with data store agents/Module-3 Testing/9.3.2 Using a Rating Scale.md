# 9.3.2 Using a Rating Scale

**設定 Rating Scales**

在建立 **Evaluation set (評估集)** 後，下一步是為所有 **testers (測試人員)** 和 **evaluators (評估人員)** 採用一個**標準的 rating scale (評分量尺)**。

**Rating Scale 設計考量點：**

設計評分量尺時，需考量的決策點包括：

- 量尺的 **endpoints (端點)** (例如，評分範圍)。
- 如何處理 data store **僅提供 URL** 的情況。
- 是否對提供的 **URL** 和 **answer (答案)** **分別評分 (rated separately)**。

**常見的問答情境 (Scenarios) 分類：**

在評估時，question and answer pairs (問答對) 幾乎總是屬於以下四種 scenarios (情境) 之一：

1. **Small talk (閒聊)：** 處理非資訊性或閒聊式的查詢 (例如 "What’s your favorite color?")。
2. **Uninformative response with no URL (無資訊性回應且無 URL)：** Bot 不確定如何回答問題，且缺乏相關 URL。通常會回應如 "I’m not sure I understand the question. Could you try rephrasing?"。
3. **Uninformative response with a source URL (無資訊性回應但提供來源 URL)：** Bot 表示無法找到答案，但提供了一個 URL (例如 "Here is a link that may be helpful.")。
4. **Informative response with relevant source URL(s) (資訊性回應並提供相關來源 URL)：** Bot 提供資訊並附上相關的來源 URL。

**評分流程 (Rating Process) - 5 步驟：**

為 Data Store Agent 的回應評分時，建議遵循以下步驟：

1. **(若適用) 分配情境 (Assign response to a scenario)：** (適用於包含 scenarios 作為標準的評分格式) 首先將回應歸類到上述四種 scenarios 之一。每個 scenario 都有其特定的評分指南。
2. **閱讀評分指南 (Read rating guidelines)。**
3. **評估答案 (Evaluate the Answer)：** 根據指南評估 **GenAI Agents (生成式 AI 客服)** 返回的 generated text (生成文本)。
4. **評估 URL (Evaluate the URL)：** 根據指南評估找到資訊的 source URL。
5. **填寫其餘問題 (Fill out remaining questions)：** 完成其他評估項目。

**核心評估資訊 (所有評分量尺通用)：**

無論使用何種量尺，通常需要評估以下三點：

1. **答案準確性/完整性：** 基於可用資訊，回應的**準確度 (accurate)** 如何？它多大程度上回答了查詢？
2. **URL 的幫助程度/相關性：** 提供的 URL 是否給出了解決查詢的**邏輯性後續步驟 (logical next steps)**？它是否適用於提出的查詢？
3. **是否包含有害資訊 (Harmful information)：** 回應是否包含例如損害公司品牌的內容？

**可選的評估點：**

以下三點常依 use case 而定，但通常也很重要：

1. **Skip flag (跳過標記)：** 用於標記查詢與答案**無法評估 (unevaluable)** 的情況 (例如 **gibberish queries - 胡言亂語的查詢**)。
2. **Related intent (相關意圖)：** (若將 data store agent 加入已有 NLU taxonomy 的 DFCX agent 中) 標註該 utterance (語句) 在您的 taxonomy 中會匹配到哪個 intent。
3. **No-match flag (無匹配標記)：** 用於標記 data store agent 未能返回資訊性回應的情況 (例如，它反問了一個釐清問題)。

**建立基準 (Baseline)：**

- 建立一個 **baseline (基準)** 以便長期追蹤品質非常重要。
- 如何使用 baseline 因 use case 而異 (例如，某些情況下最重要的是沒有最低評分；另一些則需大量高品質回應)。

**各情境的詳細評分指南範例 (+5 為完美)：**

- **Scenario 1 (Small talk/不相關查詢)：**
    - **Answer：** Perfect = 符合品牌和 persona 的適當回應 (可能是禮貌拒絕或預設品牌答案)。Slightly good = 適當禮貌但略顯尷尬。Hurtful = 不恰當或有問題。
    - **URL：** 通常提供 URL 是 harmful 的 (因 data store 中不太可能有相關來源)，但視 use case 而定。
    - _範例：_ User 問無關問題，Bot 恰當解釋無法回答，Answer 可評為佳；URL 因不需要而評為 Perfect。若 Bot 回答 "My favorite color is blue." 則視品牌和 persona 決定是 helpful 或 hurtful。
- **Scenario 2 (無資訊性回應，無 URL - Bot 請求澄清)：**
    - **Answer：** Slightly good = 請求**相關的 (relevant)** follow-up information (前提是 data store 中確實無此資訊)。Neutral = 回應極其有限。Hurtful = 問了不必要的追問或言論不符品牌/persona。
    - **URL：** Neutral (因未提供)。
    - _範例：_ User 問 "Do I have to call a team member?" (模糊)，Bot 請求澄清。Answer 禮貌並致歉，可評為 Slightly good。URL 評為 2 (假設 2 代表 Neutral)。注意此回應觸發 No Match，標記為 Yes。
    - _警示：_ 若 Bot 對於其 data store 中應有的資訊也無法回答而請求澄清，則是非常罕見的，可能表示缺少 context 或 content。
- **Scenario 3 (無資訊性回應，但提供 URL)：**
    - **Answer：** Good = Bot 指出無法直接回答的**正當理由 (legitimate reason)** (例如，問題依賴 Bot 缺乏的個人資訊)。Neutral = Bot 僅說無法回答並參考連結。Harmful = 與品牌/persona 不符。
    - **URL：** 評分基於其幫助程度。5 = 最佳 URL，完全解決查詢。隨幫助程度降低而減分，1 = 無助益甚至誤導/有害。
    - _範例：_ User 問 "Is 5G included in my phone plan?"，Bot 回應無資訊僅提供 URL，Answer 評為 Neutral。若 URL 能回答問題但未提供後續步驟，URL 可評為 Good。
- **Scenario 4 (資訊性回應 + URL)：**
    - **Answer：** 5 = Perfect answer，完全解決使用者需求。中間評級代表準確但未完全回答。最低評級保留給錯誤或誤導性答案。
    - **URL：** 5 = Perfect URL，最佳可用且完全解決查詢。評級越低表示 URL 越缺乏相關性或資訊。最差的是造成實際傷害 (如誤導)。(URL 評級有助於評估 knowledge base 品質)。
    - _範例：_ User 問 "What is the cheapest package you offer to get unlimited 5G?"，Answer 說 "Our 5G package is $25/line."，資訊正確但缺少方案名稱等關鍵資訊，Answer 評為 Good。若 URL 包含所有 5G 方案及其詳細資訊 (包含最便宜的)，則 URL 評為 Perfect。

**總結來說，** 在建立了 Evaluation Set 之後，為 Data Store Agent 的回應設定標準化的 Rating Scale 至關重要。評分時，首先可將問答對歸類到四種常見情境之一 (閒聊、無資訊無URL、無資訊有URL、有資訊有URL)。接著，依據各情境的評分指南，分別對 Agent 的 Answer (生成文本) 和提供的 URL (若有) 進行評估，核心標準包含準確性、完整性、幫助程度及是否包含有害資訊。同時也可記錄是否跳過、相關意圖及是否觸發 No-match 等輔助資訊。建立並持續追蹤評分基準，有助於量化和迭代改進 Data Store Agent 的效能。