# 4.2 What are Generative AI and LLMs ?

**一、Traditional Programming and Machine Learning**

- **Traditional programming (傳統編程)**：使用程式碼表達規則，這些規則作用於數據，並返回答案 (you express rules using code, these act on data, and in return you get answers)。但有時編寫這些規則很棘手 (writing these rules is tricky)。
- **Machine Learning (機器學習)**：只是一種不同的編碼方法 (just a different methodology for coding)。您提供答案和數據，讓電腦找出規則 (you provide the answers and data, and let the computer come up with the rules)。這有助於解決一系列傳統編程無法解決的新問題 (This has helped to solve a whole range of new problems that couldn’t be solved with traditional programming)。然而，它仍然是一種相當狹隘的方法，專注於學習特定的任務或概念 (it’s still quite a narrow approach, focused on learning about specific tasks or concepts)。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/17421940210003b47cf.png)

**二、Large Language Models (LLMs)**

近年來，像 GPT-3 和 LaMDA 這樣的生成式語言模型出現了 (generative language models like GPT-3 and LaMDA, have entered the scene)。透過這種方法，您將大量的文本數據輸入到模型中 (you feed lots of text data into a model)。

從所有這些文本數據中，這些大型模型已經學習了看似無窮無盡的概念 (from all that text data, these large models have learned a seemingly endless number of concepts)。感覺它們已經基於語言開發了一種基礎智能，可以實現各種新的事物 (it feels like they’ve developed a foundational intelligence based on language that could enable all kinds of new things)。

大型語言模型 (LLMs) 已經擁有廣泛的適用案例 (already have a wide range of applicable use cases)，並且這些案例還在不斷增長 (and these are growing all the time)。LLM 的一些最常見的用例包括：問答 (Question and answer)、推薦 (Recommendations)、腦力激盪 (Brainstorming)、自然語言對話 (Natural language Conversations)、翻譯 (Translations) 和內容生成 (Content generation)。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/17421941940006s48ow.png)

**三、How does an LLM generate its answers?**

LLM 接收一些輸入，這是您的提示 (an LLM takes in some input, this is your prompt)，然後輸出最有可能的下一個詞 (and out the other end, it returns the most likely next word)。如果您提供句子：「The cat sat on the」(貓坐在...)，那麼 LLM 會返回「mat」(墊子)。這有點簡化了，因為它實際上並不僅僅返回一個詞 (this is a bit of a simplification because it's not really just returning a single word)。實際上，它返回一個可能出現的詞元 (tokens) 的機率分佈 (in reality, it returns a probability distribution, over possible tokens that are likely to come next)。

在這個例子中，LLM 會返回「mat, rug, chair and hat」(墊子、地毯、椅子和帽子) 以及它們各自的機率 (with levels of probability)。或者，您可以將其視為詞彙表中單詞的向量，每個單詞都有一個相關的分數 (alternatively, you could think of it as a vector of words in your vocabulary, with a score associated to each word)。這個分數表示根據模型訓練的所有文本，該詞成為句子中下一個詞元的可能性 (and this score indicates the likelihood that it will be the next token in the sentence, based on all the text the model was trained on)。

例如，如果您的輸入是「the garden is full of beautiful…」(花園裡充滿了美麗的...)，輸出是一個包含可能後續單詞及其分數的向量 (if you have the input, “the garden is full of beautiful…”, and the output is a vector of possible subsequent words with a score)。這個輸出向量顯示「flowers」(花) 的可能性很高，然後可能是「trees」(樹木) 或「herbs」(香草)，而「bugs」(蟲子) 的可能性較小 (this output 1 vector shows that “flowers” is pretty likely, then maybe “trees” or “herbs”, and then “bugs” is less likely)。  

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742194168000r0qzap.png)

**四、Decoding Strategies**

問題是…從所有這些潛在的詞或詞元中，實際返回的是哪一個？(So the question is… from all these potential words, or tokens, which one is actually returned?) 選擇輸出詞元的方法是語言模型文本生成中的關鍵組成部分 (the method of picking output tokens is a key component in text generation with language models)。這些方法稱為解碼策略 (these methods are called decoding strategies)，並且有多種解碼策略用於選擇輸出詞元 (and there are several decoding strategies for picking the output token)。現在，讓我們介紹幾種常見的解碼策略 (now, let’s cover a couple of common decoding strategies)。

您可能首先考慮的是只返回機率最高的詞 (the first thing that you might consider is just returning the word with the highest probability)。這稱為貪婪解碼 (this is known as Greedy decoding)。貪婪解碼是一種合理且相當直觀的策略，但它確實有一些缺點，例如輸出包含重複的文本循環 (greedy decoding is a reasonable and fairly intuitive strategy, but it does have some drawbacks, such as outputs with repetitive loops of text)。

例如，想想您智慧型手機自動建議功能中的建議 (for example, think of the suggestions in your smartphone's auto-suggest feature)。如果您不斷選擇最高的建議詞，它可能會演變成重複的句子，並且生成的文本與任何人在現實生活中說話的方式都不相似 (if you continually picked the highest suggested word, it may devolve into repeated sentences, and the text generated wouldn’t be similar to the way anyone would speak in real life)。

或者，可以透過隨機抽樣模型返回的分佈來生成回應 (alternatively, a response could be generated by randomly sampling over the distribution returned by the model)。這個過程是隨機的 (this process would be stochastic)，輸出與上下文的相關性取決於使用案例 (and the relevance of the output to the context would depend on the use case)。也許花園裡充滿了美麗的蟲子，這對輸入來說是一個完全可以接受的回應 (maybe the garden was full of beautiful bugs and that is a perfectly acceptable response to the input)，但是，您可以想像，這並不總是一種合適的方法 (however, as you can imagine, this would not always be an appropriate method)。