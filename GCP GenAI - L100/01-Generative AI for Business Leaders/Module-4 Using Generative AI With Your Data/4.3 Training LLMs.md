# 3.3 Training LLMs

**一、Different Sources of Information for LLM Training**
- **Generic Datasets：**
	- Google 的大型語言模型是在各種通用數據集上訓練的，例如 OpenWeb Text 或 C4。
	
- **Social Media and Language Based Content：**
	- 它們也在社交媒體和基於語言的內容上進行訓練，例如書籍和其他基於文本的資源，這些被認為是狹窄的預訓練語料庫 (narrow pre-training corpora)。
	
- **Domain Specific Datasets：**
	- 然後是各種特定領域的數據集，例如 Mimic 3，它與醫療保健相關。
	
- **Domain-specific Content：**
	- 還有特定領域的內容，例如新聞、科學文檔和內部專有數據。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/17421945010008012pj.png)

**六、Different Source Combinations and Models**

不同的來源組合和不同的模型將會產生不同的結果。例如，ChatGPT-3 主要在 Common Crawl、WebText2、書籍和一些額外的 Wikipedia 上進行訓練。DeepMind 的 MassiveText 在更多樣化的數據集上進行訓練，包括 MassiveWeb、書籍、C4、新聞、GitHub 和 Wikipedia。**而 Vertex AI 背後的模型 Gemini 和 PaLM，則在許多 Google 專有的數據上進行訓練，例如社交媒體對話和一些經過濾的網頁，此外還包括 DeepMind 模型中的大部分內容。**

