# 3.7 Solution 2：Prompt Design

**一、Introduction to Prompt Design**

另一種使 LLM 更可靠的方法是提示設計。正如您可能記得的，提示 (Prompt) 只是您輸入到模型的文本。但是，提示設計是一門藝術和科學，專注於找出如何編寫和格式化提示文本，以使 LLM 執行您想要的操作。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742195781000hnup7i.png)

**二、N-shot Prompting**

有幾種提示 LLM 的技術。第一種稱為 N-shot prompting，它有不同的變體。例如：
- zero-shot prompting 是指您只是發送指令而不提供任何範例
- few-shot prompting 是指您發送指令並附帶範例。

使用單個命令讓 LLM 採取某種行為的方法稱為 zero-shot learning。除了提供指令外，透過添加範例向模型展示您希望它如何回應可能會很有幫助。這稱為 few-shot prompting，因為您向模型展示了幾個範例以及您的提示。few-shot prompting 利用了大型語言模型非常擅長識別和複製文本數據中的模式這一事實。想法是向 LLM 發送一個文本模式，它會學習完成它。例如：
- 假設您想使用 LLM 建立一個英語到法語的翻譯器原型。
- 以下是一個旨在實現這一目標的文本提示：在這個提示中，您建立了一個模式：英語單詞到法語單詞。如果您將此提示發送給大型語言模型，它將「自動完成」該模式並返回類似這樣的內容：模型的回應可能看起來有點奇怪。模型不僅返回了「lipstick」的法語翻譯（這是手寫提示中的最後一個英語單詞），還返回了整個附加的英法語對列表，那是因為 LLM 正在「continuing the pattern」。
- 如果您只是想建立一個告訴我們法語單詞的函數，您可能並不在乎模型在「rouge a levre」之後生成的任何文本。作為應用程式設計師，您可能想要截斷那些多餘的範例。
- 此外，如果您想將其建置到某種應用程式中，您可能想要參數化輸入，以便「lipstick」不是一個固定的字串，而是最終使用者提供的變數。這樣，使用者可以輸入他們想要翻譯的英語單詞。讓我們用另一個範例進一步探索這一點。
- 這是另一個 few-shot prompting - 在兩種程式語言之間進行翻譯，以便您可以找到某些 Python 腳本的 Javascript 等效程式碼。
	1. 首先，您描述任務：將 Python 轉換為 Javascript
	2. 接下來，您提供一個範例。然後是您想要轉換的實際 Python 程式碼。此提示的最後一部分是「Javascript:」，因為您希望引導模型輸出 Javascript 程式碼。
	3. 同樣，如果您想建立一個 Python 到 Javascript 的應用程式並使用 LLM，您將參數化輸入而不是在提示中硬編碼它。這樣，您的使用者可以提供他們想要轉換的 Python 程式碼。
	4. 每次使用者與您的 Python 到 Javascript 應用程式互動時，他們想要轉換的程式碼都會作為 USER_INPUT 傳遞進來，並且整個文本塊都會發送到模型。

找出確切的提示和範例應該是什麼可能需要大量的實驗。現在，您可能想知道「What is the best way to write a model prompt (編寫模型提示的最佳方式是什麼？)」。壞消息是，目前沒有編寫模型提示的「optimal」方式。那是因為您獲得的結果高度依賴於模型。有時，措辭或詞序的微小變化可以以並非總是可預測的方式改進 LLM 的輸出。

這是另一個範例 -> **reverse dictionary prompt**。由人類編寫。您有一個指令，然後是定義及其對應單詞的兩個範例。因此，您可以接受使用者輸入，並將所有這些作為您的提示傳遞。您可能已經注意到，這些 few-shot prompting 的確切模式略有不同。除了包含範例外，一些提示還以單行指令開頭：「Given a definition, return the word it defines.」。但事實是，沒有辦法確切知道哪種提示結構效果最好！有時，只需稍微更改格式或措辭，就能改進模型的輸出。

**三、Chain of Thought**

第二種技術稱為 Chain of Thought，它包括提供一個提示並將操作分解為中間推理步驟，以便 LLM 可以找到最正確、經過深思熟慮的答案。連鎖思考提示是一種提高大型語言模型推理能力的方法。它的工作原理是向 LLM 提供一個包含一系列中間推理步驟的提示。這有助於 LLM 將複雜的問題分解為更小、更易於管理的部分。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742196390000hsyeor.png)

Standard Prompting 要求模型直接給出多步驟推理問題的答案，而 Chain of Thought Prompting 則引導模型將問題分解為中間推理步驟，在本例中，這將導致正確的最終答案。

例如，假設您想要求 LLM 解決一個數學文字題。
- **Standard Prompting**：
	- 您在向 LLM 提出問題之前，會在提示中提供一個範例問題和答案。範例問題是「Roger 有 5 個網球。他又買了兩包網球。每包有 3 個網球。他現在有多少個網球？」，範例答案是「答案是 11」。
	- 當向 LLM 提出類似的問題「自助餐廳原來有 23 個蘋果。如果他們用了 20 個做午餐，又買了 6 個，他們現在有多少個蘋果？」時，LLM 試圖一次性解決問題，但答案是錯的，它回答「答案是 27」。
- **Chain of Thought Prompting**：
	- 問題保持不變，但是範例答案將問題分解為邏輯步驟，「Roger 最初有 5 個球。2 包，每包 3 個網球，總共是 6 個網球。5 加 6 等於 11。答案是 11」。當向 LLM 提出問題時，它能夠使用相同的逐步邏輯過程來計算正確答案。
	- 「自助餐廳原來有 23 個蘋果。他們用了 20 個做午餐。所以他們剩下 23 減 20 等於 3 個。他們又買了 6 個蘋果，所以他們現在有 3 加 6 等於 9 個。答案是 9」。
	![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742196560000z2inzk.png)

與現有的 LLM 一起使用的 Chain of Thought Prompting 展示了突出的能力，可以執行自我條件推理追蹤，從問題中得出答案，在各種算術、常識和符號推理任務中表現出色。然而，使用連鎖思考提示時，模型並非基於外部世界，而是使用其自身的內部表示來生成推理追蹤，這限制了其反應性探索和推理或更新其知識的能力。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742196680000i02iwo.png)

**四、ReAct**

第三種技術稱為 ReAct，它是一個循環思考過程，涉及回答問題和在外部系統中查找數據。

ReAct 結合了推理和行動方面的進展，使 LLM 能夠解決各種語言推理和決策任務。當提示更大的語言模型和微調更小的語言模型時，ReAct 模式系統地優於僅推理和僅行動的模型。ReAct 使 LLM 能夠以交錯的方式生成口頭推理追蹤和文本行動。雖然行動會導致來自外部環境的觀察回饋，但推理追蹤不會影響外部環境。相反，它們透過對上下文進行推理並使用有用的資訊更新上下文來影響模型的內部狀態，以支持未來的推理和行動。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742196832000xutxr4.png)

在提示之後，LLM 會回應一個 **Thought** 和一個 **Action**，該行動用於生成 **Observation**。
- **Thought** 是關於如何行動的推理。
- **Action** 用於制定對外部系統（可以是 Cloud Functions 或外部 API）的呼叫。
- **Observation** 是來自外部系統的回應。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742196985000qk9k4a.png)

透過這些交錯的 **Thought**、**Action** 和 **Observation**，LLM 最終得出答案。這種從思考到行動再到觀察的模式會重複進行，稱為核心 react loop 或 react cycle。react cycle 會重複進行，直到 LLM 得出答案。

讓我們從視覺上了解 ReAct cycle。所以，您有一個 LLM。您透過要求它回答問題來提示 LLM。
例如：
- 「像對一個 5 歲的孩子一樣向我解釋 Google Pixel 8 手機的功能」。然後，LLM 會回應一個想法和一個行動。
- 在我們的例子中，它會思考它是否擁有關於 Google Pixel 8 的資訊，由於它沒有，它將觸發一個在其他地方搜尋該資訊的行動。
- 然後，該行動用於呼叫外部系統，例如 API 或 Cloud Function。
- 該外部系統可能包含一個包含手機及其技術功能的數據庫。
- 然後，LLM 會再次被提示，並將來自外部系統的回應作為觀察結果。
- 然後，LLM 將再次形成一個想法。在我們的例子中，LLM 現在擁有技術功能，但它們的狀態對於 5 歲的孩子來說是無法理解的。
- 然後這個循環將重複進行。LLM 除了思考之外，還會制定一個行動。將進行另一次外部呼叫。
- 然後，LLM 將再次被提示並獲得觀察結果。在我們的例子中，LLM 的行動是根據提供的技術規格編寫適合 5 歲孩子閱讀的文本。
- 最終，LLM 會產生一個構成答案的想法，並且不再制定行動。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742197155000xsqevt.png)

**五、Advantages and Limitations of Prompt Design**

提示設計非常棒，因為它使您能夠快速建立原型。它還使您能夠將一個模型實例用於許多應用程式 - 解決數學問題、詢問有關國家首都的問題、刪除 PII 或寫詩。但是，提示設計確實有其局限性：提示設計目前更像是一門「藝術」而不是一門「科學」。它高度依賴於預訓練數據。提示的確切措辭和順序很重要。而且對於推論來說效率不高。

![gh](https://raw.githubusercontent.com/SeanChenR/img_gif/main/myimage/1742197206000u9185j.png)
