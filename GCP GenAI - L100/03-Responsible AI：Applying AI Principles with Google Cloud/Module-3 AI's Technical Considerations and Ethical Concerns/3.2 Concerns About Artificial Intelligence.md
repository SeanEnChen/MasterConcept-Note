# 3.2 Concerns About Artificial Intelligence

**一、Main Ethical Concerns Related to AI**

雖然這個單元可能不盡詳盡，但我們將探討一些在討論 AI 的倫理考量時經常受到關注的主題。Google 致力於解決每個 AI 用例帶來的獨特挑戰，但作為一個行業，我們必須提高對這些考量的認識，以便共同制定解決方案。以下是目前被提出的主要 AI 考量：

1. Transparency (透明度)：隨著 AI 系統變得越來越複雜，建立足夠的透明度讓人們理解 AI 系統如何做出決策變得越來越困難。在許多情況下，能夠理解 AI 系統的工作方式對於最終用戶的自主權或做出知情選擇的能力至關重要。缺乏透明度也可能使開發人員更難預測這些系統何時以及如何可能失敗或造成意外傷害。允許人類理解促成決策的因素的模型可以幫助 AI 系統的利害關係人更好地與 AI 協作。這可能意味著知道何時在 AI 表現不佳時進行干預、加強使用 AI 系統結果的策略以及確定如何改進 AI。
2. Unfair bias (不公平的偏見)：AI 本身不會產生不公平的偏見；它暴露了現有社會系統中存在的偏見並將其放大。AI 的一個主要缺陷是其擴展能力可能會強化和延續不公平的偏見，從而導致進一步的意外傷害。塑造社會的不公平偏見也塑造了 AI 的每個階段，從數據集和問題 формулировка 到模型創建和驗證。AI 直接反映了其設計和部署所在的社會背景。為了減輕危害，需要認識並解決社會背景和可能的偏見。例如，視覺系統正被應用於公共安全和人身安全的重要領域，以監控建築活動或公眾示威。在這裡，偏見可能使監視系統更有可能將邊緣化群體錯誤地識別為罪犯。這些挑戰源於許多根本原因，例如某些群體在訓練數據中的代表性不足而另一些群體的代表性過度、缺乏充分理解系統影響所需的關鍵數據，或者產品開發中缺乏社會背景。
3. Security (安全性)：像任何電腦系統一樣，不良行為者有可能出於惡意目的利用 AI 系統中的漏洞。隨著 AI 系統嵌入社會的關鍵組成部分，這些攻擊代表了漏洞，有可能嚴重影響安全。安全可靠的 AI 涉及資訊安全中的傳統考量以及新的考量。AI 的數據驅動特性使得訓練數據更有價值被竊取，此外，AI 可以實現更大規模和更快速度的攻擊。我們也看到 AI 獨有的新型操縱技術，例如 deepfakes，它可以模仿某人的聲音或生物特徵。
4. Privacy (隱私權)：AI 能夠快速且輕鬆地從不同來源收集、分析和組合大量數據。AI 對隱私的潛在影響是巨大的，導致數據濫用、不必要的識別和追蹤、侵入性的語音和面部辨識以及個人資料分析的風險。AI 的廣泛使用伴隨著採取負責任的隱私保護方法的必要性。
5. AI pseudoscience (AI 偽科學)：AI 從業者推廣缺乏科學基礎的系統。例如，聲稱能夠根據面部特徵和頭部形狀大小來測量人的犯罪傾向的面部分析演算法，或者用於情緒檢測以判斷某人是否值得信賴的面部表情模型。科學界認為這些做法是不科學且無效的，並且可能造成傷害。然而，它們已經被重新包裝成 AI 的形式，這可能會使偽科學看起來更可信。這些 AI 的偽科學應用不僅會傷害個人和社群，而且還會削弱 AI 的適當和有益的用例。
6. Accountability to people (對人負責)：AI 系統的設計應確保滿足所有類型人群的需求和目標，同時實現適當的人為指導和控制。我們努力以不同的方式在 AI 系統中實現責任，包括為系統明確定義目標和操作參數、透明地說明何時以及如何使用 AI，以及人們干預或向系統提供回饋的能力。
7. AI-driven unemployment and deskilling (AI 驅動的失業和技能下降)：雖然 AI 為常見任務帶來了效率和速度，但更普遍的擔憂是 AI 會導致失業和技能下降。此外，還有人擔心，隨著我們越來越依賴技術，人類的能力會下降。社會過去也曾出現過技術創新，我們也相應地進行了調整，例如汽車取代馬匹，但同時也創造了以前難以想像的新產業和就業機會。如今，創新和技術進步的速度和規模都與以往不同。如果生成式 AI (generative AI) 實現其承諾的能力，勞動力市場可能會面臨嚴重的混亂。然而，正如任何重大的技術進步一樣，工作也會發生轉變。例如，在商業航空旅行出現之前，誰能想像空中服務員呢？雖然許多工作可能會被生成式 AI 所補充，但我們今天無法想像的全新工作也會被創造出來。這個挑戰伴隨著機遇。我們需要共同努力制定計劃，幫助人們謀生並在工作中找到意義，迎接挑戰並抓住機遇。

**二、Unique Concerns for Generative AI**

除了通用 AI 應用和模型的考量之外，生成式 AI 還有其獨特的考量。作為一種廣為人知的生成式 AI，大型語言模型 (large language models) 以自然語言的形式生成文本的創意組合。關於大型語言模型主要有三個擔憂：**hallucinations (幻覺)、factuality (事實性) 和 anthropomorphization (擬人化)**。

在生成式 AI 中，幻覺指的是 AI 模型生成不切實際、虛構或完全捏造的內容的情況。事實性涉及生成式 AI 模型產生的資訊的準確性或真實性。擬人化指的是將人類般的特質、特徵或行為歸因於非人類實體，例如機器或 AI 模型。

**三、Reasons for Ethical Issues**

Capgemini 調查的受訪高管列舉了導致這些倫理問題的一些原因：缺乏專用於道德 AI 系統的資源（包括資金、人員和技術）、開發 AI 系統時缺乏多元化的團隊（就種族、性別和地域而言）以及缺乏道德 AI 行為準則或評估其偏差的能力。在報告中，高管們還認為，迫切實施 AI 的壓力是導致 AI 使用中出現倫理問題的首要原因。這種壓力可能源於獲得先行者優勢的緊迫性、在 AI 的創新應用方面超越競爭對手的需求，或者僅僅是利用 AI 所能提供的優勢的壓力。值得注意的是，調查中 33% 的受訪者表示，在構建 AI 系統時實際上並未考慮道德問題，這本身就令人擔憂。

**四、Socially Beneficial Uses of AI**

但倫理不僅僅是關於我們不想做或不應該做的事情。AI 和新興技術有許多對社會有益的用途，可以幫助為生活和社會做出積極的貢獻。AI 和新技術可以透過以下方式幫助解決複雜的問題：改進材料、設計和流程、開發新的醫學和科學突破、更可靠地預測複雜的動態系統、提供更實惠的商品和服務以及從例行或重複性工作中解放出來。即使對於這些非常有益於社會的解決方案，負責任的 AI 對於確保所有人都受益，而不僅僅是一小部分利害關係人受益，也是至關重要的。

**五、Key Benefit of Ethical Practices**

組織中道德實踐的關鍵好處是，它們有助於避免對客戶、用戶和整個社會造成傷害。道德實踐促進人類繁榮。這是我們最需要關注的。在 Google，我們的 AI 治理目標是努力解決這些引起倫理問題的擔憂，而負責任的 AI 實踐可以幫助實現這一目標。實施您自己的負責任的 AI 治理和流程可以幫助解決您業務中的這些倫理考量。